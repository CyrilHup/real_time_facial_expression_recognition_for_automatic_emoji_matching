% chktex-file 8
% chktex-file 18
% chktex-file 26
\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{float}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}

\geometry{margin=2.5cm}

% Configuration des listings pour Python
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Python
}

\lstset{style=pythonstyle}

\title{\textbf{Reconnaissance d'√âmotions Faciales en Temps R√©el}\\
\large Rapport Technique - Deep Learning}
\author{Cyril}
\date{Novembre 2025}

\begin{document}

\maketitle

\begin{abstract}
Ce rapport pr√©sente le d√©veloppement d'un syst√®me de reconnaissance d'√©motions faciales en temps r√©el utilisant des r√©seaux de neurones convolutifs (CNN). Nous d√©taillons l'architecture initiale, les optimisations apport√©es, ainsi que les choix techniques effectu√©s pour am√©liorer les performances de d√©tection. Le syst√®me est capable de classifier huit √©motions (col√®re, d√©go√ªt, peur, joie, tristesse, surprise, neutralit√©, et m√©pris) √† partir d'un flux vid√©o webcam, en utilisant le dataset Balanced AffectNet.
\end{abstract}

\tableofcontents
\newpage

%==============================================================================
\section{Introduction}
%==============================================================================

La reconnaissance automatique des √©motions faciales est un domaine en pleine expansion avec des applications dans l'interaction homme-machine, la sant√© mentale, le marketing, et l'√©ducation. Ce projet impl√©mente un syst√®me complet de reconnaissance d'√©motions en temps r√©el, depuis l'entra√Ænement du mod√®le jusqu'√† l'inf√©rence via webcam.

\subsection{Objectifs}
\begin{itemize}
    \item D√©velopper un mod√®le CNN capable de classifier 8 √©motions faciales (7 de FER2013 + Contempt de FER+)
    \item Impl√©menter une application temps r√©el avec d√©tection de visage
    \item Optimiser les performances pour une utilisation fluide
    \item Am√©liorer la pr√©cision par des techniques avanc√©es de deep learning
\end{itemize}

%==============================================================================
\section{Architecture Initiale}
%==============================================================================

\subsection{Dataset : Balanced AffectNet}

Le dataset Balanced AffectNet est une version √©quilibr√©e et pr√©-trait√©e du dataset AffectNet original. Il contient :
\begin{itemize}
    \item 41 008 images de visages en couleur (RGB)
    \item R√©solution : 75$\times$75 pixels
    \item 8 classes d'√©motions parfaitement √©quilibr√©es ($\sim$5 126 images par classe)
    \item Split pr√©d√©fini : train (29 526), val (7 382), test (4 100)
\end{itemize}

\textbf{Avantages par rapport √† FER2013 :}
\begin{itemize}
    \item Images RGB offrant plus d'informations (couleur de peau, rougeurs)
    \item R√©solution sup√©rieure (75$\times$75 vs 48$\times$48)
    \item Dataset parfaitement √©quilibr√© (pas de biais de classe)
    \item Classe Contempt (m√©pris) incluse nativement
\end{itemize}

\subsection{Mod√®le Initial (\texttt{model.py})}

L'architecture initiale est un CNN simple avec 3 blocs convolutifs :

\begin{lstlisting}[caption={Architecture CNN pour Balanced AffectNet}]
class FaceEmotionCNN(nn.Module):
    def __init__(self, num_classes=8, in_channels=3, input_size=75):
        super(FaceEmotionCNN, self).__init__()
        # Bloc Convolutif 1 (75x75 -> 37x37)
        self.conv1a = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)
        self.bn1a = nn.BatchNorm2d(64)
        self.conv1b = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.bn1b = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.dropout1 = nn.Dropout(0.1)
        
        # ... Blocs 2, 3, 4 similaires ...
        
        # Global Average Pooling
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        
        # Couches Fully Connected
        self.fc1 = nn.Linear(512, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, num_classes)  # 8 emotions
\end{lstlisting}

\subsubsection{Analyse de l'architecture initiale}

\begin{table}[H]
\centering
\caption{Dimensions √† travers le r√©seau (Balanced AffectNet)}
\begin{tabular}{lccc}
\toprule
\textbf{Couche} & \textbf{Entr√©e} & \textbf{Sortie} & \textbf{Param√®tres} \\
\midrule
Block1 (2$\times$Conv) + Pool & $3 \times 75 \times 75$ & $64 \times 37 \times 37$ & 38,592 \\
Block2 (2$\times$Conv) + Pool & $64 \times 37 \times 37$ & $128 \times 18 \times 18$ & 147,712 \\
Block3 (2$\times$Conv) + Pool & $128 \times 18 \times 18$ & $256 \times 9 \times 9$ & 590,336 \\
Block4 (2$\times$Conv) + Pool & $256 \times 9 \times 9$ & $512 \times 4 \times 4$ & 2,360,320 \\
Global Avg Pool & $512 \times 4 \times 4$ & 512 & 0 \\
FC1 + BN & 512 & 256 & 131,584 \\
FC2 + BN & 256 & 128 & 33,024 \\
FC3 & 128 & 8 & 1,032 \\
\midrule
\textbf{Total} & & & \textbf{$\approx$ 3.3M} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Points positifs :}
\begin{itemize}
    \item Utilisation de Batch Normalization pour stabiliser l'entra√Ænement
    \item Dropout (50\%) pour r√©gulariser et √©viter l'overfitting
    \item Architecture simple et rapide √† entra√Æner
\end{itemize}

\textbf{Limitations :}
\begin{itemize}
    \item Peu de couches convolutives (faible capacit√© d'extraction de features)
    \item Nombre de filtres limit√© (32-64-128)
    \item Pas de r√©gularisation dans les couches convolutives
    \item Couche FC1 tr√®s large (2.3M param√®tres) cr√©ant un goulot d'√©tranglement
\end{itemize}

\subsection{Script d'entra√Ænement initial (\texttt{train.py})}

\begin{lstlisting}[caption={Script d'entra√Ænement initial}]
# Hyperparametres
BATCH_SIZE = 64
LEARNING_RATE = 0.001
EPOCHS = 25

# Transformations simples
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.ToTensor(),
])

# Chargement du dataset
dataset = FER2013Dataset('./data/fer2013/fer2013.csv', transform=transform)
train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# Modele et optimiseur
model = FaceEmotionCNN().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# Boucle d'entrainement
for epoch in range(EPOCHS):
    running_loss = 0.0
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    
    print(f"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}")

torch.save(model.state_dict(), 'emotion_model.pth')
\end{lstlisting}

\textbf{Limitations de l'entra√Ænement initial :}
\begin{itemize}
    \item Pas de data augmentation (risque d'overfitting)
    \item Pas de s√©paration train/validation (impossible de d√©tecter l'overfitting)
    \item Learning rate fixe (convergence sous-optimale)
    \item Pas d'early stopping (gaspillage de ressources)
    \item Pas de gestion du d√©s√©quilibre des classes
\end{itemize}

\subsection{Application initiale (\texttt{app.py})}

L'application initiale effectue :
\begin{enumerate}
    \item Capture vid√©o via webcam
    \item D√©tection de visage avec Haar Cascade
    \item Pr√©traitement de l'image (redimensionnement 48$\times$48, grayscale)
    \item Inf√©rence avec le mod√®le CNN
    \item Affichage de l'√©motion d√©tect√©e avec emoji
\end{enumerate}

\begin{lstlisting}[caption={Boucle principale de l'application initiale}]
while True:
    ret, frame = cap.read()
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray_frame, 
                                          scaleFactor=1.3, 
                                          minNeighbors=5)

    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)
        roi_gray = gray_frame[y:y+h, x:x+w]
        
        # Preprocessing
        roi_tensor = data_transform(roi_gray).unsqueeze(0).to(device)

        # Prediction
        with torch.no_grad():
            outputs = model(roi_tensor)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            max_prob, predicted_idx = torch.max(probabilities, 1)
            
            idx = predicted_idx.item()
            confidence = max_prob.item() * 100
            emotion_text = emotion_dict[idx]
\end{lstlisting}

\textbf{Limitations :}
\begin{itemize}
    \item Pr√©dictions instables (oscillations entre frames)
    \item Pas de normalisation de l'√©clairage
    \item Param√®tres de d√©tection de visage sous-optimaux
    \item Interface utilisateur basique
\end{itemize}

%==============================================================================
\section{Optimisations de l'Architecture}
%==============================================================================

\subsection{Nouvelle Architecture CNN}

L'architecture am√©lior√©e comporte plusieurs optimisations majeures :

\begin{lstlisting}[caption={Architecture CNN am√©lior√©e}]
class FaceEmotionCNN(nn.Module):
    def __init__(self, num_classes=7):
        super(FaceEmotionCNN, self).__init__()
        
        # Bloc 1 (48x48 -> 24x24) - Double convolution
        self.conv1a = nn.Conv2d(1, 64, kernel_size=3, padding=1)
        self.bn1a = nn.BatchNorm2d(64)
        self.conv1b = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.bn1b = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.dropout1 = nn.Dropout(0.25)
        
        # Bloc 2 (24x24 -> 12x12)
        self.conv2a = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2a = nn.BatchNorm2d(128)
        self.conv2b = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.bn2b = nn.BatchNorm2d(128)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.dropout2 = nn.Dropout(0.25)
        
        # Bloc 3 (12x12 -> 6x6)
        self.conv3a = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn3a = nn.BatchNorm2d(256)
        self.conv3b = nn.Conv2d(256, 256, kernel_size=3, padding=1)
        self.bn3b = nn.BatchNorm2d(256)
        self.pool3 = nn.MaxPool2d(2, 2)
        self.dropout3 = nn.Dropout(0.25)
        
        # Bloc 4 (6x6 -> 3x3)
        self.conv4a = nn.Conv2d(256, 512, kernel_size=3, padding=1)
        self.bn4a = nn.BatchNorm2d(512)
        self.conv4b = nn.Conv2d(512, 512, kernel_size=3, padding=1)
        self.bn4b = nn.BatchNorm2d(512)
        self.pool4 = nn.MaxPool2d(2, 2)
        self.dropout4 = nn.Dropout(0.25)
        
        # Global Average Pooling
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        
        # Fully Connected avec dropout progressif
        self.fc1 = nn.Linear(512, 256)
        self.bn_fc1 = nn.BatchNorm1d(256)
        self.dropout_fc1 = nn.Dropout(0.5)
        
        self.fc2 = nn.Linear(256, 128)
        self.bn_fc2 = nn.BatchNorm1d(128)
        self.dropout_fc2 = nn.Dropout(0.4)
        
        self.fc3 = nn.Linear(128, num_classes)
\end{lstlisting}

\subsubsection{Justification des choix architecturaux}

\begin{enumerate}
    \item \textbf{Double convolution par bloc (style VGG)} \\
    Inspir√© de VGGNet, deux convolutions 3$\times$3 successives permettent un champ r√©ceptif de 5$\times$5 avec moins de param√®tres qu'une convolution 5$\times$5 unique :
    \begin{equation}
        \text{Param√®tres }3\times3\times2 = 2 \times (3^2 \times C^2) = 18C^2
    \end{equation}
    \begin{equation}
        \text{Param√®tres }5\times5 = 5^2 \times C^2 = 25C^2
    \end{equation}
    
    \item \textbf{Augmentation progressive des filtres (64‚Üí128‚Üí256‚Üí512)} \\
    Suit le principe que les premi√®res couches extraient des features simples (bords, textures) tandis que les couches profondes capturent des concepts complexes (parties du visage, expressions).
    
    \item \textbf{Dropout progressif (0.25 dans conv, 0.5‚Üí0.4 dans FC)} \\
    Le dropout dans les couches convolutives (0.25) r√©gularise sans trop perturber l'apprentissage spatial. Un dropout plus fort dans les FC (0.5) combat l'overfitting o√π le risque est le plus √©lev√©.
    
    \item \textbf{Global Average Pooling (GAP)} \\
    Remplace le flatten traditionnel. Avantages :
    \begin{itemize}
        \item R√©duit drastiquement les param√®tres ($512 \times 3 \times 3 = 4608 \rightarrow 512$)
        \item Agit comme r√©gularisateur
        \item Invariance spatiale accrue
    \end{itemize}
    
    \item \textbf{Initialisation Kaiming} \\
    Initialisation adapt√©e aux fonctions d'activation ReLU :
    \begin{equation}
        W \sim \mathcal{N}\left(0, \sqrt{\frac{2}{n_{in}}}\right)
    \end{equation}
\end{enumerate}

\begin{table}[H]
\centering
\caption{Comparaison des architectures}
\begin{tabular}{lcc}
\toprule
\textbf{Caract√©ristique} & \textbf{Initial} & \textbf{Am√©lior√©} \\
\midrule
Blocs convolutifs & 3 & 4 \\
Convolutions par bloc & 1 & 2 \\
Filtres max & 128 & 512 \\
Param√®tres totaux & $\approx$2.46M & $\approx$4.8M \\
Global Average Pooling & Non & Oui \\
Dropout conv & Non & Oui (0.25) \\
Batch Norm FC & Non & Oui \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Optimisations de l'Entra√Ænement}
%==============================================================================

\subsection{Data Augmentation}

La data augmentation est cruciale pour un petit dataset comme FER2013. Elle augmente artificiellement la diversit√© des donn√©es d'entra√Ænement.

\begin{lstlisting}[caption={Transformations de data augmentation}]
train_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(10),
    transforms.RandomAffine(
        degrees=0, 
        translate=(0.1, 0.1),
        scale=(0.9, 1.1)
    ),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
])
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Transformations de data augmentation}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Transformation} & \textbf{Justification} \\
\midrule
RandomHorizontalFlip (p=0.5) & Les expressions sont sym√©triques. Double effectivement le dataset. \\
RandomRotation (¬±10¬∞) & Simule les l√©g√®res inclinaisons de t√™te naturelles. \\
RandomAffine (translate) & Compense les variations de position du visage dans le cadre. \\
RandomAffine (scale 0.9-1.1) & Simule diff√©rentes distances cam√©ra-visage. \\
ColorJitter (brightness, contrast) & Robustesse aux variations d'√©clairage. \\
\bottomrule
\end{tabular}
\end{table}

\subsection{S√©paration Train/Validation}

\begin{lstlisting}[caption={S√©paration du dataset}]
VALIDATION_SPLIT = 0.15  # 15% pour validation

val_size = int(len(full_dataset) * VALIDATION_SPLIT)
train_size = len(full_dataset) - val_size

train_dataset, val_dataset = random_split(
    full_dataset, 
    [train_size, val_size],
    generator=torch.Generator().manual_seed(42)
)
\end{lstlisting}

\textbf{Importance :} La validation permet de :
\begin{itemize}
    \item D√©tecter l'overfitting (train loss $\downarrow$ mais val loss $\uparrow$)
    \item S√©lectionner le meilleur mod√®le
    \item Ajuster les hyperparam√®tres
\end{itemize}

\subsection{Early Stopping}

L'early stopping arr√™te l'entra√Ænement quand la validation ne s'am√©liore plus :

\begin{lstlisting}[caption={Impl√©mentation de l'early stopping}]
PATIENCE = 7
best_val_acc = 0.0
patience_counter = 0

for epoch in range(EPOCHS):
    # ... entrainement ...
    val_loss, val_acc = validate(model, val_loader, criterion, device)
    
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        patience_counter = 0
        torch.save(model.state_dict(), 'emotion_model_best.pth')
    else:
        patience_counter += 1
        if patience_counter >= PATIENCE:
            print("Early stopping triggered!")
            break
\end{lstlisting}

\subsection{Learning Rate Scheduler}

Le scheduler r√©duit le learning rate quand la validation stagne :

\begin{lstlisting}[caption={Learning rate scheduler}]
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='min',      # Surveille la loss
    factor=0.5,      # Divise LR par 2
    patience=3       # Attend 3 epochs sans amelioration
)

# Dans la boucle d'entrainement
scheduler.step(val_loss)
\end{lstlisting}

\textbf{Principe :} Un LR √©lev√© au d√©but permet une convergence rapide, puis un LR plus faible permet un affinage pr√©cis.

\subsection{Optimiseur AdamW}

\begin{lstlisting}[caption={Optimiseur AdamW avec weight decay}]
optimizer = optim.AdamW(
    model.parameters(), 
    lr=LEARNING_RATE, 
    weight_decay=1e-4
)
\end{lstlisting}

AdamW corrige un probl√®me de Adam : le weight decay est appliqu√© directement aux poids plut√¥t qu'au gradient, ce qui donne une meilleure r√©gularisation L2.

\subsection{Gradient Clipping}

\begin{lstlisting}[caption={Gradient clipping pour la stabilit√©}]
loss.backward()
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()
\end{lstlisting}

Emp√™che les gradients explosifs qui peuvent d√©stabiliser l'entra√Ænement.

%==============================================================================
\section{Optimisations de l'Application Temps R√©el}
%==============================================================================

\subsection{Lissage Temporel des Pr√©dictions}

Le probl√®me principal des pr√©dictions frame-par-frame est l'instabilit√© : l'√©motion affich√©e peut changer rapidement entre frames successives, cr√©ant un effet de \emph{flickering} d√©sagr√©able.

\begin{lstlisting}[caption={Lissage temporel avec moyenne pond√©r√©e}]
from collections import deque

SMOOTHING_WINDOW = 5
prediction_history = deque(maxlen=SMOOTHING_WINDOW)

def get_smoothed_prediction(current_probs):
    prediction_history.append(current_probs.cpu().numpy())
    
    if len(prediction_history) < 2:
        return current_probs
    
    # Moyenne ponderee (frames recentes = plus de poids)
    weights = np.linspace(0.5, 1.0, len(prediction_history))
    weights = weights / weights.sum()
    
    smoothed = np.zeros(7)
    for i, probs in enumerate(prediction_history):
        smoothed += weights[i] * probs
    
    return torch.tensor(smoothed)
\end{lstlisting}

\textbf{Principe :} Au lieu d'utiliser uniquement la pr√©diction de la frame courante, on calcule une moyenne pond√©r√©e sur les $N$ derni√®res frames. Les frames r√©centes ont plus de poids pour maintenir la r√©activit√©.

\subsection{√âgalisation d'Histogramme}

\begin{lstlisting}[caption={Normalisation de l'√©clairage}]
# Avant preprocessing
roi_gray = cv2.equalizeHist(roi_gray)
\end{lstlisting}

L'√©galisation d'histogramme normalise la distribution des niveaux de gris, rendant le mod√®le plus robuste aux variations d'√©clairage.

\begin{figure}[H]
\centering
\fbox{\parbox{0.8\textwidth}{\centering
\textbf{Image sombre} $\longrightarrow$ \textbf{Histogramme √©galis√©}
}}
\caption{L'√©galisation redistribue les intensit√©s sur toute la plage [0, 255]}
\end{figure}

\subsection{Param√®tres de D√©tection Optimis√©s}

\begin{lstlisting}[caption={Param√®tres de d√©tection de visage optimis√©s}]
faces = face_cascade.detectMultiScale(
    gray_frame, 
    scaleFactor=1.1,   # Plus precis (etait 1.3)
    minNeighbors=5,
    minSize=(48, 48),  # Taille minimum
    flags=cv2.CASCADE_SCALE_IMAGE
)
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Impact des param√®tres de d√©tection}
\begin{tabular}{lcc}
\toprule
\textbf{Param√®tre} & \textbf{Valeur basse} & \textbf{Valeur haute} \\
\midrule
scaleFactor & Plus pr√©cis, plus lent & Moins pr√©cis, plus rapide \\
minNeighbors & Plus de faux positifs & Moins de d√©tections \\
minSize & D√©tecte petits visages & Ignore petits visages \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Interface Utilisateur Am√©lior√©e}

\begin{itemize}
    \item \textbf{Couleurs par √©motion} : Chaque √©motion a une couleur distinctive
    \item \textbf{Barres de progression} : Affichage des probabilit√©s de toutes les classes
    \item \textbf{Fond pour le texte} : Am√©liore la lisibilit√©
\end{itemize}

\begin{lstlisting}[caption={Couleurs par √©motion}]
emotion_colors = {
    0: (0, 0, 255),     # Rouge - Angry
    1: (0, 128, 0),     # Vert fonce - Disgust
    2: (128, 0, 128),   # Violet - Fear
    3: (0, 255, 255),   # Jaune - Happy
    4: (255, 0, 0),     # Bleu - Sad
    5: (0, 165, 255),   # Orange - Surprise
    6: (128, 128, 128)  # Gris - Neutral
}
\end{lstlisting}

%==============================================================================
\section{Am√©lioration du Dataset}
%==============================================================================

\subsection{Avantages du Balanced AffectNet}

Par rapport √† FER2013, le dataset Balanced AffectNet pr√©sente plusieurs avantages majeurs :

\begin{enumerate}
    \item \textbf{R√©solution sup√©rieure} : Images 75$\times$75 pixels (vs 48$\times$48)
    \item \textbf{Images RGB} : 3 canaux de couleur offrant plus d'informations
    \begin{table}[H]
    \centering
    \begin{tabular}{lcc}
    \toprule
    \textbf{√âmotion} & \textbf{Nombre d'images} & \textbf{Pourcentage} \\
    \midrule
    Angry & 5,126 & 12.5\% \\
    Contempt & 5,126 & 12.5\% \\
    Disgust & 5,126 & 12.5\% \\
    Fear & 5,126 & 12.5\% \\
    Happy & 5,126 & 12.5\% \\
    Neutral & 5,126 & 12.5\% \\
    Sad & 5,126 & 12.5\% \\
    Surprise & 5,126 & 12.5\% \\
    \bottomrule
    \end{tabular}
    \caption{Distribution parfaitement √©quilibr√©e des classes dans Balanced AffectNet.}
    \end{table}
    
    \item \textbf{Dataset √©quilibr√©} : Pas de biais de classe, contrairement √† FER2013
    \item \textbf{8 classes natives} : Inclut Contempt (m√©pris) d√®s le d√©part
    \item \textbf{Meilleure qualit√©} : Annotations plus fiables
\end{enumerate}

\subsection{Chargement du Dataset}

Le dataset Balanced AffectNet est organis√© en dossiers par √©motion :

\begin{lstlisting}[caption={Utilisation du dataset AffectNet}]
class BalancedAffectNetDataset(Dataset):
    EMOTION_CLASSES = {
        'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3,
        'Sad': 4, 'Surprise': 5, 'Neutral': 6, 'Contempt': 7,
    }
    
    def __init__(self, root_dir, split='train', transform=None):
        self.images = []
        self.labels = []
        
        for emotion_name, emotion_idx in self.EMOTION_CLASSES.items():
            emotion_dir = os.path.join(root_dir, split, emotion_name)
            for img_name in os.listdir(emotion_dir):
                self.images.append(os.path.join(emotion_dir, img_name))
                self.labels.append(emotion_idx)
    
    def __getitem__(self, idx):
        image = Image.open(self.images[idx]).convert('RGB')
        image = np.array(image)  # 75x75x3
        label = self.labels[idx]
        
        if self.transform:
            image = self.transform(image=image)['image']
        
        return image, label
\end{lstlisting}

\subsection{√âquilibrage des Classes}

Pour compenser le d√©s√©quilibre (notamment le manque de \textit{Disgust}), deux techniques sont impl√©ment√©es :

\subsubsection{Poids de classe dans la loss}

\begin{lstlisting}[caption={CrossEntropyLoss avec poids de classe}]
def get_class_weights(dataset):
    class_counts = np.bincount(labels, minlength=7)
    weights = 1.0 / class_counts
    weights = weights / weights.sum() * len(weights)
    return torch.FloatTensor(weights)

class_weights = get_class_weights(train_dataset)
criterion = nn.CrossEntropyLoss(weight=class_weights)
\end{lstlisting}

Le poids de chaque classe est inversement proportionnel √† sa fr√©quence :
\begin{equation}
    w_c = \frac{N}{N_c \times C}
\end{equation}
o√π $N$ est le nombre total d'√©chantillons, $N_c$ le nombre d'√©chantillons de la classe $c$, et $C$ le nombre de classes.

\subsubsection{WeightedRandomSampler}

\begin{lstlisting}[caption={Sampler √©quilibr√©}]
def get_balanced_sampler(dataset):
    class_counts = np.bincount(labels, minlength=7)
    weights = 1.0 / class_counts
    sample_weights = weights[labels]
    
    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )
    return sampler
\end{lstlisting}

Le sampler sur-√©chantillonne les classes minoritaires pendant l'entra√Ænement.

%==============================================================================
\section{Techniques d'Entra√Ænement Avanc√©es}
%==============================================================================

Pour am√©liorer significativement les performances, notamment sur les classes difficiles (Disgust, Contempt, Fear, Angry), nous avons d√©velopp√© un script d'entra√Ænement avanc√© (\texttt{train\_advanced.py}) int√©grant les techniques les plus r√©centes du deep learning.

\subsection{Focal Loss pour le D√©s√©quilibre des Classes}

La Cross-Entropy standard traite tous les exemples de mani√®re √©gale. La \textbf{Focal Loss} \cite{lin2017focal} ajoute un facteur de modulation qui r√©duit la contribution des exemples faciles et se concentre sur les exemples difficiles :

\begin{equation}
    \text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
\end{equation}

o√π $p_t$ est la probabilit√© pr√©dite pour la vraie classe, $\alpha_t$ est le poids de la classe, et $\gamma$ (gamma) contr√¥le le focus sur les exemples difficiles.

\begin{lstlisting}[caption={Impl√©mentation de la Focal Loss}]
class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha  # Poids par classe
        self.gamma = gamma  # Focus parameter (2.0 recommande)
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, 
                                  weight=self.alpha, 
                                  reduction='none')
        pt = torch.exp(-ce_loss)  # p_t
        focal_loss = ((1 - pt) ** self.gamma) * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        return focal_loss
\end{lstlisting}

\textbf{Avantage :} Avec $\gamma = 2$, un exemple bien classifi√© ($p_t = 0.9$) contribue 100√ó moins qu'un exemple difficile ($p_t = 0.1$), permettant au mod√®le de se concentrer sur les √©motions sous-repr√©sent√©es comme Disgust.

\subsection{Mixup : R√©gularisation par Interpolation}

Mixup \cite{zhang2018mixup} cr√©e de nouveaux exemples d'entra√Ænement en interpolant lin√©airement des paires d'images et leurs labels :

\begin{equation}
    \tilde{x} = \lambda x_i + (1 - \lambda) x_j
\end{equation}
\begin{equation}
    \tilde{y} = \lambda y_i + (1 - \lambda) y_j
\end{equation}

o√π $\lambda \sim \text{Beta}(\alpha, \alpha)$ avec $\alpha = 0.2$.

\begin{lstlisting}[caption={Impl√©mentation de Mixup}]
def mixup_data(x, y, alpha=0.2):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)
\end{lstlisting}

\subsection{CutMix : Augmentation par D√©coupage}

CutMix \cite{yun2019cutmix} est une variante de Mixup qui d√©coupe et colle des r√©gions rectangulaires entre images :

\begin{lstlisting}[caption={Impl√©mentation de CutMix}]
def cutmix_data(x, y, alpha=1.0):
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    
    # Calcul de la boite de decoupe
    W, H = x.size(2), x.size(3)
    cut_rat = np.sqrt(1. - lam)
    cut_w = int(W * cut_rat)
    cut_h = int(H * cut_rat)
    
    cx = np.random.randint(W)
    cy = np.random.randint(H)
    
    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)
    
    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]
    
    # Ajuster lambda selon la surface decoupee
    lam = 1 - ((bbx2-bbx1)*(bby2-bby1) / (W*H))
    return x, y, y[index], lam
\end{lstlisting}

\textbf{Avantage :} CutMix force le mod√®le √† utiliser des indices partiels du visage (un ≈ìil, la bouche seule) plut√¥t que de d√©pendre de l'image enti√®re, am√©liorant la robustesse.

\subsection{Label Smoothing}

Le label smoothing r√©gularise en rempla√ßant les labels hard (one-hot) par des labels soft :

\begin{equation}
    y_\text{smooth} = (1 - \epsilon) \cdot y_\text{hard} + \frac{\epsilon}{K}
\end{equation}

avec $\epsilon = 0.1$ et $K = 8$ classes.

\begin{lstlisting}[caption={Application du Label Smoothing}]
class LabelSmoothingLoss(nn.Module):
    def __init__(self, classes, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
        self.cls = classes
        
    def forward(self, pred, target):
        confidence = 1.0 - self.smoothing
        smooth_val = self.smoothing / (self.cls - 1)
        
        one_hot = torch.zeros_like(pred)
        one_hot.fill_(smooth_val)
        one_hot.scatter_(1, target.unsqueeze(1), confidence)
        
        log_prob = F.log_softmax(pred, dim=1)
        return -(one_hot * log_prob).sum(dim=1).mean()
\end{lstlisting}

\subsection{Augmentation Avanc√©e avec Albumentations}

Nous utilisons la biblioth√®que Albumentations pour des transformations plus sophistiqu√©es :

\begin{lstlisting}[caption={Transformations Albumentations}]
import albumentations as A

train_transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(
        shift_limit=0.1,
        scale_limit=0.15,
        rotate_limit=15,
        p=0.5
    ),
    A.OneOf([
        A.MotionBlur(blur_limit=3, p=1.0),
        A.GaussianBlur(blur_limit=3, p=1.0),
        A.GaussNoise(var_limit=(10, 50), p=1.0),
    ], p=0.3),
    A.OneOf([
        A.RandomBrightnessContrast(
            brightness_limit=0.2,
            contrast_limit=0.2,
            p=1.0
        ),
        A.CLAHE(clip_limit=2.0, p=1.0),
    ], p=0.5),
    A.CoarseDropout(
        max_holes=1,
        max_height=12,
        max_width=12,
        fill_value=0,
        p=0.25
    ),
])
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Nouvelles transformations Albumentations}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Transformation} & \textbf{Justification} \\
\midrule
MotionBlur / GaussianBlur & Simule le flou de mouvement en conditions r√©elles \\
GaussNoise & Robustesse au bruit de capteur \\
CLAHE & Am√©liore le contraste local, meilleur que l'√©galisation standard \\
CoarseDropout & Similaire √† Cutout, force la redondance des features \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Gradient Accumulation}

Pour simuler des batch sizes plus grands sur GPU avec m√©moire limit√©e :

\begin{lstlisting}[caption={Gradient Accumulation}]
ACCUMULATION_STEPS = 4  # Effective batch = 32 * 4 = 128

for i, (inputs, targets) in enumerate(train_loader):
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss = loss / ACCUMULATION_STEPS
    loss.backward()
    
    if (i + 1) % ACCUMULATION_STEPS == 0:
        torch.nn.utils.clip_grad_norm_(
            model.parameters(), max_norm=1.0
        )
        optimizer.step()
        optimizer.zero_grad()
\end{lstlisting}

\textbf{Avantage :} Un batch effectif de 128 images stabilise les gradients et am√©liore la convergence.

\subsection{Cosine Annealing avec Warm Restarts}

Scheduler qui suit une courbe cosinus avec red√©marrages p√©riodiques :

\begin{lstlisting}[caption={CosineAnnealingWarmRestarts}]
scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer,
    T_0=10,     # Premiere periode de 10 epochs
    T_mult=2,   # Periodes suivantes x2 (10, 20, 40...)
    eta_min=1e-6
)
\end{lstlisting}

\begin{figure}[H]
\centering
\fbox{\parbox{0.8\textwidth}{\centering
Learning Rate suit une courbe cosinus : \\
$\eta_t = \eta_\text{min} + \frac{1}{2}(\eta_\text{max} - \eta_\text{min})(1 + \cos(\frac{T_\text{cur}}{T_i}\pi))$
}}
\caption{Le learning rate diminue selon une courbe cosinus puis remonte √† chaque restart}
\end{figure}

%==============================================================================
\section{Application √âtendue avec MediaPipe}
%==============================================================================

Pour am√©liorer la d√©tection des √©motions difficiles et enrichir le panel d'emojis affichables, nous avons int√©gr√© \textbf{MediaPipe}, la biblioth√®que de Google pour l'analyse faciale et gestuelle en temps r√©el.

\subsection{MediaPipe Face Mesh : Analyse des Landmarks Faciaux}

Face Mesh d√©tecte 468 points de rep√®re 3D sur le visage, permettant une analyse fine des caract√©ristiques faciales :

\begin{lstlisting}[caption={Initialisation de Face Mesh}]
import mediapipe as mp

mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,  # Points supplementaires iris
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)
\end{lstlisting}

\subsubsection{Features Extraites}

\begin{lstlisting}[caption={Classe FacialAnalyzer pour l'extraction de features}]
class FacialAnalyzer:
    # Indices des landmarks cles
    LEFT_EYE = [33, 160, 158, 133, 153, 144]
    RIGHT_EYE = [362, 385, 387, 263, 373, 380]
    MOUTH = [61, 291, 0, 17, 78, 308]
    LEFT_EYEBROW = [70, 63, 105, 66, 107]
    RIGHT_EYEBROW = [336, 296, 334, 293, 300]
    
    def analyze(self, landmarks):
        features = {}
        
        # Ouverture des yeux (Eye Aspect Ratio)
        features['left_eye_open'] = self._eye_aspect_ratio(
            [landmarks[i] for i in self.LEFT_EYE]
        )
        features['right_eye_open'] = self._eye_aspect_ratio(
            [landmarks[i] for i in self.RIGHT_EYE]
        )
        
        # Ouverture de la bouche
        features['mouth_open'] = self._mouth_aspect_ratio(landmarks)
        
        # Position des sourcils
        features['brow_raise'] = self._brow_raise(landmarks)
        features['brow_squeeze'] = self._brow_squeeze(landmarks)
        
        # Sourire (ratio largeur/hauteur bouche)
        features['smile'] = self._smile_ratio(landmarks)
        
        return features
    
    def _eye_aspect_ratio(self, eye_points):
        # Ratio vertical/horizontal pour detecter clignement
        vertical = np.linalg.norm(
            np.array(eye_points[1]) - np.array(eye_points[5])
        )
        horizontal = np.linalg.norm(
            np.array(eye_points[0]) - np.array(eye_points[3])
        )
        return vertical / (horizontal + 1e-6)
\end{lstlisting}

\subsubsection{Boost des √âmotions Difficiles}

Les features faciales sont utilis√©es pour renforcer la d√©tection des √©motions que le CNN a du mal √† identifier :

\begin{lstlisting}[caption={Syst√®me de boost des √©motions}]
EMOTION_BOOST = {
    'Angry': 1.4,     # Boost de 40%
    'Disgust': 1.5,   # Boost de 50%
    'Fear': 1.3,      # Boost de 30%
    'Contempt': 1.4,  # Boost de 40%
    'Neutral': 0.85   # Reduction de 15%
}

def boost_emotions(probs, features):
    boosted = probs.copy()
    
    # Boost Angry si sourcils fronces
    if features.get('brow_squeeze', 0) > 0.6:
        boosted[ANGRY_IDX] *= 1.3
    
    # Boost Disgust si nez fronce
    if features.get('nose_wrinkle', 0) > 0.5:
        boosted[DISGUST_IDX] *= 1.4
    
    # Boost Fear si yeux grands ouverts
    avg_eye = (features['left_eye_open'] + 
               features['right_eye_open']) / 2
    if avg_eye > 0.4:
        boosted[FEAR_IDX] *= 1.2
    
    return boosted / boosted.sum()  # Re-normaliser
\end{lstlisting}

\subsection{Extension du Mapping Emoji}

Au-del√† des 8 √©motions de base, les features faciales permettent d'afficher des emojis plus nuanc√©s :

\begin{lstlisting}[caption={Mapping emoji √©tendu}]
EXTENDED_EMOJI_MAP = {
    # Emotions de base
    'Happy': 'üòä',
    'Sad': 'üò¢',
    'Angry': 'üò†',
    'Surprise': 'üò≤',
    'Fear': 'üò®',
    'Disgust': 'ü§¢',
    'Neutral': 'üòê',
    'Contempt': 'üòè',
    
    # Combinaisons basees sur features
    'very_happy': 'üòÅ',      # Sourire large
    'laughing': 'üòÇ',        # Bouche ouverte + sourire
    'thinking': 'ü§î',        # Un sourcil leve
    'sleepy': 'üò¥',          # Yeux fermes
    'wink': 'üòâ',            # Un oeil ferme
    'kiss': 'üòò',            # Bouche en O
    'love': 'üòç',            # Coeurs si smile fort
    'cool': 'üòé',            # Neutre + confiant
    'skeptical': 'ü§®',       # Sourcil asymetrique
}

def get_extended_emoji(emotion, features):
    # Verifier conditions speciales
    if features['left_eye_open'] < 0.15:
        if features['right_eye_open'] > 0.25:
            return EXTENDED_EMOJI_MAP['wink']
    
    if features['smile'] > 0.7 and features['mouth_open'] > 0.4:
        return EXTENDED_EMOJI_MAP['laughing']
    
    if emotion == 'Happy' and features['smile'] > 0.8:
        return EXTENDED_EMOJI_MAP['very_happy']
    
    return EXTENDED_EMOJI_MAP.get(emotion, '‚ùì')
\end{lstlisting}

\subsection{MediaPipe Hands : D√©tection des Gestes}

Pour enrichir l'interaction, nous avons ajout√© la d√©tection des gestes de la main :

\begin{lstlisting}[caption={Initialisation de MediaPipe Hands}]
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.5
)
\end{lstlisting}

\subsubsection{Gestes Reconnus}

\begin{lstlisting}[caption={Classe HandRecognizer}]
class HandRecognizer:
    def recognize_gesture(self, landmarks):
        # Extraire les etats des doigts (leve/baisse)
        fingers = self._get_finger_states(landmarks)
        thumb, index, middle, ring, pinky = fingers
        
        # Pouce leve
        if thumb and not any([index, middle, ring, pinky]):
            return 'thumbs_up', 'üëç'
        
        # Signe de paix (V)
        if index and middle and not ring and not pinky:
            return 'peace', '‚úåÔ∏è'
        
        # Signe OK
        if self._is_ok_gesture(landmarks):
            return 'ok', 'üëå'
        
        # Rock (cornes)
        if index and pinky and not middle and not ring:
            return 'rock', 'ü§ò'
        
        # Shaka (pouce + auriculaire)
        if thumb and pinky and not index and not middle:
            return 'shaka', 'ü§ô'
        
        # Poing ferme
        if not any(fingers):
            return 'fist', '‚úä'
        
        # Main ouverte (tous les doigts)
        if all(fingers):
            return 'wave', 'üëã'
        
        # Pointer
        if index and not middle and not ring and not pinky:
            return 'point', 'üëÜ'
        
        return None, None
    
    def _get_finger_states(self, landmarks):
        # Comparer position des bouts vs articulations
        thumb = landmarks[4].y < landmarks[3].y
        index = landmarks[8].y < landmarks[6].y
        middle = landmarks[12].y < landmarks[10].y
        ring = landmarks[16].y < landmarks[14].y
        pinky = landmarks[20].y < landmarks[18].y
        return [thumb, index, middle, ring, pinky]
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Gestes reconnus et emojis correspondants}
\begin{tabular}{lcc}
\toprule
\textbf{Geste} & \textbf{Description} & \textbf{Emoji} \\
\midrule
Thumbs Up & Pouce lev√© seul & üëç \\
Peace & Index et majeur lev√©s & ‚úåÔ∏è \\
OK & Pouce et index formant un cercle & üëå \\
Rock & Index et auriculaire lev√©s & ü§ò \\
Shaka & Pouce et auriculaire & ü§ô \\
Wave & Tous les doigts lev√©s & üëã \\
Fist & Poing ferm√© & ‚úä \\
Point & Index seul lev√© & üëÜ \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Optimisations de l'Inf√©rence (\texttt{app\_v3.py})}
%==============================================================================

La version 3 de l'application int√®gre plusieurs optimisations pour am√©liorer la pr√©cision et la stabilit√© des pr√©dictions.

\subsection{Test-Time Augmentation (TTA)}

Le TTA applique plusieurs transformations √† l'image d'entr√©e et moyenne les pr√©dictions :

\begin{lstlisting}[caption={Impl√©mentation du TTA}]
class EmotionClassifier:
    def predict_with_tta(self, face_roi, num_augmentations=3):
        predictions = []
        
        # Prediction originale
        predictions.append(self._predict_single(face_roi))
        
        # Flip horizontal
        flipped = cv2.flip(face_roi, 1)
        predictions.append(self._predict_single(flipped))
        
        # Legeres variations de luminosite
        for _ in range(num_augmentations - 2):
            factor = np.random.uniform(0.9, 1.1)
            adjusted = np.clip(face_roi * factor, 0, 255)
            predictions.append(
                self._predict_single(adjusted.astype(np.uint8))
            )
        
        # Moyenne des predictions
        return np.mean(predictions, axis=0)
\end{lstlisting}

\textbf{Avantage :} Le TTA r√©duit la variance des pr√©dictions et am√©liore la robustesse aux petites perturbations.

\subsection{Pr√©traitement CLAHE}

Contrairement √† l'√©galisation d'histogramme globale, CLAHE (Contrast Limited Adaptive Histogram Equalization) travaille sur des r√©gions locales :

\begin{lstlisting}[caption={Application de CLAHE}]
def preprocess_face(self, face_roi):
    # Redimensionner
    face = cv2.resize(face_roi, (48, 48))
    
    # CLAHE pour le contraste local
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))
    face = clahe.apply(face)
    
    # Normalisation
    face = face.astype(np.float32) / 255.0
    
    return face
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Comparaison des m√©thodes d'√©galisation}
\begin{tabular}{lp{5cm}p{5cm}}
\toprule
\textbf{M√©thode} & \textbf{Avantages} & \textbf{Inconv√©nients} \\
\midrule
Histogramme global & Simple, rapide & Peut sur-amplifier le bruit \\
CLAHE & Pr√©serve les d√©tails locaux, contr√¥le le contraste & L√©g√®rement plus lent \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Lissage Temporel Am√©lior√©}

La version 3 utilise un lissage exponentiel plus sophistiqu√© :

\begin{lstlisting}[caption={Lissage temporel avec pond√©ration exponentielle}]
SMOOTHING_WINDOW = 7

def get_smoothed_prediction(self, current_probs):
    self.prediction_history.append(current_probs)
    
    if len(self.prediction_history) < 2:
        return current_probs
    
    # Poids exponentiels (frames recentes plus importantes)
    n = len(self.prediction_history)
    weights = np.exp(np.linspace(-1, 0, n))
    weights = weights / weights.sum()
    
    smoothed = np.zeros(8)
    for i, probs in enumerate(self.prediction_history):
        smoothed += weights[i] * probs
    
    return smoothed
\end{lstlisting}

\subsection{Architecture Modulaire}

L'application V3 s√©pare clairement les responsabilit√©s :

\begin{lstlisting}[caption={Architecture modulaire de app\_v3.py}]
class EmotionClassifier:
    """Gestion du modele CNN et predictions"""
    pass

class FacialAnalyzer:
    """Analyse des landmarks faciaux avec MediaPipe"""
    pass

class HandRecognizer:
    """Detection des gestes de la main"""
    pass

class EmotionApp:
    """Application principale orchestrant les composants"""
    def __init__(self):
        self.classifier = EmotionClassifier()
        self.facial_analyzer = FacialAnalyzer()
        self.hand_recognizer = HandRecognizer()
    
    def process_frame(self, frame):
        # 1. Detecter visage
        # 2. Analyser features faciales
        # 3. Predire emotion avec CNN + TTA
        # 4. Boost avec features
        # 5. Detecter gestes
        # 6. Afficher resultats
        pass
\end{lstlisting}

\subsection{Datasets Alternatifs}

Pour des performances encore meilleures, d'autres datasets sont support√©s :

\begin{table}[H]
\centering
\caption{Comparaison des datasets d'√©motions faciales}
\begin{tabular}{lcccl}
\toprule
\textbf{Dataset} & \textbf{Images} & \textbf{R√©solution} & \textbf{Classes} & \textbf{Acc√®s} \\
\midrule
FER2013 & 35,887 & 48$\times$48 (Gray) & 7 & Gratuit (Kaggle) \\
FER+ & 35,887 & 48$\times$48 (Gray) & 8 & Gratuit (GitHub) \\
\textbf{Balanced AffectNet} & \textbf{41,008} & \textbf{75$\times$75 (RGB)} & \textbf{8} & \textbf{Gratuit (Kaggle)} \\
AffectNet (original) & 450,000 & Variable & 8 & Demande requise \\
RAF-DB & 30,000 & 100$\times$100 & 7 & Demande requise \\
\bottomrule
\end{tabular}
\end{table}

\begin{lstlisting}[caption={Support multi-datasets}]
class AffectNetDataset(Dataset):
    # Mapping AffectNet vers les 7 classes FER
    AFFECTNET_TO_FER = {
        0: 6,  # Neutral
        1: 3,  # Happy
        2: 4,  # Sad
        3: 5,  # Surprise
        4: 2,  # Fear
        5: 1,  # Disgust
        6: 0,  # Anger
        7: 6,  # Contempt -> Neutral
    }
    
    def __getitem__(self, idx):
        image = Image.open(self.images[idx]).convert('L')
        image = image.resize((48, 48))
        label = self.AFFECTNET_TO_FER[self.labels[idx]]
        return image, label
\end{lstlisting}

%==============================================================================
\section{R√©sultats Attendus}
%==============================================================================

\subsection{Am√©lioration de la Pr√©cision}

\begin{table}[H]
\centering
\caption{Am√©lioration attendue de la pr√©cision avec Balanced AffectNet}
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{Pr√©cision estim√©e} & \textbf{Gain} \\
\midrule
FER2013 baseline (grayscale 48$\times$48) & 60-65\% & - \\
Balanced AffectNet (RGB 75$\times$75) & 70-75\% & +10\% \\
+ Data augmentation RGB & 73-78\% & +3\% \\
+ Mixup/CutMix & 76-80\% & +3\% \\
+ Label Smoothing & 78-82\% & +2\% \\
+ TTA √† l'inf√©rence & 80-84\% & +2\% \\
\midrule
\textbf{Total avec optimisations} & \textbf{80-84\%} & \textbf{+20-24\%} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Am√©lioration des Classes Difficiles}

\begin{table}[H]
\centering
\caption{Impact attendu sur les classes (avec dataset √©quilibr√©)}
\begin{tabular}{lccc}
\toprule
\textbf{√âmotion} & \textbf{FER2013 Baseline} & \textbf{Balanced AffectNet} & \textbf{Gain} \\
\midrule
Angry & 55\% & 75\% & +20\% \\
Disgust & 35\% & 72\% & +37\% \\
Fear & 45\% & 70\% & +25\% \\
Contempt & 40\% & 68\% & +28\% \\
\bottomrule
\end{tabular}
\end{table}

\textit{Note : L'am√©lioration majeure sur Disgust et Contempt est due au dataset parfaitement √©quilibr√© et aux images RGB de meilleure qualit√©.}

\subsection{Am√©lioration de l'Exp√©rience Utilisateur}

\begin{itemize}
    \item \textbf{Stabilit√©} : Le lissage temporel am√©lior√© √©limine le \emph{flickering}
    \item \textbf{Robustesse} : CLAHE g√®re mieux les variations d'√©clairage que l'√©galisation standard
    \item \textbf{Pr√©cision TTA} : Le test-time augmentation r√©duit la variance des pr√©dictions
    \item \textbf{Emojis √©tendus} : Plus de 15 emojis possibles gr√¢ce √† l'analyse des features faciales
    \item \textbf{Gestes de la main} : 8 gestes reconnus avec emojis correspondants
    \item \textbf{Lisibilit√©} : Les couleurs et barres de progression am√©liorent la compr√©hension
    \item \textbf{R√©activit√©} : Les param√®tres de d√©tection optimis√©s am√©liorent la fluidit√©
\end{itemize}

%==============================================================================
\section{Conclusion}
%==============================================================================

Ce projet a permis de d√©velopper un syst√®me complet et avanc√© de reconnaissance d'√©motions faciales en temps r√©el. Les principales contributions sont :

\begin{enumerate}
    \item \textbf{Architecture CNN optimis√©e} avec double convolution, dropout progressif, et Global Average Pooling
    
    \item \textbf{Pipeline d'entra√Ænement avanc√©} incluant :
    \begin{itemize}
        \item Focal Loss pour le d√©s√©quilibre des classes
        \item Mixup et CutMix pour la r√©gularisation
        \item Label Smoothing pour la g√©n√©ralisation
        \item Augmentations avanc√©es avec Albumentations
    \end{itemize}
    
    \item \textbf{Int√©gration MediaPipe} pour :
    \begin{itemize}
        \item Analyse des 468 landmarks faciaux (Face Mesh)
        \item D√©tection de 8 gestes de la main
        \item Boost contextuel des √©motions difficiles
    \end{itemize}
    
    \item \textbf{Application temps r√©el optimis√©e (V3)} avec :
    \begin{itemize}
        \item Test-Time Augmentation (TTA)
        \item Pr√©traitement CLAHE
        \item Lissage temporel exponentiel
        \item Mapping emoji √©tendu (15+ emojis)
    \end{itemize}
    
    \item \textbf{Support multi-datasets} permettant d'utiliser FER+, AffectNet, ou RAF-DB
\end{enumerate}

\subsection{Perspectives}

Pour aller plus loin, les am√©liorations suivantes pourraient √™tre envisag√©es :

\begin{itemize}
    \item Utilisation de transfer learning (VGGFace, ResNet pr√©-entra√Æn√© sur visages)
    \item Architecture avec attention mechanism (Transformer, CBAM)
    \item D√©tection multi-t√¢ches (√©motion + √¢ge + genre)
    \item Analyse de la valence et de l'arousal (mod√®le dimensionnel)
    \item D√©ploiement sur edge devices (quantification INT8, pruning)
    \item Int√©gration d'un mod√®le audio pour l'analyse multimodale
\end{itemize}

%==============================================================================
\appendix
\section{Structure du Projet}
%==============================================================================

\begin{verbatim}
Final_project/
|-- app.py                   # Application temps reel (version initiale)
|-- app_extended.py          # Version avec MediaPipe Face Mesh
|-- app_extended_v2.py       # Version avec gestes de la main
|-- app_v3.py                # Version optimisee (TTA, CLAHE, modulaire)
|-- model.py                 # Architecture CNN (RGB 75x75)
|-- train_affectnet.py       # Entrainement sur Balanced AffectNet
|-- train.py                 # Script d'entrainement (legacy FER2013)
|-- dataset_affectnet.py     # Dataset Balanced AffectNet
|-- dataset.py               # Dataset FER2013 (legacy)
|-- download_datasets.py     # Script de telechargement des datasets
|-- emotion_model.pth        # Poids du modele entraine
|-- emotion_model_best.pth   # Meilleur modele (early stopping)
|-- data/
|   +-- affectnet/           # Dataset Balanced AffectNet
|       |-- train/
|       |   |-- Anger/
|       |   |-- Contempt/
|       |   |-- Disgust/
|       |   |-- Fear/
|       |   |-- Happy/
|       |   |-- Neutral/
|       |   |-- Sad/
|       |   +-- Surprise/
|       |-- val/
|       +-- test/
|-- report/
|   +-- report.tex           # Ce rapport
+-- README.md
\end{verbatim}

%==============================================================================
\section{R√©f√©rences}
%==============================================================================

\begin{enumerate}
    \item Goodfellow, I. J., et al. (2013). "Challenges in representation learning: A report on three machine learning contests." \textit{ICML Workshop}.
    
    \item Barsoum, E., et al. (2016). "Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution." \textit{ACM ICMI}.
    
    \item Mollahosseini, A., et al. (2017). "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild." \textit{IEEE Trans. Affective Computing}.
    
    \item Li, S., et al. (2017). "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild." \textit{CVPR}.
    
    \item He, K., et al. (2015). "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification." \textit{ICCV}.
    
    \item Lin, T. Y., et al. (2017). "Focal Loss for Dense Object Detection." \textit{ICCV}. \label{lin2017focal}
    
    \item Zhang, H., et al. (2018). "mixup: Beyond Empirical Risk Minimization." \textit{ICLR}. \label{zhang2018mixup}
    
    \item Yun, S., et al. (2019). "CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features." \textit{ICCV}. \label{yun2019cutmix}
    
    \item Lugaresi, C., et al. (2019). "MediaPipe: A Framework for Building Perception Pipelines." \textit{arXiv preprint arXiv:1906.08172}.
    
    \item Buslaev, A., et al. (2020). "Albumentations: Fast and Flexible Image Augmentations." \textit{Information}, 11(2), 125.
\end{enumerate}

\end{document}
