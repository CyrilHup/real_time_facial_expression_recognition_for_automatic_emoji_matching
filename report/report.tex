\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{float}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}

\geometry{margin=2.5cm}
\setlength{\headheight}{13.6pt}
\addtolength{\topmargin}{-1.6pt}

% Code styling
\lstset{
    basicstyle=\ttfamily\small,
    backgroundcolor=\color{gray!10},
    frame=single,
    breaklines=true,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red!60!black}
}

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\rhead{Deep Learning Final Project}
\lhead{Emoji Recognition System}
\rfoot{Page \thepage}

\title{
    \vspace{-1cm}
    \textbf{Real-Time Emoji Recognition System}\\
    \large Using Convolutional Neural Networks and MediaPipe\\
    \vspace{0.5cm}
    \normalsize Deep Learning -- Final Project Report
}

\author{
    POSTECH -- Deep Learning Course\\
    December 2025
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
This report presents the development of a real-time emoji recognition system that combines facial emotion recognition using Convolutional Neural Networks (CNN) with hand gesture detection via MediaPipe. The project evolved significantly from the initial proposal, ultimately achieving a comprehensive application capable of mapping detected emotions and gestures to appropriate emojis. We detail our organizational approach, the iterative development process, and the technical implementation including model architecture, multi-dataset training, and ONNX optimization for deployment.
\end{abstract}

\tableofcontents
\newpage

%==============================================================================
\section{Project Organization}
%==============================================================================

Effective organization was crucial for the success of this project. We employed a combination of cloud computing resources, local development environments, and version control to ensure smooth collaboration and efficient progress.

\subsection{Training Infrastructure}

For model training, we chose \textbf{Google Colaboratory} over alternatives such as Vast.ai. This decision was driven by several factors:

\begin{itemize}[noitemsep]
    \item \textbf{Platform familiarity}: Our team had extensive prior experience with Google Colab from previous projects, significantly reducing the learning curve and setup time.
    \item \textbf{Collaborative features}: Colab's notebook sharing capabilities facilitated real-time collaboration on training code, allowing multiple team members to contribute and experiment simultaneously.
    \item \textbf{Integrated GPU access}: The seamless access to NVIDIA GPUs (T4) without complex configuration enabled rapid prototyping and experimentation.
    \item \textbf{Integration with Google Drive}: Easy storage and retrieval of datasets, checkpoints, and trained models streamlined our workflow.
\end{itemize}

The training workflow was centralized in a Jupyter notebook (\texttt{training.ipynb}) that encapsulates all aspects of model training, from data loading to hyperparameter configuration and result visualization.

\subsection{Development Environment}

For non-training code (application development, model export, utilities), we adopted a more traditional development approach:

\begin{itemize}[noitemsep]
    \item \textbf{IDE}: Visual Studio Code served as our primary development environment, providing excellent Python support, debugging capabilities, and integrated terminal access.
    \item \textbf{Local execution}: The real-time application (\texttt{app\_v4.py}) and utility scripts were developed and tested locally to ensure compatibility with webcam hardware and real-time performance requirements.
    \item \textbf{Version control}: GitHub was used for code versioning, enabling collaborative development, code review, and maintaining a clear history of changes throughout the project lifecycle.
\end{itemize}

\subsection{Workflow Summary}

\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Task} & \textbf{Platform} & \textbf{Tools} \\
\midrule
Model Training & Google Colab & Jupyter Notebook, PyTorch \\
Dataset Management & Google Drive / Kaggle & kagglehub \\
Application Development & Local (Windows) & VS Code, Python \\
Version Control & Cloud & GitHub \\
Real-time Testing & Local & OpenCV, Webcam \\
\bottomrule
\end{tabular}
\caption{Development workflow organization}
\end{table}

%==============================================================================
\section{Project Evolution}
%==============================================================================

The development of this project followed an iterative path that diverged significantly from our initial proposal. This section chronicles the evolution of our approach and the key decisions that shaped the final system.

\subsection{Departure from the Initial Proposal}

Our original project proposal envisioned implementing a \textbf{Multi-Layer Perceptron (MLP)} for emotion recognition. However, early research and experimentation led us to reconsider this approach:

\begin{itemize}[noitemsep]
    \item Image classification tasks benefit greatly from the spatial feature extraction capabilities of CNNs.
    \item State-of-the-art results in facial emotion recognition consistently employ convolutional architectures.
    \item The hierarchical feature learning of CNNs is particularly suited for detecting subtle facial expressions.
\end{itemize}

As a result, we made an early decision to \textbf{directly implement CNN-based architectures}, bypassing MLP entirely. This pivot proved to be a defining factor in the project's success.

\subsection{Dataset Journey}

The evolution of our training data strategy reflects our commitment to continuous improvement:

\subsubsection{Phase 1: FER2013}
Initially, we trained exclusively on the \textbf{FER2013} dataset, a widely-used benchmark containing approximately 35,000 grayscale images (48×48 pixels) across 7 emotion categories. While this provided a solid starting point, we quickly identified limitations:
\begin{itemize}[noitemsep]
    \item Noisy labels leading to inconsistent training signals
    \item Limited to 7 emotion classes (no ``Contempt'' category)
    \item Small image resolution constraining feature extraction
\end{itemize}

\subsubsection{Phase 2: Discovery of AffectNet and FER+}
Through research, we discovered two additional datasets that significantly enhanced our training:

\begin{itemize}
    \item \textbf{AffectNet}: A larger dataset with RGB images (75×75) and 8 emotion classes. The higher resolution and color information provided richer features for training.
    \item \textbf{FER+}: Microsoft's re-annotation of FER2013 with crowd-sourced labels from 10 annotators, reducing label noise and adding the ``Contempt'' class.
\end{itemize}

We adapted our training pipeline to support these datasets, implementing a unified data loading system (see Section~\ref{sec:datasets}for technical details).

\subsection{Model Refinement Process}

Throughout the project, we continuously refined our training approach. Key improvements included:

\begin{enumerate}[noitemsep]
    \item \textbf{Architecture enhancements}: Addition of Squeeze-and-Excitation (SE) blocks for channel attention
    \item \textbf{Augmentation strategies}: Implementation of Mixup and advanced geometric transformations
    \item \textbf{Training optimizations}: OneCycleLR scheduling, mixed-precision training, and gradient accumulation
    \item \textbf{Multi-dataset training}: Combined training on AffectNet and FER+ for improved generalization
    \item \textbf{Multi-model ensemble}: Exploration of model comparison and fusion strategies to overcome dataset limitations
\end{enumerate}

These refinements are documented in our training notebook and are detailed in Section~\ref{sec:training}.

\subsubsection{Addressing Dataset Limitations Through Multi-Model Approaches}

Despite incorporating multiple datasets, we recognized that each dataset carries inherent biases and limitations: label noise, limited subject diversity, class imbalance, and varying image quality. Rather than solely relying on data augmentation or naive dataset merging, we explored \textbf{multi-model strategies} as an alternative approach to improve prediction robustness.

The key insight was that models trained on different datasets learn complementary representations:
\begin{itemize}[noitemsep]
    \item \textbf{FER+-trained models}: Excel at recognizing expressions in grayscale, low-resolution contexts typical of webcam footage, benefiting from crowd-sourced probabilistic labels.
    \item \textbf{AffectNet-trained models}: Capture color-dependent cues and fine-grained details from higher-resolution RGB images with more diverse subjects.
\end{itemize}

This observation led us to develop two complementary tools:

\paragraph{Model Comparison Mode.} We implemented a real-time comparison system (\texttt{compare\_models.py}) that runs multiple models in parallel on the same video feed, displaying their predictions side-by-side. This allowed us to:
\begin{itemize}[noitemsep]
    \item Identify systematic differences in model behavior across emotions
    \item Discover expression-specific strengths (e.g., one model excelling at ``surprise'' while another better detects ``contempt'')
    \item Understand how input format (grayscale vs RGB) influences predictions
\end{itemize}

\paragraph{Ensemble Prediction Mode.} Building on these insights, we implemented ensemble inference that combines predictions from multiple models using configurable fusion strategies:
\begin{itemize}[noitemsep]
    \item \textbf{Weighted averaging}: Models with higher confidence contribute more to the final prediction
    \item \textbf{Majority voting}: Each model votes, and the most common prediction wins
    \item \textbf{Simple averaging}: Equal contribution from all models
\end{itemize}

A key addition was the \textbf{consensus metric}, which measures agreement among models. When models disagree (low consensus), it often indicates genuinely ambiguous expressions---providing users with valuable uncertainty information beyond raw confidence scores.

This multi-model exploration proved valuable not only for improving prediction quality but also for understanding the complementary nature of different training datasets. Technical details of the implementation are provided in Section~\ref{sec:ensemble}.

\subsection{Application Development}

The real-time application underwent several major iterations:

\subsubsection{Version 1--3: Core CNN Integration}
Early versions focused on establishing the basic pipeline: webcam capture, face detection (Haar Cascades), CNN inference, and emoji display.

\subsubsection{Version 4: MediaPipe Integration}
A significant milestone was the integration of \textbf{MediaPipe} for two purposes:

\begin{itemize}
    \item \textbf{Hand gesture recognition}: We added hand detection and gesture classification to expand the range of expressible emojis. Combining our CNN's emotion detection with MediaPipe's hand tracking created a richer, more interactive experience.
    \item \textbf{Facial landmark analysis}: MediaPipe Face Mesh provides detailed facial feature analysis (eye openness, smile intensity, etc.) that supplements CNN predictions for more nuanced emoji selection.
\end{itemize}

The combination of our trained CNN with MediaPipe's pre-trained models exemplifies how different recognition techniques can be fused for enhanced functionality.

\subsubsection{Debugging and Visualization}
To better understand model behavior and facilitate debugging, we implemented extensive visualization overlays:
\begin{itemize}[noitemsep]
    \item Real-time emotion probability bars
    \item Facial feature intensity indicators
    \item Hand landmark visualization
    \item Inference time and FPS monitoring
\end{itemize}

\subsection{ONNX Optimization}

The final evolution step was converting our PyTorch models to \textbf{ONNX} (Open Neural Network Exchange) format. This decision was motivated by:

\begin{itemize}
    \item \textbf{Performance}: ONNX Runtime provides optimized inference kernels, achieving 2.7$\times$ faster inference compared to native PyTorch on GPU.
    \item \textbf{Portability}: ONNX models can be deployed across different platforms and frameworks without requiring PyTorch installation.
    \item \textbf{Production readiness}: ONNX is an industry standard for deploying machine learning models, making our system more suitable for real-world applications.
    \item \textbf{Reduced dependencies}: End users can run the application with just ONNX Runtime, which has a smaller footprint than the full PyTorch installation.
\end{itemize}

The export process preserves model accuracy while enabling seamless integration with our application (see Section~\ref{sec:onnx}).

%==============================================================================
\section{Technical Implementation}
%==============================================================================

This section provides detailed technical information about our implementation, covering model architecture, training methodology, dataset handling, and deployment optimization.

\subsection{Model Architecture}
\label{sec:architecture}

We developed a CNN architecture with Squeeze-and-Excitation (SE) blocks for enhanced feature attention. The architecture is implemented in \texttt{model.py}.

\subsubsection{Squeeze-and-Excitation Blocks}

SE blocks implement a channel attention mechanism that adaptively recalibrates feature responses:

\begin{equation}
    \text{SE}(x) = x \cdot \sigma(W_2 \cdot \text{ReLU}(W_1 \cdot \text{GAP}(x)))
\end{equation}

where GAP denotes Global Average Pooling, $W_1$ and $W_2$ are learned weights, and $\sigma$ is the sigmoid function.

This mechanism allows the network to emphasize informative features while suppressing less useful ones, which is particularly beneficial for emotion recognition where subtle facial cues carry significant information.

\subsubsection{Network Architecture}

Our final architecture (\texttt{FaceEmotionCNN\_SE}) consists of:

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Block} & \textbf{Channels} & \textbf{SE Reduction} & \textbf{Output Size} \\
\midrule
Input & 1 (grayscale) & --- & $48{\times}48$ \\
Block 1 & 32 & 8 & $24{\times}24$ \\
Block 2 & 64 & 8 & $12{\times}12$ \\
Block 3 & 128 & 16 & $6{\times}6$ \\
Block 4 & 256 & 16 & $3{\times}3$ \\
Global Pool & 256 & --- & $1{\times}1$ \\
Classifier & $512 \to 8$ & --- & 8 classes \\
\bottomrule
\end{tabular}
\caption{FaceEmotionCNN\_SE architecture for FER+/FER2013 (48×48 grayscale input)}
\end{table}

Each convolutional block contains:
\begin{itemize}[noitemsep]
    \item Two 3×3 convolutional layers with BatchNorm and ReLU
    \item SE block for channel attention
    \item 2×2 MaxPooling
    \item Dropout (0.1) for regularization
\end{itemize}

\subsubsection{Model Variants}

We implemented multiple model variants to support different input formats:

\begin{itemize}
    \item \textbf{FaceEmotionCNN\_SE}: Primary model with SE blocks (used in production)
    \item \textbf{FaceEmotionCNN}: Standard CNN without SE blocks
    \item \textbf{FaceEmotionCNNLegacy}: Lightweight model for backward compatibility
\end{itemize}

The \texttt{load\_model\_smart()} function automatically detects the model architecture from checkpoint files, enabling seamless switching between variants.

\subsection{Training Methodology}
\label{sec:training}

Our training pipeline incorporates several advanced techniques, all configured through a centralized \texttt{Config} class in the training notebook.

\subsubsection{Loss Functions}

We experimented with multiple loss functions:

\begin{itemize}
    \item \textbf{Label Smoothing Cross-Entropy}: Prevents overconfident predictions by softening target labels:
    \begin{equation}
        y'_i = (1-\epsilon) \cdot y_i + \frac{\epsilon}{K}
    \end{equation}
    where $\epsilon$ is the smoothing factor and $K$ is the number of classes.
    
    \item \textbf{Focal Loss}: Addresses class imbalance by down-weighting easy examples:
    \begin{equation}
        \text{FL}(p_t) = -\alpha_t (1-p_t)^\gamma \log(p_t)
    \end{equation}
\end{itemize}

\subsubsection{Data Augmentation}

Using the Albumentations library, we applied:
\begin{itemize}[noitemsep]
    \item Horizontal flipping (p=0.5)
    \item Affine transformations: translation (±5\%), scaling (0.95--1.05), rotation (±10°)
    \item Gaussian noise and blur
    \item Brightness/contrast adjustments
    \item Coarse dropout for occlusion robustness
\end{itemize}

\subsubsection{Mixup Augmentation}

Mixup creates virtual training examples by linearly interpolating between pairs of samples:
\begin{equation}
    \tilde{x} = \lambda x_i + (1-\lambda) x_j, \quad \tilde{y} = \lambda y_i + (1-\lambda) y_j
\end{equation}
where $\lambda \sim \text{Beta}(\alpha, \alpha)$.

\subsubsection{Training Optimizations}

\begin{itemize}
    \item \textbf{Mixed Precision (AMP)}: Automatic mixed precision training for 2× speedup with minimal accuracy impact
    \item \textbf{OneCycleLR Scheduler}: Learning rate warmup followed by cosine annealing
    \item \textbf{torch.compile}: PyTorch 2.0 compilation for optimized execution graphs
    \item \textbf{Gradient Accumulation}: Enables larger effective batch sizes
\end{itemize}

\subsubsection{Training Configuration}

Key hyperparameters for our best model:
\begin{itemize}[noitemsep]
    \item Batch size: 1536
    \item Learning rate: 0.0015 (with OneCycleLR, max 0.015)
    \item Weight decay: $10^{-4}$
    \item Epochs: 100 (with early stopping, patience=20)
    \item Mixup alpha: 0.2
    \item Label smoothing: 0.1
\end{itemize}

\subsection{Dataset Handling}
\label{sec:datasets}

Our implementation supports multiple datasets through a unified interface.

\subsubsection{Supported Datasets}

\begin{table}[H]
\centering
\begin{tabular}{lcccl}
\toprule
\textbf{Dataset} & \textbf{Images} & \textbf{Size} & \textbf{Classes} & \textbf{Format} \\
\midrule
FER2013 & $\sim$35k & $48{\times}48$ & 7 & Grayscale \\
FER+ & $\sim$35k & $48{\times}48$ & 8 & Grayscale \\
AffectNet & $\sim$50k & $75{\times}75$ & 8 & RGB \\
\bottomrule
\end{tabular}
\caption{Supported datasets and their characteristics}
\end{table}

\subsubsection{Unified Emotion Mapping}

All datasets are mapped to a unified 8-class emotion scheme:

\begin{enumerate}[start=0,noitemsep]
    \item Anger
    \item Disgust
    \item Fear
    \item Happy
    \item Sad
    \item Surprise
    \item Neutral
    \item Contempt
\end{enumerate}

\subsubsection{Multi-Dataset Training}

The \texttt{CombinedEmotionDataset} class enables training on multiple datasets simultaneously:
\begin{itemize}[noitemsep]
    \item Automatic format conversion (grayscale to RGB, resizing)
    \item Unified label mapping
    \item Source tracking for analysis
    \item Class-balanced sampling
\end{itemize}

Note: FER2013 and FER+ share the same images with different labels; they should not be combined to avoid duplication.

\subsubsection{Overcoming Dataset Limitations: Multi-Model Strategies}

Due to the inherent limitations of individual emotion recognition datasets---notably label noise, limited image diversity, and class imbalance---we explored alternative strategies to improve prediction robustness. Rather than solely relying on dataset stacking, we investigated \textbf{multi-model approaches} that leverage the complementary strengths of models trained on different datasets.

This exploration was motivated by the observation that each dataset captures different aspects of emotional expression:
\begin{itemize}[noitemsep]
    \item \textbf{FER+}: Crowd-sourced annotations providing probabilistic labels that better capture ambiguous expressions
    \item \textbf{AffectNet}: Higher resolution RGB images with more diverse subjects and lighting conditions
\end{itemize}

By training separate models on each dataset and combining their predictions at inference time, we can potentially achieve better generalization than a single model trained on merged data, while avoiding issues like duplicated images or conflicting label schemes.

\subsection{Real-Time Application}
\label{sec:application}

The application (\texttt{app\_v4.py}) implements a complete real-time emoji recognition pipeline.

\subsubsection{Pipeline Overview}

\begin{enumerate}
    \item \textbf{Capture}: Webcam frame acquisition via OpenCV
    \item \textbf{Face Detection}: Haar Cascade classifier for face localization
    \item \textbf{Preprocessing}: CLAHE histogram equalization, resizing, normalization
    \item \textbf{Emotion Recognition}: CNN inference with Test-Time Augmentation (TTA)
    \item \textbf{Facial Analysis}: MediaPipe Face Mesh for detailed feature extraction
    \item \textbf{Hand Detection}: MediaPipe Hands for gesture recognition
    \item \textbf{Emoji Mapping}: Combined emotion + features + gesture → emoji selection
    \item \textbf{Display}: Real-time rendering with visualization overlays
\end{enumerate}

\subsubsection{Emotion Classification}

The \texttt{EmotionClassifier} class handles:
\begin{itemize}[noitemsep]
    \item Automatic model architecture detection
    \item Dataset-specific preprocessing
    \item Test-Time Augmentation (horizontal flip)
    \item Temporal smoothing for stable predictions
    \item Emotion-specific probability calibration
\end{itemize}

\subsubsection{Hand Gesture Recognition}

Using MediaPipe Hands, we detect and classify 11 gestures:
\begin{itemize}[noitemsep]
    \item Thumbs up/down
    \item Peace sign
    \item OK gesture
    \item Rock sign
    \item Open hand (high five)
    \item Fist
    \item Point up
    \item Pray gesture
    \item Love you sign
    \item Call me gesture
\end{itemize}

\subsubsection{Multi-Model Modes}

The application supports three operational modes (detailed in Section~\ref{sec:ensemble}):
\begin{enumerate}
    \item \textbf{Single model}: Standard operation with one model for maximum performance.
    \item \textbf{Comparison mode}: Side-by-side comparison of multiple models on the same frame, allowing visual analysis of model behavior and identification of disagreements.
    \item \textbf{Ensemble mode}: Fusion of predictions from multiple models using configurable strategies (weighted averaging, voting, or simple averaging), with real-time consensus visualization.
\end{enumerate}

These modes enable both development-time analysis (understanding model differences) and deployment-time robustness (combining model strengths).

\subsection{Model Comparison and Ensemble Strategies}
\label{sec:ensemble}

This section provides the technical details of our multi-model implementation, building on the motivation described in Section~2.3.

\subsubsection{Comparison Mode Implementation}

The \texttt{ModelComparator} class handles automatic model detection, loading, and preprocessing for different architectures:

\begin{lstlisting}[language=Python]
# Automatic architecture detection from checkpoint
in_channels = state_dict['conv1a.weight'].shape[1]
num_classes = state_dict['fc3.weight'].shape[0]
\end{lstlisting}

Each model receives appropriately preprocessed input (grayscale vs RGB, different resolutions), and predictions are displayed in real-time with individual inference times.

\subsubsection{Ensemble Fusion Strategies}

We implemented three fusion strategies with the following mathematical formulations:

\begin{enumerate}
    \item \textbf{Weighted Average}: Probabilities weighted by model confidence:
    \begin{equation}
        P_{\text{ensemble}} = \sum_{i=1}^{N} w_i \cdot P_i, \quad \text{where } w_i = \frac{\max(P_i)}{\sum_j \max(P_j)}
    \end{equation}
    
    \item \textbf{Majority Voting}: Most common prediction selected:
    \begin{equation}
        \hat{y}_{\text{ensemble}} = \arg\max_c \sum_{i=1}^{N} \mathbb{1}[\hat{y}_i = c]
    \end{equation}
    
    \item \textbf{Simple Average}: Equal-weight probability averaging:
    \begin{equation}
        P_{\text{ensemble}} = \frac{1}{N} \sum_{i=1}^{N} P_i
    \end{equation}
\end{enumerate}

\subsubsection{Consensus Metric}

The consensus metric quantifies model agreement:

\begin{equation}
    \text{Consensus} = \frac{|\{i : \hat{y}_i = \hat{y}_{\text{ensemble}}\}|}{N}
\end{equation}

The application visualizes consensus through color-coded indicators:
\begin{itemize}[noitemsep]
    \item \textbf{Green}: High agreement ($\geq 70\%$)
    \item \textbf{Yellow}: Moderate agreement ($50-70\%$)
    \item \textbf{Orange}: Low agreement ($< 50\%$)
\end{itemize}

\subsection{ONNX Export and Optimization}
\label{sec:onnx}

The \texttt{export\_to\_onnx.py} script handles model conversion for optimized deployment.

\subsubsection{Export Process}

\begin{lstlisting}[language=Python]
torch.onnx.export(
    model,
    dummy_input,
    onnx_path,
    opset_version=14,
    do_constant_folding=True,
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
\end{lstlisting}

Key export parameters:
\begin{itemize}[noitemsep]
    \item \textbf{opset\_version=14}: Good compatibility across ONNX Runtime versions
    \item \textbf{do\_constant\_folding=True}: Pre-computes constant operations
    \item \textbf{dynamic\_axes}: Enables variable batch size inference
\end{itemize}

\subsubsection{Runtime Integration}

The application automatically detects and uses ONNX models when available:
\begin{itemize}[noitemsep]
    \item Checks for \texttt{.onnx} file alongside \texttt{.pth} file
    \item Selects appropriate execution provider (CUDA or CPU)
    \item Falls back to PyTorch if ONNX Runtime unavailable
\end{itemize}

\subsubsection{Performance Comparison}

Benchmarks were conducted on an NVIDIA RTX 4050 GPU and Intel i7 CPU:

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Runtime} & \textbf{Inference Time} & \textbf{FPS} & \textbf{Speedup} \\
\midrule
\multicolumn{4}{l}{\textit{GPU (NVIDIA RTX 4050)}} \\
PyTorch & 18.3ms & 28 & 1.0$\times$ \\
ONNX Runtime (CUDA) & 6.8ms & 54 & 2.7$\times$ \\
\midrule
\multicolumn{4}{l}{\textit{CPU (Intel i7)}} \\
PyTorch & 45.2ms & 12 & 1.0$\times$ \\
ONNX Runtime & 19.6ms & 28 & 2.3$\times$ \\
\bottomrule
\end{tabular}
\caption{Inference time comparison (per frame)}
\end{table}

%==============================================================================
\section{Results and Discussion}
%==============================================================================

This section presents the results from our final training run, analyzing the training dynamics and model performance.

\subsection{Training Results}

The comprehensive training dashboard from our final 100-epoch training run on the combined AffectNet + FER+ dataset.

% TODO: Add training_dashboard.png to the report folder
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{training_dashboard.png}
% \caption{Training dashboard showing loss curves, accuracy, learning rate schedule, epoch timing, GPU memory usage, and final configuration summary.}
% \label{fig:training_dashboard}
% \end{figure}

\subsubsection{Final Metrics}

Our best model achieved the following results:

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Best Validation Accuracy & \textbf{82.13\%} \\
Best Epoch & 100 \\
Training Accuracy (final) & $\sim$95\% \\
Validation Loss (final) & $\sim$0.65 \\
Training Loss (final) & $\sim$0.80 \\
\bottomrule
\end{tabular}
\caption{Final training metrics}
\end{table}

\subsubsection{Loss Curve Analysis}

The loss curves reveal an interesting phenomenon: the \textbf{validation loss is consistently lower than the training loss} throughout the entire training process. This counter-intuitive behavior is explained by our use of \textbf{Mixup augmentation} ($\alpha = 0.2$).

Mixup creates synthetic training samples by linearly interpolating between pairs of images and their labels, making the training task inherently more difficult than predicting clean, unaugmented validation images. This regularization technique:
\begin{itemize}[noitemsep]
    \item Prevents the model from memorizing training samples
    \item Encourages smoother decision boundaries
    \item Improves generalization to unseen data
\end{itemize}

Both curves decrease steadily over the 100 epochs, indicating stable and consistent learning without signs of divergence.

\subsubsection{Accuracy Analysis}

The accuracy plot shows typical deep learning training dynamics:
\begin{itemize}[noitemsep]
    \item \textbf{Rapid initial improvement}: Accuracy increases sharply during the first 20 epochs as the model learns fundamental facial features.
    \item \textbf{Gradual convergence}: Progress slows as the model refines its representations.
    \item \textbf{Gap between train and validation}: The training accuracy reaches $\sim$95\% while validation plateaus at 82.13\%, indicating some degree of overfitting. However, this gap is expected and acceptable given the complexity of emotion recognition.
\end{itemize}

The validation accuracy of 82.13\% represents strong performance for facial emotion recognition, particularly considering the inherent ambiguity in human emotional expressions.

\subsubsection{Learning Rate Schedule}

The OneCycleLR scheduler follows a characteristic profile:
\begin{enumerate}[noitemsep]
    \item \textbf{Warmup phase} (epochs 0--30): Learning rate increases from 0.0015 to peak at 0.015
    \item \textbf{Annealing phase} (epochs 30--100): Learning rate decreases following a cosine curve toward zero
\end{enumerate}

This ``super-convergence'' strategy enables faster training and often achieves better final accuracy than constant learning rate schedules.

\subsubsection{Computational Performance}

The time-per-epoch graph reveals an important characteristic of \texttt{torch.compile}:
\begin{itemize}[noitemsep]
    \item \textbf{First epoch}: $\sim$450 seconds due to JIT compilation and optimization of the computation graph
    \item \textbf{Subsequent epochs}: Stable at $\sim$68.7 seconds average
\end{itemize}

The initial compilation overhead is a one-time cost that pays dividends across all subsequent epochs, reducing total training time significantly compared to eager execution.

\subsubsection{Memory Utilization}

GPU memory usage remained perfectly stable at \textbf{9.6 GB} throughout training, demonstrating:
\begin{itemize}[noitemsep]
    \item No memory leaks in our training pipeline
    \item Efficient batch processing with our large batch size (1536)
    \item Effective memory management through mixed precision training
\end{itemize}

\subsection{Training Configuration Summary}

The final training configuration that produced our best results:

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Batch Size & 1536 \\
Epochs & 100 \\
Mixed Precision (AMP) & Enabled \\
\texttt{torch.compile} & Enabled \\
Mixup & Enabled ($\alpha = 0.2$) \\
CutMix & Disabled \\
SE Blocks & Enabled \\
Label Smoothing & 0.1 \\
Optimizer & AdamW \\
Scheduler & OneCycleLR \\
\bottomrule
\end{tabular}
\caption{Final training configuration}
\end{table}

\subsection{Real-Time Performance}

The final application achieves:
\begin{itemize}[noitemsep]
    \item 28 FPS with PyTorch backend (GPU)
    \item 54 FPS with ONNX Runtime (GPU) --- 2.7$\times$ speedup
    \item Stable emoji predictions via temporal smoothing
    \item Responsive hand gesture detection
\end{itemize}

\subsection{Lessons Learned}

\begin{enumerate}
    \item \textbf{Architecture matters}: SE blocks provided meaningful improvements over vanilla CNNs for emotion recognition by enabling adaptive channel attention.
    \item \textbf{Data quality over quantity}: FER+ labels significantly improved training stability compared to original FER2013 due to crowd-sourced annotation quality.
    \item \textbf{Regularization is key}: Mixup augmentation proved essential for generalization, as evidenced by the lower validation loss compared to training loss.
    \item \textbf{Modern PyTorch features}: \texttt{torch.compile} and mixed precision training dramatically reduced training time with no accuracy penalty.
    \item \textbf{Multi-modal fusion}: Combining CNN predictions with facial landmarks creates more nuanced emoji mappings.
    \item \textbf{Deployment considerations}: ONNX optimization is essential for real-time applications, providing 2.7$\times$ speedup on GPU.
\end{enumerate}

%==============================================================================
\section{Conclusion}
%==============================================================================

This project successfully developed a real-time emoji recognition system that combines deep learning-based emotion recognition with hand gesture detection. Starting from a simple MLP proposal, we evolved through multiple iterations to create a sophisticated application featuring:

\begin{itemize}[noitemsep]
    \item A CNN architecture with attention mechanisms trained on multiple datasets
    \item Integration with MediaPipe for comprehensive gesture and facial analysis
    \item ONNX optimization for deployment-ready performance
    \item An intuitive real-time interface with extensive visualization capabilities
\end{itemize}

The project demonstrates the importance of iterative development, the value of combining different recognition techniques, and the practical considerations required for deploying deep learning models in real-time applications.

\subsection{Future Work}

Potential extensions include:
\begin{itemize}[noitemsep]
    \item Transformer-based architectures for improved accuracy
    \item Mobile deployment using TensorFlow Lite or CoreML
    \item Additional gesture vocabularies
    \item Multi-face tracking and recognition
    \item Integration with video conferencing platforms
\end{itemize}
\end{document}
