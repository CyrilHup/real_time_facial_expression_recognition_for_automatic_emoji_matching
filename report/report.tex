% chktex-file 8
% chktex-file 18
% chktex-file 26
\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{float}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}

\geometry{margin=2.5cm}

% Configuration des listings pour Python
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Python
}

\lstset{style=pythonstyle}

\title{\textbf{Reconnaissance d'Ã‰motions Faciales en Temps RÃ©el}\\
\large Rapport Technique - Deep Learning}
\author{Cyril}
\date{Novembre 2025}

\begin{document}

\maketitle

\begin{abstract}
Ce rapport prÃ©sente le dÃ©veloppement d'un systÃ¨me de reconnaissance d'Ã©motions faciales en temps rÃ©el utilisant des rÃ©seaux de neurones convolutifs (CNN). Nous dÃ©taillons l'architecture initiale, les optimisations apportÃ©es, ainsi que les choix techniques effectuÃ©s pour amÃ©liorer les performances de dÃ©tection. Le systÃ¨me est capable de classifier huit Ã©motions (colÃ¨re, dÃ©goÃ»t, peur, joie, tristesse, surprise, neutralitÃ©, et mÃ©pris) Ã  partir d'un flux vidÃ©o webcam, en utilisant une approche multi-sources unifiÃ©e (AffectNet + FER+).
\end{abstract}

\tableofcontents
\newpage

%==============================================================================
\section{Introduction}
%==============================================================================

La reconnaissance automatique des Ã©motions faciales est un domaine en pleine expansion avec des applications dans l'interaction homme-machine, la santÃ© mentale, le marketing, et l'Ã©ducation. Ce projet implÃ©mente un systÃ¨me complet de reconnaissance d'Ã©motions en temps rÃ©el, depuis l'entraÃ®nement du modÃ¨le jusqu'Ã  l'infÃ©rence via webcam.

\subsection{Objectifs}
\begin{itemize}
    \item DÃ©velopper un modÃ¨le CNN capable de classifier 8 Ã©motions faciales (7 de FER2013 + Contempt de FER+)
    \item ImplÃ©menter une application temps rÃ©el avec dÃ©tection de visage
    \item Optimiser les performances pour une utilisation fluide
    \item AmÃ©liorer la prÃ©cision par des techniques avancÃ©es de deep learning
\end{itemize}

%==============================================================================
\section{Architecture Initiale}
%==============================================================================

\subsection{Dataset : Approche Multi-Sources UnifiÃ©e}

Pour maximiser la robustesse du modÃ¨le, nous avons adoptÃ© une stratÃ©gie de fusion de datasets. Le systÃ¨me utilise dÃ©sormais un **dataset unifiÃ© Ã  8 classes** combinant :

\begin{itemize}
    \item \textbf{Balanced AffectNet} : Source principale ($\sim$41k images, RGB 75$\times$75).
    \item \textbf{FER+} : Version amÃ©liorÃ©e de FER2013 avec labels corrigÃ©s par vote (Microsoft), upscalÃ©e Ã  75$\times$75.
\end{itemize}

\textbf{Avantages de cette approche :}
\begin{itemize}
    \item \textbf{Volume de donnÃ©es accru} : Combine la diversitÃ© d'AffectNet avec les cas difficiles de FER+.
    \item \textbf{Labels de haute qualitÃ©} : Utilisation exclusive de datasets vÃ©rifiÃ©s (FER+ vs FER2013 original).
    \item \textbf{Mapping UnifiÃ©} : Toutes les sources sont mappÃ©es vers les 8 Ã©motions standards (Anger, Disgust, Fear, Happy, Sad, Surprise, Neutral, Contempt).
\end{itemize}

\subsection{ModÃ¨le Initial (\texttt{model.py})}

L'architecture initiale est un CNN simple avec 3 blocs convolutifs :

\begin{lstlisting}[caption={Architecture CNN pour Balanced AffectNet}]
class FaceEmotionCNN(nn.Module):
    def __init__(self, num_classes=8, in_channels=3, input_size=75):
        super(FaceEmotionCNN, self).__init__()
        # Bloc Convolutif 1 (75x75 -> 37x37)
        self.conv1a = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)
        self.bn1a = nn.BatchNorm2d(64)
        self.conv1b = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.bn1b = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.dropout1 = nn.Dropout(0.1)
        
        # ... Blocs 2, 3, 4 similaires ...
        
        # Global Average Pooling
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        
        # Couches Fully Connected
        self.fc1 = nn.Linear(512, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, num_classes)  # 8 emotions
\end{lstlisting}

\subsubsection{Analyse de l'architecture initiale}

\begin{table}[H]
\centering
\caption{Dimensions Ã  travers le rÃ©seau (Balanced AffectNet)}
\begin{tabular}{lccc}
\toprule
\textbf{Couche} & \textbf{EntrÃ©e} & \textbf{Sortie} & \textbf{ParamÃ¨tres} \\
\midrule
Block1 (2$\times$Conv) + Pool & $3 \times 75 \times 75$ & $64 \times 37 \times 37$ & 38,592 \\
Block2 (2$\times$Conv) + Pool & $64 \times 37 \times 37$ & $128 \times 18 \times 18$ & 147,712 \\
Block3 (2$\times$Conv) + Pool & $128 \times 18 \times 18$ & $256 \times 9 \times 9$ & 590,336 \\
Block4 (2$\times$Conv) + Pool & $256 \times 9 \times 9$ & $512 \times 4 \times 4$ & 2,360,320 \\
Global Avg Pool & $512 \times 4 \times 4$ & 512 & 0 \\
FC1 + BN & 512 & 256 & 131,584 \\
FC2 + BN & 256 & 128 & 33,024 \\
FC3 & 128 & 8 & 1,032 \\
\midrule
\textbf{Total} & & & \textbf{$\approx$ 3.3M} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Points positifs :}
\begin{itemize}
    \item Utilisation de Batch Normalization pour stabiliser l'entraÃ®nement
    \item Dropout (50\%) pour rÃ©gulariser et Ã©viter l'overfitting
    \item Architecture simple et rapide Ã  entraÃ®ner
\end{itemize}

\textbf{Limitations :}
\begin{itemize}
    \item Peu de couches convolutives (faible capacitÃ© d'extraction de features)
    \item Nombre de filtres limitÃ© (32-64-128)
    \item Pas de rÃ©gularisation dans les couches convolutives
    \item Couche FC1 trÃ¨s large (2.3M paramÃ¨tres) crÃ©ant un goulot d'Ã©tranglement
\end{itemize}

\subsection{Script d'entraÃ®nement initial (\texttt{train.py})}

\begin{lstlisting}[caption={Script d'entraÃ®nement initial}]
# Hyperparametres
BATCH_SIZE = 64
LEARNING_RATE = 0.001
EPOCHS = 25

# Transformations simples
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.ToTensor(),
])

# Chargement du dataset
dataset = FER2013Dataset('./data/fer2013/fer2013.csv', transform=transform)
train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# Modele et optimiseur
model = FaceEmotionCNN().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# Boucle d'entrainement
for epoch in range(EPOCHS):
    running_loss = 0.0
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    
    print(f"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}")

torch.save(model.state_dict(), 'emotion_model.pth')
\end{lstlisting}

\textbf{Limitations de l'entraÃ®nement initial :}
\begin{itemize}
    \item Pas de data augmentation (risque d'overfitting)
    \item Pas de sÃ©paration train/validation (impossible de dÃ©tecter l'overfitting)
    \item Learning rate fixe (convergence sous-optimale)
    \item Pas d'early stopping (gaspillage de ressources)
    \item Pas de gestion du dÃ©sÃ©quilibre des classes
\end{itemize}

\subsection{Application initiale (\texttt{app.py})}

L'application initiale effectue :
\begin{enumerate}
    \item Capture vidÃ©o via webcam
    \item DÃ©tection de visage avec Haar Cascade
    \item PrÃ©traitement de l'image (redimensionnement 48$\times$48, grayscale)
    \item InfÃ©rence avec le modÃ¨le CNN
    \item Affichage de l'Ã©motion dÃ©tectÃ©e avec emoji
\end{enumerate}

\begin{lstlisting}[caption={Boucle principale de l'application initiale}]
while True:
    ret, frame = cap.read()
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray_frame, 
                                          scaleFactor=1.3, 
                                          minNeighbors=5)

    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)
        roi_gray = gray_frame[y:y+h, x:x+w]
        
        # Preprocessing
        roi_tensor = data_transform(roi_gray).unsqueeze(0).to(device)

        # Prediction
        with torch.no_grad():
            outputs = model(roi_tensor)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            max_prob, predicted_idx = torch.max(probabilities, 1)
            
            idx = predicted_idx.item()
            confidence = max_prob.item() * 100
            emotion_text = emotion_dict[idx]
\end{lstlisting}

\textbf{Limitations :}
\begin{itemize}
    \item PrÃ©dictions instables (oscillations entre frames)
    \item Pas de normalisation de l'Ã©clairage
    \item ParamÃ¨tres de dÃ©tection de visage sous-optimaux
    \item Interface utilisateur basique
\end{itemize}

%==============================================================================
\section{Optimisations de l'Architecture}
%==============================================================================

\subsection{Nouvelle Architecture CNN}

L'architecture amÃ©liorÃ©e comporte plusieurs optimisations majeures :

\begin{lstlisting}[caption={Architecture CNN amÃ©liorÃ©e}]
class FaceEmotionCNN(nn.Module):
    def __init__(self, num_classes=7):
        super(FaceEmotionCNN, self).__init__()
        
        # Bloc 1 (48x48 -> 24x24) - Double convolution
        self.conv1a = nn.Conv2d(1, 64, kernel_size=3, padding=1)
        self.bn1a = nn.BatchNorm2d(64)
        self.conv1b = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.bn1b = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.dropout1 = nn.Dropout(0.25)
        
        # Bloc 2 (24x24 -> 12x12)
        self.conv2a = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2a = nn.BatchNorm2d(128)
        self.conv2b = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.bn2b = nn.BatchNorm2d(128)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.dropout2 = nn.Dropout(0.25)
        
        # Bloc 3 (12x12 -> 6x6)
        self.conv3a = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn3a = nn.BatchNorm2d(256)
        self.conv3b = nn.Conv2d(256, 256, kernel_size=3, padding=1)
        self.bn3b = nn.BatchNorm2d(256)
        self.pool3 = nn.MaxPool2d(2, 2)
        self.dropout3 = nn.Dropout(0.25)
        
        # Bloc 4 (6x6 -> 3x3)
        self.conv4a = nn.Conv2d(256, 512, kernel_size=3, padding=1)
        self.bn4a = nn.BatchNorm2d(512)
        self.conv4b = nn.Conv2d(512, 512, kernel_size=3, padding=1)
        self.bn4b = nn.BatchNorm2d(512)
        self.pool4 = nn.MaxPool2d(2, 2)
        self.dropout4 = nn.Dropout(0.25)
        
        # Global Average Pooling
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        
        # Fully Connected avec dropout progressif
        self.fc1 = nn.Linear(512, 256)
        self.bn_fc1 = nn.BatchNorm1d(256)
        self.dropout_fc1 = nn.Dropout(0.5)
        
        self.fc2 = nn.Linear(256, 128)
        self.bn_fc2 = nn.BatchNorm1d(128)
        self.dropout_fc2 = nn.Dropout(0.4)
        
        self.fc3 = nn.Linear(128, num_classes)
\end{lstlisting}

\subsubsection{Justification des choix architecturaux}

\begin{enumerate}
    \item \textbf{Double convolution par bloc (style VGG)} \\
    InspirÃ© de VGGNet, deux convolutions 3$\times$3 successives permettent un champ rÃ©ceptif de 5$\times$5 avec moins de paramÃ¨tres qu'une convolution 5$\times$5 unique :
    \begin{equation}
        \text{ParamÃ¨tres }3\times3\times2 = 2 \times (3^2 \times C^2) = 18C^2
    \end{equation}
    \begin{equation}
        \text{ParamÃ¨tres }5\times5 = 5^2 \times C^2 = 25C^2
    \end{equation}
    
    \item \textbf{Augmentation progressive des filtres (64â†’128â†’256â†’512)} \\
    Suit le principe que les premiÃ¨res couches extraient des features simples (bords, textures) tandis que les couches profondes capturent des concepts complexes (parties du visage, expressions).
    
    \item \textbf{Dropout progressif (0.25 dans conv, 0.5â†’0.4 dans FC)} \\
    Le dropout dans les couches convolutives (0.25) rÃ©gularise sans trop perturber l'apprentissage spatial. Un dropout plus fort dans les FC (0.5) combat l'overfitting oÃ¹ le risque est le plus Ã©levÃ©.
    
    \item \textbf{Global Average Pooling (GAP)} \\
    Remplace le flatten traditionnel. Avantages :
    \begin{itemize}
        \item RÃ©duit drastiquement les paramÃ¨tres ($512 \times 3 \times 3 = 4608 \rightarrow 512$)
        \item Agit comme rÃ©gularisateur
        \item Invariance spatiale accrue
    \end{itemize}
    
    \item \textbf{Initialisation Kaiming} \\
    Initialisation adaptÃ©e aux fonctions d'activation ReLU :
    \begin{equation}
        W \sim \mathcal{N}\left(0, \sqrt{\frac{2}{n_{in}}}\right)
    \end{equation}

    \item \textbf{Squeeze-and-Excitation (SE) Blocks} \\
    IntÃ©gration de modules d'attention SE qui recalibrent dynamiquement les features maps par canal, permettant au rÃ©seau de se concentrer sur les caractÃ©ristiques les plus pertinentes pour l'Ã©motion.
\end{enumerate}

\begin{table}[H]
\centering
\caption{Comparaison des architectures}
\begin{tabular}{lcc}
\toprule
\textbf{CaractÃ©ristique} & \textbf{Initial} & \textbf{AmÃ©liorÃ©} \\
\midrule
Blocs convolutifs & 3 & 4 \\
Convolutions par bloc & 1 & 2 \\
Filtres max & 128 & 512 \\
ParamÃ¨tres totaux & $\approx$2.46M & $\approx$4.8M \\
Global Average Pooling & Non & Oui \\
Dropout conv & Non & Oui (0.25) \\
Batch Norm FC & Non & Oui \\
Attention & Non & SE-Blocks \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Optimisations de l'EntraÃ®nement}
%==============================================================================

\subsection{Data Augmentation}

La data augmentation est cruciale pour un petit dataset comme FER2013. Elle augmente artificiellement la diversitÃ© des donnÃ©es d'entraÃ®nement.

\begin{lstlisting}[caption={Transformations de data augmentation}]
train_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(10),
    transforms.RandomAffine(
        degrees=0, 
        translate=(0.1, 0.1),
        scale=(0.9, 1.1)
    ),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
])
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Transformations de data augmentation}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Transformation} & \textbf{Justification} \\
\midrule
RandomHorizontalFlip (p=0.5) & Les expressions sont symÃ©triques. Double effectivement le dataset. \\
RandomRotation (Â±10Â°) & Simule les lÃ©gÃ¨res inclinaisons de tÃªte naturelles. \\
RandomAffine (translate) & Compense les variations de position du visage dans le cadre. \\
RandomAffine (scale 0.9-1.1) & Simule diffÃ©rentes distances camÃ©ra-visage. \\
ColorJitter (brightness, contrast) & Robustesse aux variations d'Ã©clairage. \\
\bottomrule
\end{tabular}
\end{table}

\subsection{SÃ©paration Train/Validation}

\begin{lstlisting}[caption={SÃ©paration du dataset}]
VALIDATION_SPLIT = 0.15  # 15% pour validation

val_size = int(len(full_dataset) * VALIDATION_SPLIT)
train_size = len(full_dataset) - val_size

train_dataset, val_dataset = random_split(
    full_dataset, 
    [train_size, val_size],
    generator=torch.Generator().manual_seed(42)
)
\end{lstlisting}

\textbf{Importance :} La validation permet de :
\begin{itemize}
    \item DÃ©tecter l'overfitting (train loss $\downarrow$ mais val loss $\uparrow$)
    \item SÃ©lectionner le meilleur modÃ¨le
    \item Ajuster les hyperparamÃ¨tres
\end{itemize}

\subsection{Early Stopping}

L'early stopping arrÃªte l'entraÃ®nement quand la validation ne s'amÃ©liore plus :

\begin{lstlisting}[caption={ImplÃ©mentation de l'early stopping}]
PATIENCE = 7
best_val_acc = 0.0
patience_counter = 0

for epoch in range(EPOCHS):
    # ... entrainement ...
    val_loss, val_acc = validate(model, val_loader, criterion, device)
    
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        patience_counter = 0
        torch.save(model.state_dict(), 'emotion_model_best.pth')
    else:
        patience_counter += 1
        if patience_counter >= PATIENCE:
            print("Early stopping triggered!")
            break
\end{lstlisting}

\subsection{Learning Rate Scheduler}

Le scheduler rÃ©duit le learning rate quand la validation stagne :

\begin{lstlisting}[caption={Learning rate scheduler}]
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='min',      # Surveille la loss
    factor=0.5,      # Divise LR par 2
    patience=3       # Attend 3 epochs sans amelioration
)

# Dans la boucle d'entrainement
scheduler.step(val_loss)
\end{lstlisting}

\textbf{Principe :} Un LR Ã©levÃ© au dÃ©but permet une convergence rapide, puis un LR plus faible permet un affinage prÃ©cis.

\subsection{Optimiseur AdamW}

\begin{lstlisting}[caption={Optimiseur AdamW avec weight decay}]
optimizer = optim.AdamW(
    model.parameters(), 
    lr=LEARNING_RATE, 
    weight_decay=1e-4
)
\end{lstlisting}

AdamW corrige un problÃ¨me de Adam : le weight decay est appliquÃ© directement aux poids plutÃ´t qu'au gradient, ce qui donne une meilleure rÃ©gularisation L2.

\subsection{Gradient Clipping}

\begin{lstlisting}[caption={Gradient clipping pour la stabilitÃ©}]
loss.backward()
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()
\end{lstlisting}

EmpÃªche les gradients explosifs qui peuvent dÃ©stabiliser l'entraÃ®nement.

%==============================================================================
\section{Optimisations de l'Application Temps RÃ©el}
%==============================================================================

\subsection{Lissage Temporel des PrÃ©dictions}

Le problÃ¨me principal des prÃ©dictions frame-par-frame est l'instabilitÃ© : l'Ã©motion affichÃ©e peut changer rapidement entre frames successives, crÃ©ant un effet de \emph{flickering} dÃ©sagrÃ©able.

\begin{lstlisting}[caption={Lissage temporel avec moyenne pondÃ©rÃ©e}]
from collections import deque

SMOOTHING_WINDOW = 5
prediction_history = deque(maxlen=SMOOTHING_WINDOW)

def get_smoothed_prediction(current_probs):
    prediction_history.append(current_probs.cpu().numpy())
    
    if len(prediction_history) < 2:
        return current_probs
    
    # Moyenne ponderee (frames recentes = plus de poids)
    weights = np.linspace(0.5, 1.0, len(prediction_history))
    weights = weights / weights.sum()
    
    smoothed = np.zeros(7)
    for i, probs in enumerate(prediction_history):
        smoothed += weights[i] * probs
    
    return torch.tensor(smoothed)
\end{lstlisting}

\textbf{Principe :} Au lieu d'utiliser uniquement la prÃ©diction de la frame courante, on calcule une moyenne pondÃ©rÃ©e sur les $N$ derniÃ¨res frames. Les frames rÃ©centes ont plus de poids pour maintenir la rÃ©activitÃ©.

\subsection{Ã‰galisation d'Histogramme}

\begin{lstlisting}[caption={Normalisation de l'Ã©clairage}]
# Avant preprocessing
roi_gray = cv2.equalizeHist(roi_gray)
\end{lstlisting}

L'Ã©galisation d'histogramme normalise la distribution des niveaux de gris, rendant le modÃ¨le plus robuste aux variations d'Ã©clairage.

\begin{figure}[H]
\centering
\fbox{\parbox{0.8\textwidth}{\centering
\textbf{Image sombre} $\longrightarrow$ \textbf{Histogramme Ã©galisÃ©}
}}
\caption{L'Ã©galisation redistribue les intensitÃ©s sur toute la plage [0, 255]}
\end{figure}

\subsection{ParamÃ¨tres de DÃ©tection OptimisÃ©s}

\begin{lstlisting}[caption={ParamÃ¨tres de dÃ©tection de visage optimisÃ©s}]
faces = face_cascade.detectMultiScale(
    gray_frame, 
    scaleFactor=1.1,   # Plus precis (etait 1.3)
    minNeighbors=5,
    minSize=(48, 48),  # Taille minimum
    flags=cv2.CASCADE_SCALE_IMAGE
)
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Impact des paramÃ¨tres de dÃ©tection}
\begin{tabular}{lcc}
\toprule
\textbf{ParamÃ¨tre} & \textbf{Valeur basse} & \textbf{Valeur haute} \\
\midrule
scaleFactor & Plus prÃ©cis, plus lent & Moins prÃ©cis, plus rapide \\
minNeighbors & Plus de faux positifs & Moins de dÃ©tections \\
minSize & DÃ©tecte petits visages & Ignore petits visages \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Interface Utilisateur AmÃ©liorÃ©e}

\begin{itemize}
    \item \textbf{Couleurs par Ã©motion} : Chaque Ã©motion a une couleur distinctive
    \item \textbf{Barres de progression} : Affichage des probabilitÃ©s de toutes les classes
    \item \textbf{Fond pour le texte} : AmÃ©liore la lisibilitÃ©
\end{itemize}

\begin{lstlisting}[caption={Couleurs par Ã©motion}]
emotion_colors = {
    0: (0, 0, 255),     # Rouge - Angry
    1: (0, 128, 0),     # Vert fonce - Disgust
    2: (128, 0, 128),   # Violet - Fear
    3: (0, 255, 255),   # Jaune - Happy
    4: (255, 0, 0),     # Bleu - Sad
    5: (0, 165, 255),   # Orange - Surprise
    6: (128, 128, 128)  # Gris - Neutral
}
\end{lstlisting}

%==============================================================================
\section{AmÃ©lioration du Dataset}
%==============================================================================

\subsection{Avantages de l'Approche Multi-Sources}

Par rapport Ã  FER2013, notre dataset unifiÃ© prÃ©sente plusieurs avantages majeurs :

\begin{enumerate}
    \item \textbf{RÃ©solution supÃ©rieure} : Images 75$\times$75 pixels (vs 48$\times$48)
    \item \textbf{Images RGB} : 3 canaux de couleur offrant plus d'informations
    \begin{table}[H]
    \centering
    \begin{tabular}{lcc}
    \toprule
    \textbf{Ã‰motion} & \textbf{Nombre d'images} & \textbf{Pourcentage} \\
    \midrule
    Angry & 5,126 & 12.5\% \\
    Contempt & 5,126 & 12.5\% \\
    Disgust & 5,126 & 12.5\% \\
    Fear & 5,126 & 12.5\% \\
    Happy & 5,126 & 12.5\% \\
    Neutral & 5,126 & 12.5\% \\
    Sad & 5,126 & 12.5\% \\
    Surprise & 5,126 & 12.5\% \\
    \bottomrule
    \end{tabular}
    \caption{Distribution des classes dans le dataset unifiÃ©.}
    \end{table}
    
    \item \textbf{Dataset Ã©quilibrÃ©} : Pas de biais de classe, contrairement Ã  FER2013
    \item \textbf{8 classes natives} : Inclut Contempt (mÃ©pris) dÃ¨s le dÃ©part
    \item \textbf{Meilleure qualitÃ©} : Annotations plus fiables
\end{enumerate}

\subsection{Chargement du Dataset}

Le dataset unifiÃ© est organisÃ© en dossiers par Ã©motion :

\begin{lstlisting}[caption={Utilisation du dataset unifiÃ©}]
class UnifiedDataset(Dataset):
    EMOTION_CLASSES = {
        'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3,
        'Sad': 4, 'Surprise': 5, 'Neutral': 6, 'Contempt': 7,
    }
    
    def __init__(self, root_dir, split='train', transform=None):
        self.images = []
        self.labels = []
        
        for emotion_name, emotion_idx in self.EMOTION_CLASSES.items():
            emotion_dir = os.path.join(root_dir, split, emotion_name)
            for img_name in os.listdir(emotion_dir):
                self.images.append(os.path.join(emotion_dir, img_name))
                self.labels.append(emotion_idx)
    
    def __getitem__(self, idx):
        image = Image.open(self.images[idx]).convert('RGB')
        image = np.array(image)  # 75x75x3
        label = self.labels[idx]
        
        if self.transform:
            image = self.transform(image=image)['image']
        
        return image, label
\end{lstlisting}

\subsection{Ã‰quilibrage des Classes}

Pour compenser le dÃ©sÃ©quilibre (notamment le manque de \textit{Disgust}), deux techniques sont implÃ©mentÃ©es :

\subsubsection{Poids de classe dans la loss}

\begin{lstlisting}[caption={CrossEntropyLoss avec poids de classe}]
def get_class_weights(dataset):
    class_counts = np.bincount(labels, minlength=7)
    weights = 1.0 / class_counts
    weights = weights / weights.sum() * len(weights)
    return torch.FloatTensor(weights)

class_weights = get_class_weights(train_dataset)
criterion = nn.CrossEntropyLoss(weight=class_weights)
\end{lstlisting}

Le poids de chaque classe est inversement proportionnel Ã  sa frÃ©quence :
\begin{equation}
    w_c = \frac{N}{N_c \times C}
\end{equation}
oÃ¹ $N$ est le nombre total d'Ã©chantillons, $N_c$ le nombre d'Ã©chantillons de la classe $c$, et $C$ le nombre de classes.

\subsubsection{WeightedRandomSampler}

\begin{lstlisting}[caption={Sampler Ã©quilibrÃ©}]
def get_balanced_sampler(dataset):
    class_counts = np.bincount(labels, minlength=7)
    weights = 1.0 / class_counts
    sample_weights = weights[labels]
    
    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )
    return sampler
\end{lstlisting}

Le sampler sur-Ã©chantillonne les classes minoritaires pendant l'entraÃ®nement.

%==============================================================================
\section{Techniques d'EntraÃ®nement AvancÃ©es}
%==============================================================================

Pour amÃ©liorer significativement les performances, notamment sur les classes difficiles (Disgust, Contempt, Fear, Angry), nous avons dÃ©veloppÃ© un workflow d'entraÃ®nement avancÃ© (notebook Jupyter) intÃ©grant les techniques les plus rÃ©centes du deep learning.

\subsection{Focal Loss pour le DÃ©sÃ©quilibre des Classes}

La Cross-Entropy standard traite tous les exemples de maniÃ¨re Ã©gale. La \textbf{Focal Loss} \cite{lin2017focal} ajoute un facteur de modulation qui rÃ©duit la contribution des exemples faciles et se concentre sur les exemples difficiles :

\begin{equation}
    \text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
\end{equation}

oÃ¹ $p_t$ est la probabilitÃ© prÃ©dite pour la vraie classe, $\alpha_t$ est le poids de la classe, et $\gamma$ (gamma) contrÃ´le le focus sur les exemples difficiles.

\begin{lstlisting}[caption={ImplÃ©mentation de la Focal Loss}]
class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha  # Poids par classe
        self.gamma = gamma  # Focus parameter (2.0 recommande)
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, 
                                  weight=self.alpha, 
                                  reduction='none')
        pt = torch.exp(-ce_loss)  # p_t
        focal_loss = ((1 - pt) ** self.gamma) * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        return focal_loss
\end{lstlisting}

\textbf{Avantage :} Avec $\gamma = 2$, un exemple bien classifiÃ© ($p_t = 0.9$) contribue 100Ã— moins qu'un exemple difficile ($p_t = 0.1$), permettant au modÃ¨le de se concentrer sur les Ã©motions sous-reprÃ©sentÃ©es comme Disgust.

\subsection{Mixup : RÃ©gularisation par Interpolation}

Mixup \cite{zhang2018mixup} crÃ©e de nouveaux exemples d'entraÃ®nement en interpolant linÃ©airement des paires d'images et leurs labels :

\begin{equation}
    \tilde{x} = \lambda x_i + (1 - \lambda) x_j
\end{equation}
\begin{equation}
    \tilde{y} = \lambda y_i + (1 - \lambda) y_j
\end{equation}

oÃ¹ $\lambda \sim \text{Beta}(\alpha, \alpha)$ avec $\alpha = 0.2$.

\begin{lstlisting}[caption={ImplÃ©mentation de Mixup}]
def mixup_data(x, y, alpha=0.2):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)
\end{lstlisting}

\subsection{CutMix : Augmentation par DÃ©coupage}

CutMix \cite{yun2019cutmix} est une variante de Mixup qui dÃ©coupe et colle des rÃ©gions rectangulaires entre images :

\begin{lstlisting}[caption={ImplÃ©mentation de CutMix}]
def cutmix_data(x, y, alpha=1.0):
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    
    # Calcul de la boite de decoupe
    W, H = x.size(2), x.size(3)
    cut_rat = np.sqrt(1. - lam)
    cut_w = int(W * cut_rat)
    cut_h = int(H * cut_rat)
    
    cx = np.random.randint(W)
    cy = np.random.randint(H)
    
    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)
    
    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]
    
    # Ajuster lambda selon la surface decoupee
    lam = 1 - ((bbx2-bbx1)*(bby2-bby1) / (W*H))
    return x, y, y[index], lam
\end{lstlisting}

\textbf{Avantage :} CutMix force le modÃ¨le Ã  utiliser des indices partiels du visage (un Å“il, la bouche seule) plutÃ´t que de dÃ©pendre de l'image entiÃ¨re, amÃ©liorant la robustesse.

\subsection{Label Smoothing}

Le label smoothing rÃ©gularise en remplaÃ§ant les labels hard (one-hot) par des labels soft :

\begin{equation}
    y_\text{smooth} = (1 - \epsilon) \cdot y_\text{hard} + \frac{\epsilon}{K}
\end{equation}

avec $\epsilon = 0.1$ et $K = 8$ classes.

\begin{lstlisting}[caption={Application du Label Smoothing}]
class LabelSmoothingLoss(nn.Module):
    def __init__(self, classes, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
        self.cls = classes
        
    def forward(self, pred, target):
        confidence = 1.0 - self.smoothing
        smooth_val = self.smoothing / (self.cls - 1)
        
        one_hot = torch.zeros_like(pred)
        one_hot.fill_(smooth_val)
        one_hot.scatter_(1, target.unsqueeze(1), confidence)
        
        log_prob = F.log_softmax(pred, dim=1)
        return -(one_hot * log_prob).sum(dim=1).mean()
\end{lstlisting}

\subsection{Augmentation AvancÃ©e avec Albumentations}

Nous utilisons la bibliothÃ¨que Albumentations pour des transformations plus sophistiquÃ©es :

\begin{lstlisting}[caption={Transformations Albumentations}]
import albumentations as A

train_transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(
        shift_limit=0.1,
        scale_limit=0.15,
        rotate_limit=15,
        p=0.5
    ),
    A.OneOf([
        A.MotionBlur(blur_limit=3, p=1.0),
        A.GaussianBlur(blur_limit=3, p=1.0),
        A.GaussNoise(var_limit=(10, 50), p=1.0),
    ], p=0.3),
    A.OneOf([
        A.RandomBrightnessContrast(
            brightness_limit=0.2,
            contrast_limit=0.2,
            p=1.0
        ),
        A.CLAHE(clip_limit=2.0, p=1.0),
    ], p=0.5),
    A.CoarseDropout(
        max_holes=1,
        max_height=12,
        max_width=12,
        fill_value=0,
        p=0.25
    ),
])
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Nouvelles transformations Albumentations}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Transformation} & \textbf{Justification} \\
\midrule
MotionBlur / GaussianBlur & Simule le flou de mouvement en conditions rÃ©elles \\
GaussNoise & Robustesse au bruit de capteur \\
CLAHE & AmÃ©liore le contraste local, meilleur que l'Ã©galisation standard \\
CoarseDropout & Similaire Ã  Cutout, force la redondance des features \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Gradient Accumulation}

Pour simuler des batch sizes plus grands sur GPU avec mÃ©moire limitÃ©e :

\begin{lstlisting}[caption={Gradient Accumulation}]
ACCUMULATION_STEPS = 4  # Effective batch = 32 * 4 = 128

for i, (inputs, targets) in enumerate(train_loader):
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss = loss / ACCUMULATION_STEPS
    loss.backward()
    
    if (i + 1) % ACCUMULATION_STEPS == 0:
        torch.nn.utils.clip_grad_norm_(
            model.parameters(), max_norm=1.0
        )
        optimizer.step()
        optimizer.zero_grad()
\end{lstlisting}

\textbf{Avantage :} Un batch effectif de 128 images stabilise les gradients et amÃ©liore la convergence.

\subsection{Cosine Annealing avec Warm Restarts}

Scheduler qui suit une courbe cosinus avec redÃ©marrages pÃ©riodiques :

\begin{lstlisting}[caption={CosineAnnealingWarmRestarts}]
scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer,
    T_0=10,     # Premiere periode de 10 epochs
    T_mult=2,   # Periodes suivantes x2 (10, 20, 40...)
    eta_min=1e-6
)
\end{lstlisting}

\begin{figure}[H]
\centering
\fbox{\parbox{0.8\textwidth}{\centering
Learning Rate suit une courbe cosinus : \\
$\eta_t = \eta_\text{min} + \frac{1}{2}(\eta_\text{max} - \eta_\text{min})(1 + \cos(\frac{T_\text{cur}}{T_i}\pi))$
}}
\caption{Le learning rate diminue selon une courbe cosinus puis remonte Ã  chaque restart}
\end{figure}

\subsection{Optimisations MatÃ©rielles et Vitesse}

Pour accÃ©lÃ©rer l'entraÃ®nement sur des datasets volumineux, nous avons exploitÃ© les capacitÃ©s modernes de PyTorch 2.0+ :

\begin{itemize}
    \item \textbf{Automatic Mixed Precision (AMP)} : Utilisation de \texttt{float16} pour les calculs tensoriels, rÃ©duisant l'empreinte mÃ©moire et accÃ©lÃ©rant les opÃ©rations sur GPU (Tensor Cores).
    \item \textbf{torch.compile} : Compilation JIT du modÃ¨le avec le mode \texttt{max-autotune}, optimisant le graphe de calcul pour l'architecture spÃ©cifique du GPU.
    \item \textbf{Optimisations CUDA} : Activation de TF32 (TensorFloat-32) et tuning des kernels cuDNN.
\end{itemize}

Ces optimisations permettent d'utiliser des batch sizes plus importants (jusqu'Ã  1536) et de rÃ©duire le temps d'entraÃ®nement par Ã©poque.

%==============================================================================
\section{Application Ã‰tendue avec MediaPipe}
%==============================================================================

Pour amÃ©liorer la dÃ©tection des Ã©motions difficiles et enrichir le panel d'emojis affichables, nous avons intÃ©grÃ© \textbf{MediaPipe}, la bibliothÃ¨que de Google pour l'analyse faciale et gestuelle en temps rÃ©el.

\subsection{MediaPipe Face Mesh : Analyse des Landmarks Faciaux}

Face Mesh dÃ©tecte 468 points de repÃ¨re 3D sur le visage, permettant une analyse fine des caractÃ©ristiques faciales :

\begin{lstlisting}[caption={Initialisation de Face Mesh}]
import mediapipe as mp

mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,  # Points supplementaires iris
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)
\end{lstlisting}

\subsubsection{Features Extraites}

\begin{lstlisting}[caption={Classe FacialAnalyzer pour l'extraction de features}]
class FacialAnalyzer:
    # Indices des landmarks cles
    LEFT_EYE = [33, 160, 158, 133, 153, 144]
    RIGHT_EYE = [362, 385, 387, 263, 373, 380]
    MOUTH = [61, 291, 0, 17, 78, 308]
    LEFT_EYEBROW = [70, 63, 105, 66, 107]
    RIGHT_EYEBROW = [336, 296, 334, 293, 300]
    
    def analyze(self, landmarks):
        features = {}
        
        # Ouverture des yeux (Eye Aspect Ratio)
        features['left_eye_open'] = self._eye_aspect_ratio(
            [landmarks[i] for i in self.LEFT_EYE]
        )
        features['right_eye_open'] = self._eye_aspect_ratio(
            [landmarks[i] for i in self.RIGHT_EYE]
        )
        
        # Ouverture de la bouche
        features['mouth_open'] = self._mouth_aspect_ratio(landmarks)
        
        # Position des sourcils
        features['brow_raise'] = self._brow_raise(landmarks)
        features['brow_squeeze'] = self._brow_squeeze(landmarks)
        
        # Sourire (ratio largeur/hauteur bouche)
        features['smile'] = self._smile_ratio(landmarks)
        
        return features
    
    def _eye_aspect_ratio(self, eye_points):
        # Ratio vertical/horizontal pour detecter clignement
        vertical = np.linalg.norm(
            np.array(eye_points[1]) - np.array(eye_points[5])
        )
        horizontal = np.linalg.norm(
            np.array(eye_points[0]) - np.array(eye_points[3])
        )
        return vertical / (horizontal + 1e-6)
\end{lstlisting}

\subsubsection{Boost des Ã‰motions Difficiles}

Les features faciales sont utilisÃ©es pour renforcer la dÃ©tection des Ã©motions que le CNN a du mal Ã  identifier :

\begin{lstlisting}[caption={SystÃ¨me de boost des Ã©motions}]
EMOTION_BOOST = {
    'Angry': 1.4,     # Boost de 40%
    'Disgust': 1.5,   # Boost de 50%
    'Fear': 1.3,      # Boost de 30%
    'Contempt': 1.4,  # Boost de 40%
    'Neutral': 0.85   # Reduction de 15%
}

def boost_emotions(probs, features):
    boosted = probs.copy()
    
    # Boost Angry si sourcils fronces
    if features.get('brow_squeeze', 0) > 0.6:
        boosted[ANGRY_IDX] *= 1.3
    
    # Boost Disgust si nez fronce
    if features.get('nose_wrinkle', 0) > 0.5:
        boosted[DISGUST_IDX] *= 1.4
    
    # Boost Fear si yeux grands ouverts
    avg_eye = (features['left_eye_open'] + 
               features['right_eye_open']) / 2
    if avg_eye > 0.4:
        boosted[FEAR_IDX] *= 1.2
    
    return boosted / boosted.sum()  # Re-normaliser
\end{lstlisting}

\subsection{Extension du Mapping Emoji}

Au-delÃ  des 8 Ã©motions de base, les features faciales permettent d'afficher des emojis plus nuancÃ©s :

\begin{lstlisting}[caption={Mapping emoji Ã©tendu}]
EXTENDED_EMOJI_MAP = {
    # Emotions de base
    'Happy': 'ğŸ˜Š',
    'Sad': 'ğŸ˜¢',
    'Angry': 'ğŸ˜ ',
    'Surprise': 'ğŸ˜²',
    'Fear': 'ğŸ˜¨',
    'Disgust': 'ğŸ¤¢',
    'Neutral': 'ğŸ˜',
    'Contempt': 'ğŸ˜',
    
    # Combinaisons basees sur features
    'very_happy': 'ğŸ˜',      # Sourire large
    'laughing': 'ğŸ˜‚',        # Bouche ouverte + sourire
    'thinking': 'ğŸ¤”',        # Un sourcil leve
    'sleepy': 'ğŸ˜´',          # Yeux fermes
    'wink': 'ğŸ˜‰',            # Un oeil ferme
    'kiss': 'ğŸ˜˜',            # Bouche en O
    'love': 'ğŸ˜',            # Coeurs si smile fort
    'cool': 'ğŸ˜',            # Neutre + confiant
    'skeptical': 'ğŸ¤¨',       # Sourcil asymetrique
}

def get_extended_emoji(emotion, features):
    # Verifier conditions speciales
    if features['left_eye_open'] < 0.15:
        if features['right_eye_open'] > 0.25:
            return EXTENDED_EMOJI_MAP['wink']
    
    if features['smile'] > 0.7 and features['mouth_open'] > 0.4:
        return EXTENDED_EMOJI_MAP['laughing']
    
    if emotion == 'Happy' and features['smile'] > 0.8:
        return EXTENDED_EMOJI_MAP['very_happy']
    
    return EXTENDED_EMOJI_MAP.get(emotion, 'â“')
\end{lstlisting}

\subsection{MediaPipe Hands : DÃ©tection des Gestes}

Pour enrichir l'interaction, nous avons ajoutÃ© la dÃ©tection des gestes de la main :

\begin{lstlisting}[caption={Initialisation de MediaPipe Hands}]
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.5
)
\end{lstlisting}

\subsubsection{Gestes Reconnus}

\begin{lstlisting}[caption={Classe HandRecognizer}]
class HandRecognizer:
    def recognize_gesture(self, landmarks):
        # Extraire les etats des doigts (leve/baisse)
        fingers = self._get_finger_states(landmarks)
        thumb, index, middle, ring, pinky = fingers
        
        # Pouce leve
        if thumb and not any([index, middle, ring, pinky]):
            return 'thumbs_up', 'ğŸ‘'
        
        # Signe de paix (V)
        if index and middle and not ring and not pinky:
            return 'peace', 'âœŒï¸'
        
        # Signe OK
        if self._is_ok_gesture(landmarks):
            return 'ok', 'ğŸ‘Œ'
        
        # Rock (cornes)
        if index and pinky and not middle and not ring:
            return 'rock', 'ğŸ¤˜'
        
        # Shaka (pouce + auriculaire)
        if thumb and pinky and not index and not middle:
            return 'shaka', 'ğŸ¤™'
        
        # Poing ferme
        if not any(fingers):
            return 'fist', 'âœŠ'
        
        # Main ouverte (tous les doigts)
        if all(fingers):
            return 'wave', 'ğŸ‘‹'
        
        # Pointer
        if index and not middle and not ring and not pinky:
            return 'point', 'ğŸ‘†'
        
        return None, None
    
    def _get_finger_states(self, landmarks):
        # Comparer position des bouts vs articulations
        thumb = landmarks[4].y < landmarks[3].y
        index = landmarks[8].y < landmarks[6].y
        middle = landmarks[12].y < landmarks[10].y
        ring = landmarks[16].y < landmarks[14].y
        pinky = landmarks[20].y < landmarks[18].y
        return [thumb, index, middle, ring, pinky]
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Gestes reconnus et emojis correspondants}
\begin{tabular}{lcc}
\toprule
\textbf{Geste} & \textbf{Description} & \textbf{Emoji} \\
\midrule
Thumbs Up & Pouce levÃ© seul & ğŸ‘ \\
Peace & Index et majeur levÃ©s & âœŒï¸ \\
OK & Pouce et index formant un cercle & ğŸ‘Œ \\
Rock & Index et auriculaire levÃ©s & ğŸ¤˜ \\
Shaka & Pouce et auriculaire & ğŸ¤™ \\
Wave & Tous les doigts levÃ©s & ğŸ‘‹ \\
Fist & Poing fermÃ© & âœŠ \\
Point & Index seul levÃ© & ğŸ‘† \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Optimisations de l'InfÃ©rence (\texttt{app\_v3.py})}
%==============================================================================

La version 3 de l'application intÃ¨gre plusieurs optimisations pour amÃ©liorer la prÃ©cision et la stabilitÃ© des prÃ©dictions.

\subsection{Test-Time Augmentation (TTA)}

Le TTA applique plusieurs transformations Ã  l'image d'entrÃ©e et moyenne les prÃ©dictions :

\begin{lstlisting}[caption={ImplÃ©mentation du TTA}]
class EmotionClassifier:
    def predict_with_tta(self, face_roi, num_augmentations=3):
        predictions = []
        
        # Prediction originale
        predictions.append(self._predict_single(face_roi))
        
        # Flip horizontal
        flipped = cv2.flip(face_roi, 1)
        predictions.append(self._predict_single(flipped))
        
        # Legeres variations de luminosite
        for _ in range(num_augmentations - 2):
            factor = np.random.uniform(0.9, 1.1)
            adjusted = np.clip(face_roi * factor, 0, 255)
            predictions.append(
                self._predict_single(adjusted.astype(np.uint8))
            )
        
        # Moyenne des predictions
        return np.mean(predictions, axis=0)
\end{lstlisting}

\textbf{Avantage :} Le TTA rÃ©duit la variance des prÃ©dictions et amÃ©liore la robustesse aux petites perturbations.

\subsection{PrÃ©traitement CLAHE}

Contrairement Ã  l'Ã©galisation d'histogramme globale, CLAHE (Contrast Limited Adaptive Histogram Equalization) travaille sur des rÃ©gions locales :

\begin{lstlisting}[caption={Application de CLAHE}]
def preprocess_face(self, face_roi):
    # Redimensionner
    face = cv2.resize(face_roi, (48, 48))
    
    # CLAHE pour le contraste local
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))
    face = clahe.apply(face)
    
    # Normalisation
    face = face.astype(np.float32) / 255.0
    
    return face
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Comparaison des mÃ©thodes d'Ã©galisation}
\begin{tabular}{lp{5cm}p{5cm}}
\toprule
\textbf{MÃ©thode} & \textbf{Avantages} & \textbf{InconvÃ©nients} \\
\midrule
Histogramme global & Simple, rapide & Peut sur-amplifier le bruit \\
CLAHE & PrÃ©serve les dÃ©tails locaux, contrÃ´le le contraste & LÃ©gÃ¨rement plus lent \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Lissage Temporel AmÃ©liorÃ©}

La version 3 utilise un lissage exponentiel plus sophistiquÃ© :

\begin{lstlisting}[caption={Lissage temporel avec pondÃ©ration exponentielle}]
SMOOTHING_WINDOW = 7

def get_smoothed_prediction(self, current_probs):
    self.prediction_history.append(current_probs)
    
    if len(self.prediction_history) < 2:
        return current_probs
    
    # Poids exponentiels (frames recentes plus importantes)
    n = len(self.prediction_history)
    weights = np.exp(np.linspace(-1, 0, n))
    weights = weights / weights.sum()
    
    smoothed = np.zeros(8)
    for i, probs in enumerate(self.prediction_history):
        smoothed += weights[i] * probs
    
    return smoothed
\end{lstlisting}

\subsection{Architecture Modulaire}

L'application V3 sÃ©pare clairement les responsabilitÃ©s :

\begin{lstlisting}[caption={Architecture modulaire de app\_v3.py}]
class EmotionClassifier:
    """Gestion du modele CNN et predictions"""
    pass

class FacialAnalyzer:
    """Analyse des landmarks faciaux avec MediaPipe"""
    pass

class HandRecognizer:
    """Detection des gestes de la main"""
    pass

class EmotionApp:
    """Application principale orchestrant les composants"""
    def __init__(self):
        self.classifier = EmotionClassifier()
        self.facial_analyzer = FacialAnalyzer()
        self.hand_recognizer = HandRecognizer()
    
    def process_frame(self, frame):
        # 1. Detecter visage
        # 2. Analyser features faciales
        # 3. Predire emotion avec CNN + TTA
        # 4. Boost avec features
        # 5. Detecter gestes
        # 6. Afficher resultats
        pass
\end{lstlisting}

\subsection{Datasets Alternatifs}

Pour des performances encore meilleures, d'autres datasets sont supportÃ©s :

\begin{table}[H]
\centering
\caption{Comparaison des datasets d'Ã©motions faciales}
\begin{tabular}{lcccl}
\toprule
\textbf{Dataset} & \textbf{Images} & \textbf{RÃ©solution} & \textbf{Classes} & \textbf{AccÃ¨s} \\
\midrule
FER2013 & 35,887 & 48$\times$48 (Gray) & 7 & Gratuit (Kaggle) \\
FER+ & 35,887 & 48$\times$48 (Gray) & 8 & Gratuit (GitHub) \\
\textbf{Dataset UnifiÃ©} & \textbf{41,008} & \textbf{75$\times$75 (RGB)} & \textbf{8} & \textbf{Gratuit (Kaggle)} \\
AffectNet (original) & 450,000 & Variable & 8 & Demande requise \\
RAF-DB & 30,000 & 100$\times$100 & 7 & Demande requise \\
\bottomrule
\end{tabular}
\end{table}

\begin{lstlisting}[caption={Support multi-datasets}]
class AffectNetDataset(Dataset):
    # Mapping AffectNet vers les 7 classes FER
    AFFECTNET_TO_FER = {
        0: 6,  # Neutral
        1: 3,  # Happy
        2: 4,  # Sad
        3: 5,  # Surprise
        4: 2,  # Fear
        5: 1,  # Disgust
        6: 0,  # Anger
        7: 6,  # Contempt -> Neutral
    }
    
    def __getitem__(self, idx):
        image = Image.open(self.images[idx]).convert('L')
        image = image.resize((48, 48))
        label = self.AFFECTNET_TO_FER[self.labels[idx]]
        return image, label
\end{lstlisting}

%==============================================================================
\section{RÃ©sultats Attendus}
%==============================================================================

\subsection{AmÃ©lioration de la PrÃ©cision}

\begin{table}[H]
\centering
\caption{AmÃ©lioration attendue de la prÃ©cision avec le Dataset UnifiÃ©}
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{PrÃ©cision estimÃ©e} & \textbf{Gain} \\
\midrule
FER2013 baseline (grayscale 48$\times$48) & 60-65\% & - \\
Dataset UnifiÃ© (RGB 75$\times$75) & 70-75\% & +10\% \\
+ Data augmentation RGB & 73-78\% & +3\% \\
+ Mixup/CutMix & 76-80\% & +3\% \\
+ Label Smoothing & 78-82\% & +2\% \\
+ TTA Ã  l'infÃ©rence & 80-84\% & +2\% \\
\midrule
\textbf{Total avec optimisations} & \textbf{80-84\%} & \textbf{+20-24\%} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{AmÃ©lioration des Classes Difficiles}

\begin{table}[H]
\centering
\caption{Impact attendu sur les classes (avec dataset Ã©quilibrÃ©)}
\begin{tabular}{lccc}
\toprule
\textbf{Ã‰motion} & \textbf{FER2013 Baseline} & \textbf{Dataset UnifiÃ©} & \textbf{Gain} \\
\midrule
Angry & 55\% & 75\% & +20\% \\
Disgust & 35\% & 72\% & +37\% \\
Fear & 45\% & 70\% & +25\% \\
Contempt & 40\% & 68\% & +28\% \\
\bottomrule
\end{tabular}
\end{table}

\textit{Note : L'amÃ©lioration majeure sur Disgust et Contempt est due au dataset parfaitement Ã©quilibrÃ© et aux images RGB de meilleure qualitÃ©.}

\subsection{AmÃ©lioration de l'ExpÃ©rience Utilisateur}

\begin{itemize}
    \item \textbf{StabilitÃ©} : Le lissage temporel amÃ©liorÃ© Ã©limine le \emph{flickering}
    \item \textbf{Robustesse} : CLAHE gÃ¨re mieux les variations d'Ã©clairage que l'Ã©galisation standard
    \item \textbf{PrÃ©cision TTA} : Le test-time augmentation rÃ©duit la variance des prÃ©dictions
    \item \textbf{Emojis Ã©tendus} : Plus de 15 emojis possibles grÃ¢ce Ã  l'analyse des features faciales
    \item \textbf{Gestes de la main} : 8 gestes reconnus avec emojis correspondants
    \item \textbf{LisibilitÃ©} : Les couleurs et barres de progression amÃ©liorent la comprÃ©hension
    \item \textbf{RÃ©activitÃ©} : Les paramÃ¨tres de dÃ©tection optimisÃ©s amÃ©liorent la fluiditÃ©
\end{itemize}

%==============================================================================
\section{Conclusion}
%==============================================================================

Ce projet a permis de dÃ©velopper un systÃ¨me complet et avancÃ© de reconnaissance d'Ã©motions faciales en temps rÃ©el. Les principales contributions sont :

\begin{enumerate}
    \item \textbf{Architecture CNN optimisÃ©e} avec double convolution, dropout progressif, et Global Average Pooling
    
    \item \textbf{Pipeline d'entraÃ®nement avancÃ©} incluant :
    \begin{itemize}
        \item Focal Loss pour le dÃ©sÃ©quilibre des classes
        \item Mixup et CutMix pour la rÃ©gularisation
        \item Label Smoothing pour la gÃ©nÃ©ralisation
        \item Augmentations avancÃ©es avec Albumentations
    \end{itemize}
    
    \item \textbf{IntÃ©gration MediaPipe} pour :
    \begin{itemize}
        \item Analyse des 468 landmarks faciaux (Face Mesh)
        \item DÃ©tection de 8 gestes de la main
        \item Boost contextuel des Ã©motions difficiles
    \end{itemize}
    
    \item \textbf{Application temps rÃ©el optimisÃ©e (V3)} avec :
    \begin{itemize}
        \item Test-Time Augmentation (TTA)
        \item PrÃ©traitement CLAHE
        \item Lissage temporel exponentiel
        \item Mapping emoji Ã©tendu (15+ emojis)
    \end{itemize}
    
    \item \textbf{Support multi-datasets} permettant d'utiliser FER+, AffectNet, ou RAF-DB
\end{enumerate}

\subsection{Perspectives}

Pour aller plus loin, les amÃ©liorations suivantes pourraient Ãªtre envisagÃ©es :

\begin{itemize}
    \item Utilisation de transfer learning (VGGFace, ResNet prÃ©-entraÃ®nÃ© sur visages)
    \item Architecture avec attention mechanism (Transformer, CBAM)
    \item DÃ©tection multi-tÃ¢ches (Ã©motion + Ã¢ge + genre)
    \item Analyse de la valence et de l'arousal (modÃ¨le dimensionnel)
    \item DÃ©ploiement sur edge devices (quantification INT8, pruning)
    \item IntÃ©gration d'un modÃ¨le audio pour l'analyse multimodale
\end{itemize}

%==============================================================================
\appendix
\section{Structure du Projet}
%==============================================================================

\begin{verbatim}
Final_project/
|-- app.py                   # Application temps reel (version initiale)
|-- app_extended.py          # Version avec MediaPipe Face Mesh
|-- app_extended_v2.py       # Version avec gestes de la main
|-- app_v3.py                # Version optimisee (TTA, CLAHE, modulaire)
|-- model.py                 # Architecture CNN (RGB 75x75)
|-- train_affectnet_notebook.ipynb # Notebook d'entrainement (Multi-source)
|-- train_affectnet.py       # Script d'entrainement (Legacy)
|-- train.py                 # Script d'entrainement (Legacy FER2013)
|-- dataset_affectnet.py     # Dataset Balanced AffectNet
|-- dataset.py               # Dataset FER2013 (legacy)
|-- download_datasets.py     # Script de telechargement des datasets
|-- emotion_model.pth        # Poids du modele entraine
|-- emotion_model_best.pth   # Meilleur modele (early stopping)
|-- data/
|   +-- affectnet/           # Dataset Balanced AffectNet
|       |-- train/
|       |   |-- Anger/
|       |   |-- Contempt/
|       |   |-- Disgust/
|       |   |-- Fear/
|       |   |-- Happy/
|       |   |-- Neutral/
|       |   |-- Sad/
|       |   +-- Surprise/
|       |-- val/
|       +-- test/
|-- report/
|   +-- report.tex           # Ce rapport
+-- README.md
\end{verbatim}

%==============================================================================
\section{RÃ©fÃ©rences}
%==============================================================================

\begin{enumerate}
    \item Goodfellow, I. J., et al. (2013). "Challenges in representation learning: A report on three machine learning contests." \textit{ICML Workshop}.
    
    \item Barsoum, E., et al. (2016). "Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution." \textit{ACM ICMI}.
    
    \item Mollahosseini, A., et al. (2017). "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild." \textit{IEEE Trans. Affective Computing}.
    
    \item Li, S., et al. (2017). "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild." \textit{CVPR}.
    
    \item He, K., et al. (2015). "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification." \textit{ICCV}.
    
    \item Lin, T. Y., et al. (2017). "Focal Loss for Dense Object Detection." \textit{ICCV}. \label{lin2017focal}
    
    \item Zhang, H., et al. (2018). "mixup: Beyond Empirical Risk Minimization." \textit{ICLR}. \label{zhang2018mixup}
    
    \item Yun, S., et al. (2019). "CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features." \textit{ICCV}. \label{yun2019cutmix}
    
    \item Lugaresi, C., et al. (2019). "MediaPipe: A Framework for Building Perception Pipelines." \textit{arXiv preprint arXiv:1906.08172}.
    
    \item Buslaev, A., et al. (2020). "Albumentations: Fast and Flexible Image Augmentations." \textit{Information}, 11(2), 125.
\end{enumerate}

\end{document}
