% chktex-file 8
% chktex-file 18
% chktex-file 26
\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{float}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}

\geometry{margin=2.5cm}

% Python listings configuration
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Python
}

\lstset{style=pythonstyle}

\title{\textbf{Real-Time Facial Emotion Recognition}\\
\large Technical Report - Deep Learning}
\author{Cyril}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
This report presents the development of a real-time facial emotion recognition system using Convolutional Neural Networks (CNN). We detail the initial architecture, the optimizations made, and the technical choices implemented to improve detection performance. The system is capable of classifying eight emotions (anger, disgust, fear, happiness, sadness, surprise, neutrality, and contempt) from a webcam video stream, using a unified multi-source approach (AffectNet + FER+).
\end{abstract}

\tableofcontents
\newpage

%==============================================================================
\section{Introduction}
%==============================================================================

Automatic facial emotion recognition is a rapidly expanding field with applications in human-computer interaction, mental health, marketing, and education. This project implements a complete real-time emotion recognition system, from model training to inference via webcam.

\subsection{Objectives}
\begin{itemize}
    \item Develop a CNN model capable of classifying 8 facial emotions (7 from FER2013 + Contempt from FER+).
    \item Implement a real-time application with face detection.
    \item Optimize performance for smooth usage.
    \item Improve accuracy through advanced deep learning techniques.
\end{itemize}

%==============================================================================
\section{Initial Architecture}
%==============================================================================

\subsection{Dataset: Unified Multi-Source Approach}

To maximize model robustness, we adopted a dataset fusion strategy. The system now uses a **unified 8-class dataset** combining:

\begin{itemize}
    \item \textbf{Balanced AffectNet}: Main source ($\sim$41k images, RGB 75$\times$75).
    \item \textbf{FER+}: Improved version of FER2013 with labels corrected by voting (Microsoft), upscaled to 75$\times$75.
\end{itemize}

\textbf{Advantages of this approach:}
\begin{itemize}
    \item \textbf{Increased Data Volume}: Combines the diversity of AffectNet with the hard cases of FER+.
    \item \textbf{High-Quality Labels}: Exclusive use of verified datasets (FER+ vs original FER2013).
    \item \textbf{Unified Mapping}: All sources are mapped to the 8 standard emotions (Anger, Disgust, Fear, Happy, Sad, Surprise, Neutral, Contempt).
\end{itemize}

\subsection{Initial Model (\texttt{model.py})}

The initial architecture is a simple CNN with 3 convolutional blocks:

\begin{lstlisting}[caption={CNN Architecture for Balanced AffectNet}]
class FaceEmotionCNN(nn.Module):
    def __init__(self, num_classes=8, in_channels=3, input_size=75):
        super(FaceEmotionCNN, self).__init__()
        # Convolutional Block 1 (75x75 -> 37x37)
        self.conv1a = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)
        self.bn1a = nn.BatchNorm2d(64)
        self.conv1b = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.bn1b = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.dropout1 = nn.Dropout(0.1)
        
        # ... Blocks 2, 3, 4 similar ...
        
        # Global Average Pooling
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        
        # Fully Connected Layers
        self.fc1 = nn.Linear(512, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, num_classes)  # 8 emotions
\end{lstlisting}

\subsubsection{Analysis of the Initial Architecture}

\begin{table}[H]
\centering
\caption{Dimensions through the network (Balanced AffectNet)}
\begin{tabular}{lccc}
\toprule
\textbf{Layer} & \textbf{Input} & \textbf{Output} & \textbf{Parameters} \\
\midrule
Block1 (2$\times$Conv) + Pool & $3 \times 75 \times 75$ & $64 \times 37 \times 37$ & 38,592 \\
Block2 (2$\times$Conv) + Pool & $64 \times 37 \times 37$ & $128 \times 18 \times 18$ & 147,712 \\
Block3 (2$\times$Conv) + Pool & $128 \times 18 \times 18$ & $256 \times 9 \times 9$ & 590,336 \\
Block4 (2$\times$Conv) + Pool & $256 \times 9 \times 9$ & $512 \times 4 \times 4$ & 2,360,320 \\
Global Avg Pool & $512 \times 4 \times 4$ & 512 & 0 \\
FC1 + BN & 512 & 256 & 131,584 \\
FC2 + BN & 256 & 128 & 33,024 \\
FC3 & 128 & 8 & 1,032 \\
\midrule
\textbf{Total} & & & \textbf{$\approx$ 3.3M} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Pros:}
\begin{itemize}
    \item Use of Batch Normalization to stabilize training.
    \item Dropout (50\%) to regularize and avoid overfitting.
    \item Simple architecture, fast to train.
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
    \item Few convolutional layers (low feature extraction capacity).
    \item Limited number of filters (32-64-128).
    \item No regularization in convolutional layers.
    \item Very large FC1 layer (2.3M parameters) creating a bottleneck.
\end{itemize}

\subsection{Initial Training Script (\texttt{train.py})}

\begin{lstlisting}[caption={Initial Training Script}]
# Hyperparameters
BATCH_SIZE = 64
LEARNING_RATE = 0.001
EPOCHS = 25

# Simple transformations
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.ToTensor(),
])

# Dataset loading
dataset = FER2013Dataset('./data/fer2013/fer2013.csv', transform=transform)
train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# Model and optimizer
model = FaceEmotionCNN().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# Training loop
for epoch in range(EPOCHS):
    running_loss = 0.0
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    
    print(f"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}")

torch.save(model.state_dict(), 'emotion_model.pth')
\end{lstlisting}

\textbf{Limitations of Initial Training:}
\begin{itemize}
    \item No data augmentation (risk of overfitting).
    \item No train/validation split (impossible to detect overfitting).
    \item Fixed learning rate (suboptimal convergence).
    \item No early stopping (resource waste).
    \item No class imbalance management.
\end{itemize}

\subsection{Initial Application (\texttt{app.py})}

The initial application performs:
\begin{enumerate}
    \item Video capture via webcam.
    \item Face detection with Haar Cascade.
    \item Image preprocessing (resize 48$\times$48, grayscale).
    \item Inference with the CNN model.
    \item Display of detected emotion with emoji.
\end{enumerate}

\begin{lstlisting}[caption={Main loop of the initial application}]
while True:
    ret, frame = cap.read()
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray_frame, 
                                          scaleFactor=1.3, 
                                          minNeighbors=5)

    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)
        roi_gray = gray_frame[y:y+h, x:x+w]
        
        # Preprocessing
        roi_tensor = data_transform(roi_gray).unsqueeze(0).to(device)

        # Prediction
        with torch.no_grad():
            outputs = model(roi_tensor)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            max_prob, predicted_idx = torch.max(probabilities, 1)
            
            idx = predicted_idx.item()
            confidence = max_prob.item() * 100
            emotion_text = emotion_dict[idx]
\end{lstlisting}

\textbf{Limitations:}
\begin{itemize}
    \item Unstable predictions (oscillations between frames).
    \item No lighting normalization.
    \item Suboptimal face detection parameters.
    \item Basic user interface.
\end{itemize}

%==============================================================================
\section{Architecture Optimizations}
%==============================================================================

\subsection{New CNN Architecture}

The improved architecture is designed to process RGB 75$\times$75 images and integrates Squeeze-and-Excitation (SE) attention blocks:

\begin{lstlisting}[caption={Improved CNN Architecture with SE Blocks}]
class ConvBlock(nn.Module):
    def __init__(self, in_c, out_c, use_se=True):
        super().__init__()
        self.conv1 = nn.Conv2d(in_c, out_c, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_c)
        self.conv2 = nn.Conv2d(out_c, out_c, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_c)
        self.pool = nn.MaxPool2d(2, 2)
        self.se = SEBlock(out_c) if use_se else nn.Identity()
        
    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.se(x)
        return self.pool(x)

class FaceEmotionCNN(nn.Module):
    def __init__(self, num_classes=8):
        super().__init__()
        # Input: 75x75 RGB
        self.block1 = ConvBlock(3, 32, use_se=True)    # -> 37x37
        self.block2 = ConvBlock(32, 64, use_se=True)   # -> 18x18
        self.block3 = ConvBlock(64, 128, use_se=True)  # -> 9x9
        self.block4 = ConvBlock(128, 256, use_se=True) # -> 4x4
        
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )
\end{lstlisting}

\subsubsection{Justification of Architectural Choices}

\begin{enumerate}
    \item \textbf{Squeeze-and-Excitation (SE) Blocks} \\
    These blocks dynamically recalibrate feature maps per channel, allowing the network to focus on the most relevant features for the emotion.
    
    \item \textbf{Double Convolution per Block} \\
    Increases depth and non-linearity without exploding the number of parameters.
    
    \item \textbf{Global Average Pooling (GAP)} \\
    Replaces traditional flattening, drastically reducing parameters and the risk of overfitting.
    
    \item \textbf{RGB 75$\times$75 Input} \\
    Unlike classic models on FER2013 (48$\times$48 Grayscale), using color and higher resolution allows capturing finer details.
\end{enumerate}

\begin{table}[H]
\centering
\caption{Architecture Comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Feature} & \textbf{Initial} & \textbf{Improved} \\
\midrule
Input & 48$\times$48 Gray & 75$\times$75 RGB \\
Conv Blocks & 3 & 4 \\
Max Filters & 128 & 256 \\
Attention & No & SE-Blocks \\
Global Average Pooling & No & Yes \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Training Optimizations}
%==============================================================================

\subsection{Data Augmentation}

Data augmentation is crucial for a small dataset like FER2013. It artificially increases the diversity of training data.

\begin{lstlisting}[caption={Data Augmentation Transformations}]
train_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(10),
    transforms.RandomAffine(
        degrees=0, 
        translate=(0.1, 0.1),
        scale=(0.9, 1.1)
    ),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
])
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Data Augmentation Transformations}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Transformation} & \textbf{Justification} \\
\midrule
RandomHorizontalFlip (p=0.5) & Expressions are symmetric. Effectively doubles the dataset. \\
RandomRotation (±10°) & Simulates natural slight head tilts. \\
RandomAffine (translate) & Compensates for face position variations in the frame. \\
RandomAffine (scale 0.9-1.1) & Simulates different camera-face distances. \\
ColorJitter (brightness, contrast) & Robustness to lighting variations. \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Train/Validation Split}

\begin{lstlisting}[caption={Dataset Split}]
VALIDATION_SPLIT = 0.15  # 15% for validation

val_size = int(len(full_dataset) * VALIDATION_SPLIT)
train_size = len(full_dataset) - val_size

train_dataset, val_dataset = random_split(
    full_dataset, 
    [train_size, val_size],
    generator=torch.Generator().manual_seed(42)
)
\end{lstlisting}

\textbf{Importance:} Validation allows to:
\begin{itemize}
    \item Detect overfitting (train loss $\downarrow$ but val loss $\uparrow$).
    \item Select the best model.
    \item Tune hyperparameters.
\end{itemize}

\subsection{Early Stopping}

Early stopping halts training when validation no longer improves:

\begin{lstlisting}[caption={Early Stopping Implementation}]
PATIENCE = 7
best_val_acc = 0.0
patience_counter = 0

for epoch in range(EPOCHS):
    # ... training ...
    val_loss, val_acc = validate(model, val_loader, criterion, device)
    
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        patience_counter = 0
        torch.save(model.state_dict(), 'emotion_model_best.pth')
    else:
        patience_counter += 1
        if patience_counter >= PATIENCE:
            print("Early stopping triggered!")
            break
\end{lstlisting}

\subsection{Learning Rate Scheduler}

The scheduler reduces the learning rate when validation stagnates:

\begin{lstlisting}[caption={Learning Rate Scheduler}]
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='min',      # Monitors loss
    factor=0.5,      # Divides LR by 2
    patience=3       # Waits 3 epochs without improvement
)

# In the training loop
scheduler.step(val_loss)
\end{lstlisting}

\textbf{Principle:} A high LR at the beginning allows fast convergence, then a lower LR allows precise fine-tuning.

\subsection{AdamW Optimizer}

\begin{lstlisting}[caption={AdamW Optimizer with Weight Decay}]
optimizer = optim.AdamW(
    model.parameters(), 
    lr=LEARNING_RATE, 
    weight_decay=1e-4
)
\end{lstlisting}

AdamW corrects an issue in Adam: weight decay is applied directly to weights rather than gradients, resulting in better L2 regularization.

\subsection{Gradient Clipping}

\begin{lstlisting}[caption={Gradient Clipping for Stability}]
loss.backward()
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()
\end{lstlisting}

Prevents exploding gradients which can destabilize training.

%==============================================================================
\section{Real-Time Application Optimizations}
%==============================================================================

\subsection{Temporal Smoothing of Predictions}

The main problem with frame-by-frame predictions is instability: the displayed emotion can change rapidly between successive frames, creating an unpleasant \emph{flickering} effect.

\begin{lstlisting}[caption={Temporal Smoothing with Weighted Average}]
from collections import deque

SMOOTHING_WINDOW = 5
prediction_history = deque(maxlen=SMOOTHING_WINDOW)

def get_smoothed_prediction(current_probs):
    prediction_history.append(current_probs.cpu().numpy())
    
    if len(prediction_history) < 2:
        return current_probs
    
    # Weighted average (recent frames = more weight)
    weights = np.linspace(0.5, 1.0, len(prediction_history))
    weights = weights / weights.sum()
    
    smoothed = np.zeros(7)
    for i, probs in enumerate(prediction_history):
        smoothed += weights[i] * probs
    
    return torch.tensor(smoothed)
\end{lstlisting}

\textbf{Principle:} Instead of using only the current frame's prediction, we calculate a weighted average over the last $N$ frames. Recent frames have more weight to maintain responsiveness.

\subsection{Histogram Equalization}

\begin{lstlisting}[caption={Lighting Normalization}]
# Before preprocessing
roi_gray = cv2.equalizeHist(roi_gray)
\end{lstlisting}

Histogram equalization normalizes the distribution of gray levels, making the model more robust to lighting variations.

\begin{figure}[H]
\centering
\fbox{\parbox{0.8\textwidth}{\centering
\textbf{Dark Image} $\longrightarrow$ \textbf{Equalized Histogram}
}}
\caption{Equalization redistributes intensities across the full [0, 255] range}
\end{figure}

\subsection{Optimized Detection Parameters}

\begin{lstlisting}[caption={Optimized Face Detection Parameters}]
faces = face_cascade.detectMultiScale(
    gray_frame, 
    scaleFactor=1.1,   # More precise (was 1.3)
    minNeighbors=5,
    minSize=(48, 48),  # Minimum size
    flags=cv2.CASCADE_SCALE_IMAGE
)
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Impact of Detection Parameters}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{Low Value} & \textbf{High Value} \\
\midrule
scaleFactor & More precise, slower & Less precise, faster \\
minNeighbors & More false positives & Fewer detections \\
minSize & Detects small faces & Ignores small faces \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Improved User Interface}

\begin{itemize}
    \item \textbf{Colors per Emotion}: Each emotion has a distinctive color.
    \item \textbf{Progress Bars}: Display probabilities of all classes.
    \item \textbf{Text Background}: Improves readability.
\end{itemize}

\begin{lstlisting}[caption={Colors per Emotion}]
emotion_colors = {
    0: (0, 0, 255),     # Red - Angry
    1: (0, 128, 0),     # Dark Green - Disgust
    2: (128, 0, 128),   # Purple - Fear
    3: (0, 255, 255),   # Yellow - Happy
    4: (255, 0, 0),     # Blue - Sad
    5: (0, 165, 255),   # Orange - Surprise
    6: (128, 128, 128)  # Gray - Neutral
}
\end{lstlisting}

%==============================================================================
\section{Dataset Improvement}
%==============================================================================

\subsection{Advantages of Multi-Source Approach}

Compared to FER2013, our unified dataset presents several major advantages:

\begin{enumerate}
    \item \textbf{Higher Resolution}: 75$\times$75 pixel images (vs 48$\times$48).
    \item \textbf{RGB Images}: 3 color channels offering more information.
    \begin{table}[H]
    \centering
    \begin{tabular}{lcc}
    \toprule
    \textbf{Emotion} & \textbf{Number of Images} & \textbf{Percentage} \\
    \midrule
    Angry & 5,126 & 12.5\% \\
    Contempt & 5,126 & 12.5\% \\
    Disgust & 5,126 & 12.5\% \\
    Fear & 5,126 & 12.5\% \\
    Happy & 5,126 & 12.5\% \\
    Neutral & 5,126 & 12.5\% \\
    Sad & 5,126 & 12.5\% \\
    Surprise & 5,126 & 12.5\% \\
    \bottomrule
    \end{tabular}
    \caption{Class distribution in the unified dataset.}
    \end{table}
    
    \item \textbf{Balanced Dataset}: No class bias, unlike FER2013.
    \item \textbf{8 Native Classes}: Includes Contempt from the start.
    \item \textbf{Better Quality}: More reliable annotations.
\end{enumerate}

\subsection{Dataset Loading}

The unified dataset is organized in folders per emotion:

\begin{lstlisting}[caption={Using the Unified Dataset}]
class UnifiedDataset(Dataset):
    EMOTION_CLASSES = {
        'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3,
        'Sad': 4, 'Surprise': 5, 'Neutral': 6, 'Contempt': 7,
    }
    
    def __init__(self, root_dir, split='train', transform=None):
        self.images = []
        self.labels = []
        
        for emotion_name, emotion_idx in self.EMOTION_CLASSES.items():
            emotion_dir = os.path.join(root_dir, split, emotion_name)
            for img_name in os.listdir(emotion_dir):
                self.images.append(os.path.join(emotion_dir, img_name))
                self.labels.append(emotion_idx)
    
    def __getitem__(self, idx):
        image = Image.open(self.images[idx]).convert('RGB')
        image = np.array(image)  # 75x75x3
        label = self.labels[idx]
        
        if self.transform:
            image = self.transform(image=image)['image']
        
        return image, label
\end{lstlisting}

\subsection{Class Balancing}

To compensate for imbalance (especially the lack of \textit{Disgust}), two techniques are implemented:

\subsubsection{Class Weights in Loss}

\begin{lstlisting}[caption={CrossEntropyLoss with Class Weights}]
def get_class_weights(dataset):
    class_counts = np.bincount(labels, minlength=7)
    weights = 1.0 / class_counts
    weights = weights / weights.sum() * len(weights)
    return torch.FloatTensor(weights)

class_weights = get_class_weights(train_dataset)
criterion = nn.CrossEntropyLoss(weight=class_weights)
\end{lstlisting}

The weight of each class is inversely proportional to its frequency:
\begin{equation}
    w_c = \frac{N}{N_c \times C}
\end{equation}
where $N$ is the total number of samples, $N_c$ the number of samples of class $c$, and $C$ the number of classes.

\subsubsection{WeightedRandomSampler}

\begin{lstlisting}[caption={Balanced Sampler}]
def get_balanced_sampler(dataset):
    class_counts = np.bincount(labels, minlength=7)
    weights = 1.0 / class_counts
    sample_weights = weights[labels]
    
    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )
    return sampler
\end{lstlisting}

The sampler oversamples minority classes during training.

%==============================================================================
\section{Advanced Training Techniques}
%==============================================================================

To significantly improve performance, especially on difficult classes (Disgust, Contempt, Fear, Angry), we developed an advanced training workflow (Jupyter notebook) integrating the latest deep learning techniques.

\subsection{Label Smoothing}

Label smoothing regularizes by replacing hard labels (one-hot) with soft labels, preventing the model from being too confident in its predictions and improving generalization:

\begin{equation}
    y_\text{smooth} = (1 - \epsilon) \cdot y_\text{hard} + \frac{\epsilon}{K}
\end{equation}

with $\epsilon = 0.1$ and $K = 8$ classes.

\begin{lstlisting}[caption={Label Smoothing Application}]
class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
        
    def forward(self, inputs, targets):
        n_classes = inputs.size(-1)
        log_probs = F.log_softmax(inputs, dim=-1)
        
        targets_smooth = torch.zeros_like(log_probs)
        targets_smooth.fill_(self.smoothing / (n_classes - 1))
        targets_smooth.scatter_(1, targets.unsqueeze(1), 1.0 - self.smoothing)
        
        loss = -(targets_smooth * log_probs).sum(dim=-1)
        return loss.mean()
\end{lstlisting}

\subsection{Mixup: Regularization by Interpolation}

Mixup \cite{zhang2018mixup} creates new training examples by linearly interpolating pairs of images and their labels:

\begin{equation}
    \tilde{x} = \lambda x_i + (1 - \lambda) x_j
\end{equation}
\begin{equation}
    \tilde{y} = \lambda y_i + (1 - \lambda) y_j
\end{equation}

where $\lambda \sim \text{Beta}(\alpha, \alpha)$ with $\alpha = 0.2$.

\begin{lstlisting}[caption={Mixup Implementation}]
def mixup_data(x, y, alpha=0.2):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam
\end{lstlisting}

\subsection{Advanced Augmentation with Albumentations}

We use the Albumentations library for more sophisticated transformations:

\begin{lstlisting}[caption={Albumentations Transformations}]
import albumentations as A

train_transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.Affine(
        translate_percent={"x": (-0.05, 0.05), "y": (-0.05, 0.05)},
        scale=(0.95, 1.05),
        rotate=(-10, 10),
        p=0.4
    ),
    A.OneOf([
        A.GaussNoise(std_range=(0.02, 0.08), p=1),
        A.GaussianBlur(blur_limit=(3, 5), p=1),
    ], p=0.2),
    A.OneOf([
        A.RandomBrightnessContrast(p=1),
        A.RandomGamma(p=1),
        A.HueSaturationValue(p=1),
    ], p=0.4),
    A.CoarseDropout(
        num_holes_range=(1, 2),
        hole_height_range=(4, 8),
        hole_width_range=(4, 8),
        fill=0,
        p=0.2
    ),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])
\end{lstlisting}

\subsection{OneCycleLR Scheduler}

We use the \textbf{OneCycleLR} learning rate policy, which increases the learning rate to a maximum and then decreases it rapidly. This allows for faster convergence and better generalization ("super-convergence" phenomenon).

\begin{lstlisting}[caption={OneCycleLR Scheduler}]
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=config.LEARNING_RATE * 10,
    epochs=config.EPOCHS,
    steps_per_epoch=len(train_loader),
    pct_start=0.3,
    anneal_strategy='cos'
)
\end{lstlisting}

\subsection{Hardware Optimizations and Speed}

To accelerate training on large datasets, we leveraged modern PyTorch 2.0+ capabilities:

\begin{itemize}
    \item \textbf{Automatic Mixed Precision (AMP)}: Using \texttt{float16} for tensor calculations, reducing memory footprint and accelerating operations on GPU (Tensor Cores).
    \item \textbf{torch.compile}: JIT compilation of the model with \texttt{max-autotune} mode, optimizing the computation graph for the specific GPU architecture.
    \item \textbf{Large Batch Size}: Thanks to memory optimizations, we use a batch size of \textbf{1536}, which stabilizes gradients and significantly speeds up training.
    \item \textbf{CUDA Optimizations}: Enabling TF32 (TensorFloat-32) and tuning cuDNN kernels.
\end{itemize}

%==============================================================================
\section{Final Application (\texttt{app\_v3.py})}
%==============================================================================

The final application (\texttt{app\_v3.py}) integrates all optimizations: emotion recognition by CNN, facial analysis via MediaPipe, and gesture detection.

\subsection{MediaPipe Face Mesh: Facial Landmark Analysis}

Face Mesh detects 468 3D landmarks on the face, allowing fine analysis of facial features to refine emotion prediction:

\begin{lstlisting}[caption={FacialAnalyzer Class (excerpt)}]
class FacialAnalyzer:
    def analyze(self, frame_rgb):
        results = self.face_mesh.process(frame_rgb)
        features = {}
        
        # Mouth opening calculation
        mouth_h = dist(13, 14)
        mouth_w = dist(61, 291)
        features["mouth_open"] = np.clip(mouth_h / (face_h * 0.12), 0, 1)
        
        # Smile detection
        if mouth_h > 1:
            smile_ratio = mouth_w / mouth_h
            features["smile"] = np.clip((smile_ratio - 2.5) / 4.0, 0, 1)
            
        # Asymmetry (for Contempt)
        left_corner = point(61)
        right_corner = point(291)
        asymmetry = abs(left_corner[1] - right_corner[1]) / (face_h * 0.05)
        features["asymmetric_mouth"] = np.clip(asymmetry, 0, 1)
        
        return features
\end{lstlisting}

\subsection{MediaPipe Hands: Gesture Detection}

To enrich interaction, we added hand gesture detection:

\begin{lstlisting}[caption={HandRecognizer Class (excerpt)}]
class HandRecognizer:
    def _classify(self, fingers, landmarks):
        thumb, index, middle, ring, pinky = fingers
        
        # Thumbs up
        if thumb and not any([index, middle, ring, pinky]):
            return "thumbs_up"
            
        # Peace (V sign)
        if not thumb and index and middle and not ring and not pinky:
            return "peace"
            
        # OK sign
        if thumb and index:
            # ... thumb-index distance verification ...
            return "ok"
            
        # Rock
        if not thumb and index and not middle and not ring and pinky:
            return "rock"
            
        return None
\end{lstlisting}

\subsection{Extended Emoji Mapping}

The application combines the emotion predicted by the CNN, facial features (wink, smile), and gestures to display the most relevant emoji.

\begin{table}[H]
\centering
\caption{Examples of Emoji Mapping}
\begin{tabular}{lll}
\toprule
\textbf{CNN Emotion} & \textbf{Condition} & \textbf{Emoji} \\
\midrule
Happy & Smile > 0.6 & Grinning Face \\
Happy & Eyes Closed > 0.5 & Smiling Face with Smiling Eyes \\
Any & Wink Detected & Winking Face \\
Angry & Mouth Open > 0.5 & Face with Symbols on Mouth \\
Disgust & Nose Wrinkle > 0.3 & Nauseated Face \\
Neutral & Brow Raised > 0.4 & Thinking Face \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Inference Optimizations}
%==============================================================================

\subsection{Test-Time Augmentation (TTA)}

TTA applies multiple transformations to the input image and averages the predictions to reduce variance:

\begin{lstlisting}[caption={TTA Implementation}]
def predict(self, face_roi, use_tta=True):
    if use_tta:
        all_probs = []
        for transform in self.tta_transforms:
            # Apply transformation (e.g., Flip)
            tensor = transform(face_roi).unsqueeze(0).to(self.device)
            output = self.model(tensor)
            all_probs.append(F.softmax(output, dim=1)[0])
        
        # Average probabilities
        probs = torch.stack(all_probs).mean(dim=0)
    else:
        # ... simple prediction ...
\end{lstlisting}

\subsection{CLAHE Preprocessing}

We use CLAHE (Contrast Limited Adaptive Histogram Equalization) to improve local contrast and robustness to lighting variations, applied to each RGB channel.

\subsection{Exponential Temporal Smoothing}

To avoid prediction flickering, we use an exponential weighted average over the last frames:

\begin{lstlisting}[caption={Temporal Smoothing}]
# Exponential weights (recent frames more important)
weights = np.exp(np.linspace(-1, 0, len(self.history)))
weights /= weights.sum()

smoothed = np.zeros(NUM_CLASSES)
for i, h in enumerate(self.history):
    smoothed += weights[i] * h
\end{lstlisting}

%==============================================================================
\section{Expected Results}
%==============================================================================

\subsection{Accuracy Improvement}

\begin{table}[H]
\centering
\caption{Expected Accuracy Improvement with Unified Dataset}
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{Estimated Accuracy} & \textbf{Gain} \\
\midrule
FER2013 baseline (grayscale 48$\times$48) & 60-65\% & - \\
Unified Dataset (RGB 75$\times$75) & 70-75\% & +10\% \\
+ Data augmentation RGB & 73-78\% & +3\% \\
+ Mixup/CutMix & 76-80\% & +3\% \\
+ Label Smoothing & 78-82\% & +2\% \\
+ TTA at inference & 80-84\% & +2\% \\
\midrule
\textbf{Total with optimizations} & \textbf{80-84\%} & \textbf{+20-24\%} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Improvement on Difficult Classes}

\begin{table}[H]
\centering
\caption{Expected Impact on Classes (with balanced dataset)}
\begin{tabular}{lccc}
\toprule
\textbf{Emotion} & \textbf{FER2013 Baseline} & \textbf{Unified Dataset} & \textbf{Gain} \\
\midrule
Angry & 55\% & 75\% & +20\% \\
Disgust & 35\% & 72\% & +37\% \\
Fear & 45\% & 70\% & +25\% \\
Contempt & 40\% & 68\% & +28\% \\
\bottomrule
\end{tabular}
\end{table}

\textit{Note: The major improvement on Disgust and Contempt is due to the perfectly balanced dataset and higher quality RGB images.}

\subsection{User Experience Improvement}

\begin{itemize}
    \item \textbf{Stability}: Improved temporal smoothing eliminates \emph{flickering}.
    \item \textbf{Robustness}: CLAHE handles lighting variations better than standard equalization.
    \item \textbf{TTA Accuracy}: Test-time augmentation reduces prediction variance.
    \item \textbf{Extended Emojis}: Over 15 possible emojis thanks to facial feature analysis.
    \item \textbf{Hand Gestures}: 8 recognized gestures with corresponding emojis.
    \item \textbf{Readability}: Colors and progress bars improve understanding.
    \item \textbf{Responsiveness}: Optimized detection parameters improve fluidity.
\end{itemize}

%==============================================================================
\section{Conclusion}
%==============================================================================

This project has enabled the development of a complete and advanced real-time facial emotion recognition system. The main contributions are:

\begin{enumerate}
    \item \textbf{Optimized CNN Architecture} for RGB 75$\times$75 images, integrating Squeeze-and-Excitation (SE) attention blocks and Global Average Pooling.
    
    \item \textbf{Advanced Training Pipeline} including:
    \begin{itemize}
        \item Label Smoothing to improve generalization.
        \item Mixup for regularization.
        \item OneCycleLR Scheduler for fast convergence.
        \item Hardware optimizations (Mixed Precision, torch.compile) allowing a batch size of 1536.
    \end{itemize}
    
    \item \textbf{Final Real-Time Application (\texttt{app\_v3.py})} combining:
    \begin{itemize}
        \item Multimodal analysis (CNN + MediaPipe Face Mesh + Hands).
        \item Test-Time Augmentation (TTA) and CLAHE preprocessing.
        \item Exponential temporal smoothing.
        \item Extended and interactive emoji mapping.
    \end{itemize}
    
    \item \textbf{Unified Dataset} combining AffectNet and FER+ for increased robustness (8 balanced classes).
\end{enumerate}

\subsection{Perspectives}

To go further, the following improvements could be considered:

\begin{itemize}
    \item Use of transfer learning (VGGFace, ResNet pre-trained on faces).
    \item Architecture with attention mechanism (Transformer, CBAM).
    \item Multi-task detection (emotion + age + gender).
    \item Valence and arousal analysis (dimensional model).
    \item Deployment on edge devices (INT8 quantization, pruning).
    \item Integration of an audio model for multimodal analysis.
\end{itemize}

%==============================================================================
\appendix
\section{Project Structure}
%==============================================================================

\begin{verbatim}
Final_project/
|-- app.py                   # Real-time application (initial version)
|-- app_v3.py                # Optimized version (TTA, CLAHE, MediaPipe)
|-- model.py                 # CNN Architecture (RGB 75x75)
|-- training.ipynb           # Complete training notebook (Multi-source)
|-- emotion_model.pth        # Trained model weights
|-- emotion_model_best.pth   # Best model (early stopping)
|-- data/
|   +-- affectnet/           # Balanced AffectNet Dataset
|   +-- fer2013/             # FER2013 Dataset
|-- report/
|   +-- report.tex           # This report
+-- README.md
\end{verbatim}

%==============================================================================
\begin{thebibliography}{9}
%==============================================================================

\bibitem{goodfellow2013}
Goodfellow, I. J., et al. (2013). "Challenges in representation learning: A report on three machine learning contests." \textit{ICML Workshop}.

\bibitem{barsoum2016}
Barsoum, E., et al. (2016). "Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution." \textit{ACM ICMI}.

\bibitem{mollahosseini2017}
Mollahosseini, A., et al. (2017). "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild." \textit{IEEE Trans. Affective Computing}.

\bibitem{li2017}
Li, S., et al. (2017). "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild." \textit{CVPR}.

\bibitem{he2015}
He, K., et al. (2015). "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification." \textit{ICCV}.

\bibitem{zhang2018mixup}
Zhang, H., et al. (2018). "mixup: Beyond Empirical Risk Minimization." \textit{ICLR}.

\bibitem{lugaresi2019}
Lugaresi, C., et al. (2019). "MediaPipe: A Framework for Building Perception Pipelines." \textit{arXiv preprint arXiv:1906.08172}.

\bibitem{buslaev2020}
Buslaev, A., et al. (2020). "Albumentations: Fast and Flexible Image Augmentations." \textit{Information}, 11(2), 125.

\bibitem{hu2018}
Hu, J., et al. (2018). "Squeeze-and-Excitation Networks." \textit{CVPR}.

\end{thebibliography}

\end{document}
