# Real-Time Facial Expression Recognition for Automatic Emoji Matching

A deep learning application that detects facial expressions in real-time using your webcam and displays the corresponding emoji.

![Python](https://img.shields.io/badge/Python-3.11-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red)
![OpenCV](https://img.shields.io/badge/OpenCV-4.0+-green)

## Features

- ğŸ¥ Real-time face detection using Haar Cascades
- ğŸ§  CNN-based emotion classification (8 emotions)
- ğŸ˜ƒ Automatic emoji overlay on detected faces
- ğŸ“‹ Copy emoji to clipboard with 's' key

## Supported Emotions

| Emotion | Emoji |
|---------|-------|
| Angry | ğŸ˜  |
| Disgust | ğŸ¤¢ |
| Fear | ğŸ˜¨ |
| Happy | ğŸ˜ƒ |
| Sad | ğŸ˜¢ |
| Surprise | ğŸ˜² |
| Neutral | ğŸ˜ |
| Contempt | ğŸ˜ |

## Dataset

Training now uses a **multi-source, unified 8-class dataset**:
- **Balanced AffectNet (RGB, 75Ã—75)** â€” main source (~41k images, 8 classes)
- **FER+ (48Ã—48 â†’ upscaled to 75Ã—75)** â€” FER2013 images with Microsoft-voted labels (adds **Contempt**)
- (Optional) **FER2013** â€” legacy labels (7 classes); avoid mixing with FER+ at the same time because they share images.

The notebook auto-downloads datasets with `kagglehub` and builds a combined loader that maps every source to the same 8 emotions: Anger, Disgust, Fear, Happy, Sad, Surprise, Neutral, Contempt.

Manual placement (if you download yourself):
```
data/
â”œâ”€â”€ affectnet/
â”‚   â”œâ”€â”€ train|val|test/Anger|Disgust|Fear|Happy|Sad|Surprise|Neutral|Contempt/
â”œâ”€â”€ ferplus_generated/           # produced from FER2013 CSV + fer2013new.csv
â”‚   â”œâ”€â”€ FER2013Train|FER2013Valid|FER2013Test/
â”‚   â””â”€â”€ fer2013new.csv
â””â”€â”€ fer2013/ (optional legacy, 7 classes)
```

## Installation

### 1. Clone the repository

```bash
git clone https://github.com/CyrilHup/real_time_facial_expression_recognition_for_automatic_emoji_matching.git
cd real_time_facial_expression_recognition_for_automatic_emoji_matching
```

### 2. Create a conda environment

```bash
conda create -n fer_project python=3.11 -y
conda activate fer_project
```

### 3. Install dependencies

```bash
pip install torch torchvision pandas opencv-python pillow pyperclip albumentations mediapipe
```

### 4. Download the Balanced AffectNet Dataset

Download the dataset from [Kaggle Balanced AffectNet](https://www.kaggle.com/datasets/dollyprajapati182/balanced-affectnet) and extract the folders directly into `data/`:

```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ Anger/
â”‚   â”œâ”€â”€ Contempt/
â”‚   â”œâ”€â”€ Disgust/
â”‚   â”œâ”€â”€ Fear/
â”‚   â”œâ”€â”€ Happy/
â”‚   â”œâ”€â”€ Neutral/
â”‚   â”œâ”€â”€ Sad/
â”‚   â””â”€â”€ Surprise/
â”œâ”€â”€ val/
â”‚   â””â”€â”€ ... (same structure)
â””â”€â”€ test/
    â””â”€â”€ ... (same structure)
```

## Usage

### Train the model

**Recommended (multi-dataset, optimized):** run the notebook `train_affectnet_notebook (2).ipynb` which:
- Downloads AffectNet + FER+ via `kagglehub`
- Merges them with the unified `CombinedEmotionDataset`
- Uses AMP + `torch.compile` for fast large-batch training
- Saves the best weights to `emotion_model_best.pth` and a deployable `emotion_model.pth`

**Legacy script (single-dataset):**
```bash
python train_affectnet.py
```
Trains on AffectNet only and produces `emotion_model.pth`.

### Run the application

You can run different versions of the application depending on your needs:

**Basic Version (Fast & Simple):**
```bash
python app.py
```

**Advanced Version (Recommended):**
Includes hand gesture recognition, facial feature analysis, and improved accuracy.
```bash
python app_v3.py
```

**Controls:**
- Press `q` to quit
- Press `s` to copy the current emoji to clipboard
- Press `e` to toggle emotion probability bars (v3)
- Press `f` to toggle facial feature analysis (v3)
- Press `h` to toggle hand tracking (v3)

## Model Architecture

The CNN architecture consists of:
- 4 Convolutional blocks with BatchNorm, MaxPooling, and progressive Dropout
- Global Average Pooling for flexibility
- 3 Fully connected layers
- Output: 8 emotion classes

```
Input (3, 75, 75) - RGB Image
    â†“
Conv2D(64) Ã— 2 â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout(0.1)
    â†“
Conv2D(128) Ã— 2 â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout(0.1)
    â†“
Conv2D(256) Ã— 2 â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout(0.15)
    â†“
Conv2D(512) Ã— 2 â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout(0.2)
    â†“
Global Average Pooling
    â†“
FC(512â†’256) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.4)
    â†“
FC(256â†’128) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.3)
    â†“
FC(128â†’8) â†’ Output
```

## Project Structure

```
â”œâ”€â”€ app.py                  # Basic real-time webcam application
â”œâ”€â”€ app_v3.py               # Advanced app with hand gestures & feature analysis
â”œâ”€â”€ train_affectnet.py      # Training script for AffectNet
â”œâ”€â”€ model.py                # CNN architecture definition
â”œâ”€â”€ dataset_affectnet.py    # Balanced AffectNet dataset loader
â”œâ”€â”€ data/                   # Dataset (not included, download from Kaggle)
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ emotion_model.pth       # Trained model (generated after training)
â”œâ”€â”€ report/
â”‚   â””â”€â”€ report.tex          # Technical report (LaTeX)
â””â”€â”€ README.md
```

## Training Features

- **Multi-dataset fusion**: AffectNet + FER+ (unified 8-class mapping); optional FER2013 fallback
- **SE-Block CNN**: Attention-enhanced conv blocks with global avg pooling
- **Advanced augmentation**: Albumentations (flip, affine, noise/blur, color jitter, coarse dropout) + balanced intensity
- **Mixup (on)**, CutMix (off by default), **Label Smoothing**; optional Focal Loss
- **Class balancing**: adaptive class weights; oversized batches with gradient clipping
- **Optimizers**: AdamW + OneCycleLR; **AMP** + **torch.compile (max-autotune)** for speed
- **Regularization**: dropout, weight decay, early stopping, optional SWA
- **Evaluation**: per-class metrics and optional TTA (flip + brightness variants)

## Requirements

- Python 3.11+
- PyTorch 2.0+
- OpenCV 4.0+
- Pillow
- albumentations
- mediapipe
- pyperclip

## License

MIT License

## Acknowledgments

- [Balanced AffectNet Dataset](https://www.kaggle.com/datasets/dollyprajapati182/balanced-affectnet)
- PyTorch team for the deep learning framework
- Albumentations team for the augmentation library
