{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce613dfb",
   "metadata": {},
   "source": [
    "## 1. üì¶ Imports et V√©rification GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# V√©rification GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import albumentations pour l'augmentation avanc√©e\n",
    "try:\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    HAS_ALBUMENTATIONS = True\n",
    "    print(\"‚úÖ Albumentations disponible pour l'augmentation avanc√©e\")\n",
    "except ImportError:\n",
    "    HAS_ALBUMENTATIONS = False\n",
    "    print(\"‚ö†Ô∏è Installez albumentations: pip install albumentations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9cb719",
   "metadata": {},
   "source": [
    "## 2. ‚öôÔ∏è Configuration des Hyperparam√®tres\n",
    "\n",
    "Tous les param√®tres d'entra√Ænement sont centralis√©s ici pour faciliter l'exp√©rimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158351ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # === DONN√âES ===\n",
    "    DATASET_ROOT = './data'\n",
    "    \n",
    "    # === MOD√àLE ===\n",
    "    NUM_CLASSES = 8       # 8 √©motions AffectNet\n",
    "    IN_CHANNELS = 3       # RGB\n",
    "    INPUT_SIZE = 75       # 75x75 pixels\n",
    "    \n",
    "    # === ENTRA√éNEMENT ===\n",
    "    BATCH_SIZE = 64       # Ajuster selon la VRAM disponible\n",
    "    ACCUMULATION_STEPS = 1\n",
    "    LEARNING_RATE = 0.0005\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    EPOCHS = 80\n",
    "    PATIENCE = 15         # Early stopping\n",
    "    \n",
    "    # === TECHNIQUES AVANC√âES ===\n",
    "    USE_MIXUP = True\n",
    "    MIXUP_ALPHA = 0.4\n",
    "    USE_CUTMIX = False\n",
    "    CUTMIX_ALPHA = 1.0\n",
    "    CUTMIX_PROB = 0.5\n",
    "    \n",
    "    USE_LABEL_SMOOTHING = True\n",
    "    LABEL_SMOOTHING = 0.1\n",
    "    \n",
    "    USE_FOCAL_LOSS = False\n",
    "    FOCAL_GAMMA = 2.0\n",
    "    \n",
    "    # === AUGMENTATION ===\n",
    "    USE_ADVANCED_AUG = True\n",
    "    \n",
    "    # === √âQUILIBRAGE DES CLASSES ===\n",
    "    USE_OVERSAMPLING = False\n",
    "    MAX_CLASS_WEIGHT = 3.0\n",
    "    \n",
    "    # === DEVICE ===\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(f\"üñ•Ô∏è Device: {config.DEVICE}\")\n",
    "print(f\"üìä Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"üìà Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"üîÑ Epochs: {config.EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd297a",
   "metadata": {},
   "source": [
    "## 3. üìâ Fonctions de Perte (Loss Functions)\n",
    "\n",
    "### Focal Loss\n",
    "Utile pour les datasets d√©s√©quilibr√©s - r√©duit l'importance des exemples faciles.\n",
    "\n",
    "### Label Smoothing Cross Entropy\n",
    "Emp√™che le mod√®le d'√™tre trop confiant sur les pr√©dictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb893d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss pour g√©rer le d√©s√©quilibre de classes.\"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean', label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        if self.label_smoothing > 0:\n",
    "            n_classes = inputs.size(-1)\n",
    "            targets_smooth = torch.zeros_like(inputs)\n",
    "            targets_smooth.fill_(self.label_smoothing / (n_classes - 1))\n",
    "            targets_smooth.scatter_(1, targets.unsqueeze(1), 1.0 - self.label_smoothing)\n",
    "            \n",
    "            log_probs = F.log_softmax(inputs, dim=-1)\n",
    "            ce_loss = -(targets_smooth * log_probs).sum(dim=-1)\n",
    "        else:\n",
    "            ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        probs = torch.softmax(inputs, dim=-1)\n",
    "        pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha.gather(0, targets)\n",
    "            focal_weight = focal_weight * alpha_t\n",
    "        \n",
    "        loss = focal_weight * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Cross Entropy avec label smoothing.\"\"\"\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        n_classes = inputs.size(-1)\n",
    "        log_probs = F.log_softmax(inputs, dim=-1)\n",
    "        \n",
    "        targets_smooth = torch.zeros_like(log_probs)\n",
    "        targets_smooth.fill_(self.smoothing / (n_classes - 1))\n",
    "        targets_smooth.scatter_(1, targets.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        \n",
    "        loss = -(targets_smooth * log_probs).sum(dim=-1)\n",
    "        return loss.mean()\n",
    "\n",
    "print(\"‚úÖ Fonctions de perte d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7661f",
   "metadata": {},
   "source": [
    "## 4. üîÄ Mixup & CutMix\n",
    "\n",
    "Techniques d'augmentation qui m√©langent des images pour am√©liorer la g√©n√©ralisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Mixup: m√©lange deux √©chantillons.\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"CutMix: coupe et colle des patches entre √©chantillons.\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    _, _, H, W = x.shape\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "    \n",
    "    return x, y, y[index], lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Calcule la loss mix√©e pour mixup/cutmix.\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "print(\"‚úÖ Fonctions Mixup et CutMix d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e9094",
   "metadata": {},
   "source": [
    "## 5. üñºÔ∏è Transformations et Augmentation de Donn√©es\n",
    "\n",
    "Utilise Albumentations pour des augmentations avanc√©es (rotation, bruit, flou, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    \"\"\"Transformations pour l'entra√Ænement (images RGB 75x75).\"\"\"\n",
    "    if HAS_ALBUMENTATIONS and config.USE_ADVANCED_AUG:\n",
    "        return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Affine(\n",
    "                translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)},\n",
    "                scale=(0.9, 1.1),\n",
    "                rotate=(-10, 10),\n",
    "                p=0.5\n",
    "            ),\n",
    "            A.OneOf([\n",
    "                A.GaussNoise(std_range=(0.02, 0.1), p=1),\n",
    "                A.GaussianBlur(blur_limit=(3, 5), p=1),\n",
    "                A.MotionBlur(blur_limit=3, p=1),\n",
    "            ], p=0.3),\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1),\n",
    "                A.RandomGamma(gamma_limit=(80, 120), p=1),\n",
    "                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=20, p=1),\n",
    "            ], p=0.5),\n",
    "            A.CoarseDropout(\n",
    "                num_holes_range=(1, 4),\n",
    "                hole_height_range=(6, 12),\n",
    "                hole_width_range=(6, 12),\n",
    "                fill=0,\n",
    "                p=0.3\n",
    "            ),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        # Fallback vers torchvision\n",
    "        return transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.9, 1.1)),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "\n",
    "def get_val_transforms():\n",
    "    \"\"\"Transformations pour la validation (juste normalisation).\"\"\"\n",
    "    if HAS_ALBUMENTATIONS:\n",
    "        return A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "print(\"‚úÖ Transformations d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2d4b7",
   "metadata": {},
   "source": [
    "## 6. üìÅ Dataset AffectNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "\n",
    "class BalancedAffectNetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset pour Balanced AffectNet.\n",
    "    \n",
    "    Structure attendue:\n",
    "    data/\n",
    "        train/Anger/, Contempt/, Disgust/, Fear/, Happy/, Neutral/, Sad/, Surprise/\n",
    "        val/...\n",
    "        test/...\n",
    "    \"\"\"\n",
    "    \n",
    "    NUM_CLASSES = 8\n",
    "    \n",
    "    EMOTION_CLASSES = {\n",
    "        'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3,\n",
    "        'Sad': 4, 'Surprise': 5, 'Neutral': 6, 'Contempt': 7,\n",
    "    }\n",
    "    \n",
    "    IDX_TO_EMOTION = {v: k for k, v in EMOTION_CLASSES.items()}\n",
    "    \n",
    "    def __init__(self, root_dir='./data', split='train', transform=None, use_albumentations=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.use_albumentations = use_albumentations\n",
    "        \n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        split_dir = os.path.join(root_dir, split)\n",
    "        \n",
    "        if not os.path.exists(split_dir):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Dataset non trouv√©: {split_dir}\\n\"\n",
    "                f\"T√©l√©chargez depuis: https://www.kaggle.com/datasets/dollyprajapati182/balanced-affectnet\"\n",
    "            )\n",
    "        \n",
    "        # Charger toutes les images\n",
    "        for emotion_name, emotion_idx in self.EMOTION_CLASSES.items():\n",
    "            emotion_dir = os.path.join(split_dir, emotion_name)\n",
    "            if not os.path.exists(emotion_dir):\n",
    "                print(f\"‚ö†Ô∏è {emotion_dir} non trouv√©, ignor√©...\")\n",
    "                continue\n",
    "            \n",
    "            for img_name in os.listdir(emotion_dir):\n",
    "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                    self.images.append(os.path.join(emotion_dir, img_name))\n",
    "                    self.labels.append(emotion_idx)\n",
    "        \n",
    "        print(f\"üìÇ Charg√© {len(self.images)} images depuis AffectNet {split}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            if self.use_albumentations:\n",
    "                augmented = self.transform(image=image)\n",
    "                image = augmented['image']\n",
    "            else:\n",
    "                image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        return np.bincount(self.labels, minlength=self.NUM_CLASSES)\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return np.array(self.labels)\n",
    "\n",
    "\n",
    "def get_class_weights(dataset, max_weight=5.0):\n",
    "    \"\"\"Calcule les poids pour √©quilibrer les classes.\"\"\"\n",
    "    counts = dataset.get_class_distribution()\n",
    "    counts = np.maximum(counts, 1)\n",
    "    \n",
    "    weights = 1.0 / counts\n",
    "    weights = weights / weights.sum() * len(weights)\n",
    "    weights = np.clip(weights, 0.3, max_weight)\n",
    "    weights = weights / weights.sum() * len(weights)\n",
    "    \n",
    "    print(\"\\nüìä Poids des classes:\")\n",
    "    for i, (count, weight) in enumerate(zip(counts, weights)):\n",
    "        emotion = BalancedAffectNetDataset.IDX_TO_EMOTION.get(i, f\"Class_{i}\")\n",
    "        print(f\"    {emotion:10s}: {count:5d} samples, poids: {weight:.3f}\")\n",
    "    \n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "\n",
    "def get_balanced_sampler(dataset):\n",
    "    \"\"\"Cr√©e un sampler √©quilibr√© pour l'entra√Ænement.\"\"\"\n",
    "    labels = dataset.get_labels()\n",
    "    counts = np.bincount(labels, minlength=BalancedAffectNetDataset.NUM_CLASSES)\n",
    "    counts = np.maximum(counts, 1)\n",
    "    \n",
    "    weights = 1.0 / counts\n",
    "    sample_weights = weights[labels]\n",
    "    \n",
    "    return WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Classes Dataset d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea561e",
   "metadata": {},
   "source": [
    "## 7. üß† Architecture du Mod√®le CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du mod√®le depuis le fichier existant\n",
    "from model import FaceEmotionCNN, create_model\n",
    "\n",
    "# Cr√©er et afficher le mod√®le\n",
    "model = create_model(dataset='affectnet', num_classes=config.NUM_CLASSES)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"üß† Mod√®le cr√©√© avec {total_params:,} param√®tres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a29893",
   "metadata": {},
   "source": [
    "## 8. üîß Utilitaires d'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Suit les valeurs moyennes.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, per_class=False):\n",
    "    \"\"\"Validation avec m√©triques optionnelles par classe.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    loss_meter = AverageMeter()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    if per_class:\n",
    "        class_correct = defaultdict(int)\n",
    "        class_total = defaultdict(int)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss_meter.update(loss.item(), inputs.size(0))\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            if per_class:\n",
    "                for pred, label in zip(predicted, labels):\n",
    "                    class_total[label.item()] += 1\n",
    "                    if pred == label:\n",
    "                        class_correct[label.item()] += 1\n",
    "    \n",
    "    accuracy = 100.0 * correct / total\n",
    "    \n",
    "    if per_class:\n",
    "        emotions = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral', 'Contempt']\n",
    "        print(\"\\n  üìä Pr√©cision par classe:\")\n",
    "        for i, emo in enumerate(emotions):\n",
    "            if class_total[i] > 0:\n",
    "                acc = 100.0 * class_correct[i] / class_total[i]\n",
    "                print(f\"    {emo:10s}: {acc:5.1f}% ({class_correct[i]}/{class_total[i]})\")\n",
    "    \n",
    "    return loss_meter.avg, accuracy\n",
    "\n",
    "print(\"‚úÖ Utilitaires d√©finis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36818018",
   "metadata": {},
   "source": [
    "## 9. üìÇ Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÇ Chargement du dataset Balanced AffectNet...\\n\")\n",
    "\n",
    "train_transform = get_train_transforms()\n",
    "val_transform = get_val_transforms()\n",
    "\n",
    "train_dataset = BalancedAffectNetDataset(\n",
    "    root_dir=config.DATASET_ROOT,\n",
    "    split='train',\n",
    "    transform=train_transform,\n",
    "    use_albumentations=HAS_ALBUMENTATIONS\n",
    ")\n",
    "\n",
    "val_dataset = BalancedAffectNetDataset(\n",
    "    root_dir=config.DATASET_ROOT,\n",
    "    split='val',\n",
    "    transform=val_transform,\n",
    "    use_albumentations=HAS_ALBUMENTATIONS\n",
    ")\n",
    "\n",
    "# Poids des classes\n",
    "class_weights = get_class_weights(train_dataset, max_weight=config.MAX_CLASS_WEIGHT).to(config.DEVICE)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Train: {len(train_dataset)} samples, Val: {len(val_dataset)} samples\")\n",
    "print(f\"   Batches - Train: {len(train_loader)}, Val: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed802a6",
   "metadata": {},
   "source": [
    "## 10. üëÄ Visualisation d'√âchantillons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2cc43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser quelques images du dataset\n",
    "def show_samples(dataset, n_samples=8):\n",
    "    \"\"\"Affiche des √©chantillons du dataset.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # D√©normalisation\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    indices = random.sample(range(len(dataset)), n_samples)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        img, label = dataset[idx]\n",
    "        \n",
    "        # Convertir tensor en numpy et d√©normaliser\n",
    "        img_np = img.numpy().transpose(1, 2, 0)\n",
    "        img_np = img_np * std + mean\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "        \n",
    "        emotion = BalancedAffectNetDataset.IDX_TO_EMOTION[label]\n",
    "        \n",
    "        axes[i].imshow(img_np)\n",
    "        axes[i].set_title(emotion, fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('√âchantillons du Dataset AffectNet', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_samples(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f6b5b",
   "metadata": {},
   "source": [
    "## 11. üöÄ Configuration de l'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc21e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod√®le\n",
    "model = create_model(dataset='affectnet', num_classes=config.NUM_CLASSES).to(config.DEVICE)\n",
    "\n",
    "# Fonction de perte\n",
    "if config.USE_FOCAL_LOSS:\n",
    "    criterion = FocalLoss(\n",
    "        gamma=config.FOCAL_GAMMA,\n",
    "        alpha=class_weights,\n",
    "        label_smoothing=config.LABEL_SMOOTHING if config.USE_LABEL_SMOOTHING else 0.0\n",
    "    )\n",
    "    print(f\"‚úì Focal Loss (gamma={config.FOCAL_GAMMA})\")\n",
    "elif config.USE_LABEL_SMOOTHING:\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=config.LABEL_SMOOTHING)\n",
    "    print(f\"‚úì Label Smoothing (smoothing={config.LABEL_SMOOTHING})\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "val_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimiseur\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Scheduler OneCycleLR\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=config.LEARNING_RATE * 10,\n",
    "    epochs=config.EPOCHS,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìã Configuration d'entra√Ænement:\")\n",
    "print(f\"  Dataset: Balanced AffectNet (75x75 RGB, 8 classes)\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {config.EPOCHS}, Patience: {config.PATIENCE}\")\n",
    "print(f\"  Mixup: {config.USE_MIXUP} (alpha={config.MIXUP_ALPHA})\")\n",
    "print(f\"  Label Smoothing: {config.USE_LABEL_SMOOTHING} ({config.LABEL_SMOOTHING})\")\n",
    "print(f\"  Advanced Aug: {HAS_ALBUMENTATIONS and config.USE_ADVANCED_AUG}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedaa5fa",
   "metadata": {},
   "source": [
    "## 12. üèãÔ∏è Boucle d'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2378eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables de suivi\n",
    "best_val_acc = 0.0\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_epoch = 0\n",
    "\n",
    "# Historique pour les graphiques\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\nüöÄ D√©marrage de l'entra√Ænement...\\n\")\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    loss_meter = AverageMeter()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(config.DEVICE), labels.to(config.DEVICE)\n",
    "        \n",
    "        # Mixup ou CutMix al√©atoire\n",
    "        use_mixup = config.USE_MIXUP and random.random() > 0.5\n",
    "        use_cutmix = config.USE_CUTMIX and random.random() < config.CUTMIX_PROB and not use_mixup\n",
    "        \n",
    "        if use_mixup:\n",
    "            inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, config.MIXUP_ALPHA)\n",
    "        elif use_cutmix:\n",
    "            inputs, labels_a, labels_b, lam = cutmix_data(inputs, labels, config.CUTMIX_ALPHA)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Loss\n",
    "        if use_mixup or use_cutmix:\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "        else:\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss = loss / config.ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (batch_idx + 1) % config.ACCUMULATION_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # M√©triques\n",
    "        loss_meter.update(loss.item() * config.ACCUMULATION_STEPS, inputs.size(0))\n",
    "        \n",
    "        if not (use_mixup or use_cutmix):\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    train_acc = 100.0 * correct / max(total, 1)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc = validate(model, val_loader, val_criterion, config.DEVICE, \n",
    "                                 per_class=(epoch % 10 == 0))\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Sauvegarder historique\n",
    "    history['train_loss'].append(loss_meter.avg)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:3d}/{config.EPOCHS} | \"\n",
    "          f\"Train Loss: {loss_meter.avg:.4f} | Train Acc: {train_acc:.1f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.1f}% | \"\n",
    "          f\"LR: {current_lr:.6f} | Time: {elapsed/60:.1f}min\")\n",
    "    \n",
    "    # Sauvegarder le meilleur mod√®le\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        patience_counter = 0\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'config': {\n",
    "                'num_classes': config.NUM_CLASSES,\n",
    "                'in_channels': config.IN_CHANNELS,\n",
    "                'input_size': config.INPUT_SIZE,\n",
    "                'dataset': 'affectnet',\n",
    "            }\n",
    "        }, 'emotion_model_best.pth')\n",
    "        print(f\"  ‚úÖ Nouveau meilleur mod√®le! (Val Acc: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= config.PATIENCE:\n",
    "            print(f\"\\n‚èπÔ∏è Early stopping apr√®s {epoch+1} √©poques!\")\n",
    "            break\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéâ Entra√Ænement termin√©!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Temps total: {elapsed/60:.1f} minutes\")\n",
    "print(f\"Meilleure √©poque: {best_epoch}\")\n",
    "print(f\"Meilleure pr√©cision validation: {best_val_acc:.2f}%\")\n",
    "print(f\"Meilleure loss validation: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f42c0b",
   "metadata": {},
   "source": [
    "## 13. üìà Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e82ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphiques d'entra√Ænement\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train', color='blue')\n",
    "axes[0].plot(history['val_loss'], label='Validation', color='orange')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('üìâ Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train', color='blue')\n",
    "axes[1].plot(history['val_acc'], label='Validation', color='orange')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('üìä Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "axes[2].plot(history['lr'], color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('üìà Learning Rate (OneCycleLR)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Graphiques sauvegard√©s dans 'training_curves.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e32d2",
   "metadata": {},
   "source": [
    "## 14. üîç √âvaluation Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca470346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur mod√®le\n",
    "print(\"üì• Chargement du meilleur mod√®le...\")\n",
    "checkpoint = torch.load('emotion_model_best.pth', weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"\\nüìä √âvaluation finale sur le set de validation:\")\n",
    "val_loss, val_acc = validate(model, val_loader, val_criterion, config.DEVICE, per_class=True)\n",
    "\n",
    "print(f\"\\nüéØ R√©sultats finaux:\")\n",
    "print(f\"   - Pr√©cision globale: {val_acc:.2f}%\")\n",
    "print(f\"   - Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126d542b",
   "metadata": {},
   "source": [
    "## 15. üíæ Sauvegarde du Mod√®le Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le final (poids uniquement)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'num_classes': config.NUM_CLASSES,\n",
    "    'in_channels': config.IN_CHANNELS,\n",
    "    'input_size': config.INPUT_SIZE,\n",
    "    'dataset': 'affectnet',\n",
    "    'best_val_acc': best_val_acc,\n",
    "}, 'emotion_model.pth')\n",
    "\n",
    "print(\"‚úÖ Mod√®le sauvegard√© dans 'emotion_model.pth'\")\n",
    "print(f\"   Taille: {os.path.getsize('emotion_model.pth') / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f580e241",
   "metadata": {},
   "source": [
    "## 16. üß™ Test sur Quelques Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c907058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(model, image_tensor, device):\n",
    "    \"\"\"Pr√©dit l'√©motion pour une image.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "        outputs = model(image_tensor)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        pred_idx = outputs.argmax(1).item()\n",
    "        confidence = probs[0, pred_idx].item()\n",
    "    return pred_idx, confidence, probs[0].cpu().numpy()\n",
    "\n",
    "# Test sur quelques images de validation\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "emotions = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral', 'Contempt']\n",
    "\n",
    "indices = random.sample(range(len(val_dataset)), 8)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    img, true_label = val_dataset[idx]\n",
    "    pred_idx, confidence, probs = predict_emotion(model, img, config.DEVICE)\n",
    "    \n",
    "    img_np = img.numpy().transpose(1, 2, 0)\n",
    "    img_np = img_np * std + mean\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "    \n",
    "    true_emotion = emotions[true_label]\n",
    "    pred_emotion = emotions[pred_idx]\n",
    "    \n",
    "    color = 'green' if pred_idx == true_label else 'red'\n",
    "    \n",
    "    axes[i].imshow(img_np)\n",
    "    axes[i].set_title(f\"Vrai: {true_emotion}\\nPr√©d: {pred_emotion} ({confidence*100:.1f}%)\", \n",
    "                      color=color, fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('üîç Pr√©dictions sur le Set de Validation', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
