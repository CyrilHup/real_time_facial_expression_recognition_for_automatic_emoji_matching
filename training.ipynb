{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce613dfb",
   "metadata": {
    "id": "ce613dfb"
   },
   "source": [
    "## 1. ğŸ“¦ Imports and GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b7e4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae9b7e4b",
    "outputId": "7346ae7b-f315-45ac-e7f4-5c630a29f446"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GPU Verification\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58df31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b58df31",
    "outputId": "c97e2786-7f16-4a35-d983-3c8a190e3ced"
   },
   "outputs": [],
   "source": [
    "# Import albumentations for advanced augmentation\n",
    "try:\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    HAS_ALBUMENTATIONS = True\n",
    "    print(\"âœ… Albumentations available for advanced augmentation\")\n",
    "except ImportError:\n",
    "    HAS_ALBUMENTATIONS = False\n",
    "    print(\"âš ï¸ Install albumentations: pip install albumentations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9cb719",
   "metadata": {
    "id": "fc9cb719"
   },
   "source": [
    "## 2. âš™ï¸ Hyperparameter Configuration\n",
    "\n",
    "All training parameters are centralized here to facilitate experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158351ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "158351ef",
    "outputId": "f3d0b830-7cdf-4afb-91c7-9df2065adae3"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"MAXIMUM Configuration - Best possible result (epoch 1 slow, rest fast)\"\"\"\n",
    "\n",
    "    # === DATA ===\n",
    "    DATASET_ROOT = './kaggle/input/balanced-affectnet'\n",
    "\n",
    "    # === MODEL ===\n",
    "    NUM_CLASSES = 8\n",
    "    IN_CHANNELS = 3\n",
    "    INPUT_SIZE = 75\n",
    "    USE_SE_BLOCKS = True\n",
    "\n",
    "    # === TRAINING ===\n",
    "    BATCH_SIZE = 1536     # âš¡ Good GPU compromise (uses ~10-11GB)\n",
    "    ACCUMULATION_STEPS = 1\n",
    "    LEARNING_RATE = 0.0015  # Adjusted for batch 1536\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    EPOCHS = 100\n",
    "    PATIENCE = 20\n",
    "\n",
    "    # === ADVANCED TECHNIQUES ===\n",
    "    USE_MIXUP = True\n",
    "    MIXUP_ALPHA = 0.2\n",
    "    USE_CUTMIX = False\n",
    "    CUTMIX_ALPHA = 1.0\n",
    "    CUTMIX_PROB = 0.0\n",
    "\n",
    "    USE_LABEL_SMOOTHING = True\n",
    "    LABEL_SMOOTHING = 0.1\n",
    "\n",
    "    USE_FOCAL_LOSS = False\n",
    "    FOCAL_GAMMA = 2.0\n",
    "\n",
    "    # === AUGMENTATION ===\n",
    "    USE_ADVANCED_AUG = True\n",
    "    USE_CLAHE = False\n",
    "    USE_GRID_DISTORTION = False\n",
    "\n",
    "    # === CLASS BALANCING ===\n",
    "    USE_OVERSAMPLING = False\n",
    "    MAX_CLASS_WEIGHT = 3.0\n",
    "\n",
    "    # === MAXIMUM GPU OPTIMIZATION ===\n",
    "    USE_AMP = True                    # âœ… Mixed Precision\n",
    "    USE_COMPILE = True                # âœ… torch.compile\n",
    "    COMPILE_MODE = 'max-autotune'     # âš¡ MAXIMUM: epoch 1 slow (~2-3min) but rest very fast\n",
    "    NUM_WORKERS = 2                   # Optimal\n",
    "    PREFETCH_FACTOR = 4\n",
    "    PERSISTENT_WORKERS = True\n",
    "\n",
    "    # === SWA ===\n",
    "    USE_SWA = False\n",
    "    SWA_START_EPOCH = 75\n",
    "    SWA_LR = 0.0001\n",
    "\n",
    "    # === DEVICE ===\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # === SAVING ===\n",
    "    SAVE_PATH = 'emotion_model_best.pth'\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# âš¡ MAXIMUM CUDA Optimizations\n",
    "if torch.cuda.is_available():\n",
    "    # Performance\n",
    "    torch.backends.cudnn.benchmark = True          # Auto-tune kernels\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True   # TensorFloat-32 (2x faster)\n",
    "    torch.backends.cudnn.allow_tf32 = True         # TF32 for cuDNN\n",
    "    torch.backends.cudnn.deterministic = False     # Non-deterministic = faster\n",
    "    torch.set_float32_matmul_precision('high')     # Optimized Tensor Cores\n",
    "\n",
    "    # âš¡ NEW: Memory optimizations for larger batches\n",
    "    torch.cuda.set_per_process_memory_fraction(0.95)  # Uses 95% of VRAM\n",
    "\n",
    "    gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total VRAM: {gpu_mem_total:.1f} GB\")\n",
    "    print(f\"âš¡ Mode: max-autotune (epoch 1 slow, rest very fast)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“‹ MAXIMUM CONFIGURATION (Quality + Speed)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "print(f\"âš¡ Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"âš¡ Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"âš¡ torch.compile: {config.COMPILE_MODE}\")\n",
    "print(f\"âš¡ Mixed Precision: {config.USE_AMP}\")\n",
    "print(f\"âš¡ TF32: Enabled\")\n",
    "print(f\"âš ï¸ Epoch 1: ~2-3 min (compilation)\")\n",
    "print(f\"âœ… Epochs 2+: ~20-25s (very fast)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd297a",
   "metadata": {
    "id": "c2cd297a"
   },
   "source": [
    "## 3. ğŸ“‰ Loss Functions\n",
    "\n",
    "### Focal Loss\n",
    "Useful for imbalanced datasets - reduces the importance of easy examples.\n",
    "\n",
    "### Label Smoothing Cross Entropy\n",
    "Prevents the model from being too confident about predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb893d83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb893d83",
    "outputId": "68c1ada2-bf83-45fb-e35b-31c1b0329b61"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss to handle class imbalance.\"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean', label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.label_smoothing > 0:\n",
    "            n_classes = inputs.size(-1)\n",
    "            targets_smooth = torch.zeros_like(inputs)\n",
    "            targets_smooth.fill_(self.label_smoothing / (n_classes - 1))\n",
    "            targets_smooth.scatter_(1, targets.unsqueeze(1), 1.0 - self.label_smoothing)\n",
    "\n",
    "            log_probs = F.log_softmax(inputs, dim=-1)\n",
    "            ce_loss = -(targets_smooth * log_probs).sum(dim=-1)\n",
    "        else:\n",
    "            ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "\n",
    "        probs = torch.softmax(inputs, dim=-1)\n",
    "        pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha.gather(0, targets)\n",
    "            focal_weight = focal_weight * alpha_t\n",
    "\n",
    "        loss = focal_weight * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Cross Entropy with label smoothing.\"\"\"\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        n_classes = inputs.size(-1)\n",
    "        log_probs = F.log_softmax(inputs, dim=-1)\n",
    "\n",
    "        targets_smooth = torch.zeros_like(log_probs)\n",
    "        targets_smooth.fill_(self.smoothing / (n_classes - 1))\n",
    "        targets_smooth.scatter_(1, targets.unsqueeze(1), 1.0 - self.smoothing)\n",
    "\n",
    "        loss = -(targets_smooth * log_probs).sum(dim=-1)\n",
    "        return loss.mean()\n",
    "\n",
    "print(\"âœ… Loss functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f7661f",
   "metadata": {
    "id": "88f7661f"
   },
   "source": [
    "## 4. ğŸ”€ Mixup & CutMix\n",
    "\n",
    "Augmentation techniques that mix images to improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6cd7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3c6cd7f",
    "outputId": "d4dcff92-c947-4c0a-c83f-94dd022a09bf"
   },
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Mixup: mixes two samples.\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"CutMix: cuts and pastes patches between samples.\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    _, _, H, W = x.shape\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "\n",
    "    return x, y, y[index], lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Calculates the mixed loss for mixup/cutmix.\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "print(\"âœ… Mixup and CutMix functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e9094",
   "metadata": {
    "id": "5c3e9094"
   },
   "source": [
    "## 5. ğŸ–¼ï¸ Transformations and Data Augmentation\n",
    "\n",
    "Uses Albumentations for advanced augmentations (rotation, noise, blur, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4f889",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cf4f889",
    "outputId": "f7136aad-d192-437e-ef0d-89f17efe9269"
   },
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    \"\"\"Transformations for training - BALANCED VERSION (neither too much nor too little).\"\"\"\n",
    "    if HAS_ALBUMENTATIONS and config.USE_ADVANCED_AUG:\n",
    "        return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Affine(\n",
    "                translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)},\n",
    "                scale=(0.95, 1.05),   # Moderate\n",
    "                rotate=(-10, 10),     # Moderate (not 15 which is too much)\n",
    "                p=0.4\n",
    "            ),\n",
    "            # NO CLAHE or GridDistortion (too aggressive on 75x75)\n",
    "            A.OneOf([\n",
    "                A.GaussNoise(std_range=(0.02, 0.08), p=1),\n",
    "                A.GaussianBlur(blur_limit=(3, 5), p=1),\n",
    "            ], p=0.2),\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=1),\n",
    "                A.RandomGamma(gamma_limit=(85, 115), p=1),\n",
    "                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=15, p=1),\n",
    "            ], p=0.4),\n",
    "            A.CoarseDropout(\n",
    "                num_holes_range=(1, 2),\n",
    "                hole_height_range=(4, 8),\n",
    "                hole_width_range=(4, 8),\n",
    "                fill=0,\n",
    "                p=0.2\n",
    "            ),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        # Fallback to torchvision\n",
    "        return transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "            transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "\n",
    "def get_val_transforms():\n",
    "    \"\"\"Transformations for validation (just normalization).\"\"\"\n",
    "    if HAS_ALBUMENTATIONS:\n",
    "        return A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "print(\"âœ… Transformations defined (balanced version)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2d4b7",
   "metadata": {
    "id": "d7d2d4b7"
   },
   "source": [
    "## 6. ğŸ“ AffectNet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a3354",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c7a3354",
    "outputId": "eb9ff69b-48e2-4056-fc90-5fbdd04c3b78"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "\n",
    "class BalancedAffectNetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Balanced AffectNet.\n",
    "\n",
    "    Expected structure:\n",
    "    data/\n",
    "        train/Anger/, Contempt/, Disgust/, Fear/, Happy/, Neutral/, Sad/, Surprise/\n",
    "        val/...\n",
    "        test/...\n",
    "    \"\"\"\n",
    "\n",
    "    NUM_CLASSES = 8\n",
    "\n",
    "    EMOTION_CLASSES = {\n",
    "        'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3,\n",
    "        'Sad': 4, 'Surprise': 5, 'Neutral': 6, 'Contempt': 7,\n",
    "    }\n",
    "\n",
    "    IDX_TO_EMOTION = {v: k for k, v in EMOTION_CLASSES.items()}\n",
    "\n",
    "    def __init__(self, root_dir='./kaggle/input/balanced-affectnet', split='train', transform=None, use_albumentations=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.use_albumentations = use_albumentations\n",
    "\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        split_dir = os.path.join(root_dir, split)\n",
    "\n",
    "        if not os.path.exists(split_dir):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Dataset not found: {split_dir}\\n\"\n",
    "                f\"Download from: https://www.kaggle.com/datasets/dollyprajapati182/balanced-affectnet\"\n",
    "            )\n",
    "\n",
    "        # Load all images\n",
    "        for emotion_name, emotion_idx in self.EMOTION_CLASSES.items():\n",
    "            emotion_dir = os.path.join(split_dir, emotion_name)\n",
    "            if not os.path.exists(emotion_dir):\n",
    "                print(f\"âš ï¸ {emotion_dir} not found, ignored...\")\n",
    "                continue\n",
    "\n",
    "            for img_name in os.listdir(emotion_dir):\n",
    "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                    self.images.append(os.path.join(emotion_dir, img_name))\n",
    "                    self.labels.append(emotion_idx)\n",
    "\n",
    "        print(f\"ğŸ“‚ Loaded {len(self.images)} images from AffectNet {split}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "\n",
    "        if self.transform:\n",
    "            if self.use_albumentations:\n",
    "                augmented = self.transform(image=image)\n",
    "                image = augmented['image']\n",
    "            else:\n",
    "                image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def get_class_distribution(self):\n",
    "        return np.bincount(self.labels, minlength=self.NUM_CLASSES)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return np.array(self.labels)\n",
    "\n",
    "\n",
    "def get_class_weights(dataset, max_weight=5.0):\n",
    "    \"\"\"Calculates weights to balance classes.\"\"\"\n",
    "    counts = dataset.get_class_distribution()\n",
    "    counts = np.maximum(counts, 1)\n",
    "\n",
    "    weights = 1.0 / counts\n",
    "    weights = weights / weights.sum() * len(weights)\n",
    "    weights = np.clip(weights, 0.3, max_weight)\n",
    "    weights = weights / weights.sum() * len(weights)\n",
    "\n",
    "    print(\"\\nğŸ“Š Class weights:\")\n",
    "    for i, (count, weight) in enumerate(zip(counts, weights)):\n",
    "        emotion = BalancedAffectNetDataset.IDX_TO_EMOTION.get(i, f\"Class_{i}\")\n",
    "        print(f\"    {emotion:10s}: {count:5d} samples, weight: {weight:.3f}\")\n",
    "\n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "\n",
    "def get_balanced_sampler(dataset):\n",
    "    \"\"\"Creates a balanced sampler for training.\"\"\"\n",
    "    labels = dataset.get_labels()\n",
    "    counts = np.bincount(labels, minlength=BalancedAffectNetDataset.NUM_CLASSES)\n",
    "    counts = np.maximum(counts, 1)\n",
    "\n",
    "    weights = 1.0 / counts\n",
    "    sample_weights = weights[labels]\n",
    "\n",
    "    return WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "print(\"âœ… Dataset Classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a8b07",
   "metadata": {
    "id": "af2a8b07"
   },
   "source": [
    "## 6bis. ğŸ“ FER2013 / FER2013+ Dataset\n",
    "\n",
    "FER2013 is a classic emotion recognition dataset with ~35k images in 48x48 grayscale.\n",
    "FER2013+ is a version with labels corrected and improved by Microsoft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc33f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65bc33f5",
    "outputId": "4f204b50-8525-42dc-f658-6e58537d3af1"
   },
   "outputs": [],
   "source": [
    "class FER2013Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    FER2013 Dataset with grayscale -> RGB conversion.\n",
    "\n",
    "    Kaggle Structure (msambare/fer2013):\n",
    "        train/angry/, disgust/, fear/, happy/, neutral/, sad/, surprise/\n",
    "        test/...\n",
    "\n",
    "    Images are 48x48 grayscale, automatically:\n",
    "    - Resized to target_size (75x75 by default)\n",
    "    - Converted to RGB (for model compatibility)\n",
    "    \"\"\"\n",
    "\n",
    "    NUM_CLASSES = 7  # No Contempt in FER2013\n",
    "\n",
    "    EMOTION_CLASSES = {\n",
    "        'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3,\n",
    "        'sad': 4, 'surprise': 5, 'neutral': 6\n",
    "    }\n",
    "\n",
    "    IDX_TO_EMOTION = {v: k.capitalize() for k, v in EMOTION_CLASSES.items()}\n",
    "\n",
    "    def __init__(self, root_dir, split='train', transform=None,\n",
    "                 use_albumentations=False, target_size=75):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = 'train' if split == 'train' else 'test'  # FER2013 only has train/test\n",
    "        self.transform = transform\n",
    "        self.use_albumentations = use_albumentations\n",
    "        self.target_size = target_size\n",
    "\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        split_dir = os.path.join(root_dir, self.split)\n",
    "\n",
    "        if not os.path.exists(split_dir):\n",
    "            raise FileNotFoundError(f\"FER2013 not found: {split_dir}\")\n",
    "\n",
    "        for emotion_name, emotion_idx in self.EMOTION_CLASSES.items():\n",
    "            emotion_dir = os.path.join(split_dir, emotion_name)\n",
    "            if not os.path.exists(emotion_dir):\n",
    "                continue\n",
    "\n",
    "            for img_name in os.listdir(emotion_dir):\n",
    "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.images.append(os.path.join(emotion_dir, img_name))\n",
    "                    self.labels.append(emotion_idx)\n",
    "\n",
    "        print(f\"ğŸ“‚ FER2013 {self.split}: {len(self.images)} images loaded\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image (can be grayscale)\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        # Convert to RGB if grayscale\n",
    "        if image.mode == 'L':\n",
    "            image = image.convert('RGB')\n",
    "        else:\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        # Resize to target_size (48x48 -> 75x75)\n",
    "        if image.size != (self.target_size, self.target_size):\n",
    "            image = image.resize((self.target_size, self.target_size), Image.BILINEAR)\n",
    "\n",
    "        image = np.array(image)\n",
    "\n",
    "        if self.transform:\n",
    "            if self.use_albumentations:\n",
    "                augmented = self.transform(image=image)\n",
    "                image = augmented['image']\n",
    "            else:\n",
    "                image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def get_class_distribution(self):\n",
    "        return np.bincount(self.labels, minlength=self.NUM_CLASSES)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return np.array(self.labels)\n",
    "\n",
    "\n",
    "# ===============================================================================\n",
    "# ğŸ“¦ FER+ (FER2013 with labels corrected by Microsoft)\n",
    "# ===============================================================================\n",
    "\n",
    "def download_ferplus_labels(dest_folder):\n",
    "    \"\"\"\n",
    "    Downloads fer2013new.csv from Microsoft FERPlus repo.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the downloaded file\n",
    "    \"\"\"\n",
    "    import urllib.request\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/microsoft/FERPlus/master/fer2013new.csv\"\n",
    "    dest_path = os.path.join(dest_folder, \"fer2013new.csv\")\n",
    "\n",
    "    if os.path.exists(dest_path):\n",
    "        print(f\"  âœ“ fer2013new.csv already present\")\n",
    "        return dest_path\n",
    "\n",
    "    print(f\"  ğŸ“¥ Downloading fer2013new.csv from GitHub...\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, dest_path)\n",
    "        print(f\"  âœ“ Downloaded: {dest_path}\")\n",
    "        return dest_path\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_ferplus_images(fer2013_csv_path, ferplus_csv_path, output_folder):\n",
    "    \"\"\"\n",
    "    Generates PNG images from fer2013.csv with FER+ labels.\n",
    "\n",
    "    Output structure:\n",
    "        output_folder/\n",
    "            FER2013Train/\n",
    "                fer0000000.png\n",
    "                ...\n",
    "            FER2013Valid/\n",
    "            FER2013Test/\n",
    "\n",
    "    Returns:\n",
    "        bool: True if success\n",
    "    \"\"\"\n",
    "    import csv\n",
    "\n",
    "    # Create folders\n",
    "    for split in ['FER2013Train', 'FER2013Valid', 'FER2013Test']:\n",
    "        os.makedirs(os.path.join(output_folder, split), exist_ok=True)\n",
    "\n",
    "    # Mapping usage -> folder\n",
    "    usage_to_folder = {\n",
    "        'Training': 'FER2013Train',\n",
    "        'PublicTest': 'FER2013Valid',\n",
    "        'PrivateTest': 'FER2013Test'\n",
    "    }\n",
    "\n",
    "    # Read fer2013.csv and generate images\n",
    "    print(f\"  ğŸ–¼ï¸ Generating images from fer2013.csv...\")\n",
    "\n",
    "    with open(fer2013_csv_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)  # emotion,pixels,Usage\n",
    "\n",
    "        for idx, row in enumerate(reader):\n",
    "            if len(row) < 3:\n",
    "                continue\n",
    "\n",
    "            emotion = row[0]\n",
    "            pixels = row[1]\n",
    "            usage = row[2]\n",
    "\n",
    "            # Convert pixels to image\n",
    "            pixel_values = [int(p) for p in pixels.split()]\n",
    "            img_array = np.array(pixel_values, dtype=np.uint8).reshape(48, 48)\n",
    "            img = Image.fromarray(img_array, mode='L')\n",
    "\n",
    "            # Save\n",
    "            folder = usage_to_folder.get(usage, 'FER2013Train')\n",
    "            img_name = f\"fer{idx:08d}.png\"\n",
    "            img_path = os.path.join(output_folder, folder, img_name)\n",
    "            img.save(img_path)\n",
    "\n",
    "            if idx % 5000 == 0:\n",
    "                print(f\"    Progress: {idx} images...\")\n",
    "\n",
    "    print(f\"  âœ“ {idx + 1} images generated\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def setup_ferplus_dataset(fer2013_kaggle_path, output_folder=None):\n",
    "    \"\"\"\n",
    "    Configures the complete FER+ dataset:\n",
    "    1. Downloads fer2013new.csv from GitHub\n",
    "    2. Finds/downloads fer2013.csv from Kaggle\n",
    "    3. Generates PNG images\n",
    "\n",
    "    Args:\n",
    "        fer2013_kaggle_path: Path to FER2013 Kaggle dataset (msambare/fer2013)\n",
    "        output_folder: Output folder (optional)\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the ready-to-use FER+ dataset\n",
    "    \"\"\"\n",
    "    if output_folder is None:\n",
    "        output_folder = os.path.join(os.path.dirname(fer2013_kaggle_path), 'ferplus_generated')\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Check if already generated\n",
    "    train_folder = os.path.join(output_folder, 'FER2013Train')\n",
    "    if os.path.exists(train_folder) and len(os.listdir(train_folder)) > 1000:\n",
    "        print(f\"  âœ“ FER+ already generated in {output_folder}\")\n",
    "        # Download labels anyway if not present\n",
    "        download_ferplus_labels(output_folder)\n",
    "        return output_folder\n",
    "\n",
    "    print(\"\\nğŸ”§ Configuring FER+ (first use)...\")\n",
    "\n",
    "    # 1. Download FER+ labels from GitHub\n",
    "    ferplus_csv = download_ferplus_labels(output_folder)\n",
    "    if ferplus_csv is None:\n",
    "        return None\n",
    "\n",
    "    # 2. Find fer2013.csv\n",
    "    # The Kaggle dataset msambare/fer2013 is in folder format, not CSV\n",
    "    # We must use the original dataset: deadskull7/fer2013\n",
    "    fer2013_csv = os.path.join(fer2013_kaggle_path, 'fer2013.csv')\n",
    "\n",
    "    if not os.path.exists(fer2013_csv):\n",
    "        # Search in other possible locations\n",
    "        for alt_path in [\n",
    "            os.path.join(fer2013_kaggle_path, 'fer2013', 'fer2013.csv'),\n",
    "            os.path.join(fer2013_kaggle_path, 'data', 'fer2013.csv'),\n",
    "        ]:\n",
    "            if os.path.exists(alt_path):\n",
    "                fer2013_csv = alt_path\n",
    "                break\n",
    "\n",
    "    if not os.path.exists(fer2013_csv):\n",
    "        print(f\"  âš ï¸ fer2013.csv not found. FER+ requires the original CSV dataset.\")\n",
    "        print(f\"     The Kaggle dataset 'msambare/fer2013' is in image format.\")\n",
    "        print(f\"     For FER+, use 'deadskull7/fer2013' which contains the CSV.\")\n",
    "        return None\n",
    "\n",
    "    # 3. Generate images\n",
    "    success = generate_ferplus_images(fer2013_csv, ferplus_csv, output_folder)\n",
    "\n",
    "    if success:\n",
    "        return output_folder\n",
    "    return None\n",
    "\n",
    "\n",
    "class FERPlusDataset(Dataset):\n",
    "    \"\"\"\n",
    "    FER2013+ (FER+) Dataset with labels corrected by Microsoft.\n",
    "\n",
    "    FER+ improves FER2013 with:\n",
    "    - Labels voted by 10 annotators (more reliable)\n",
    "    - 8 classes (addition of Contempt)\n",
    "    - Possibility to use vote probabilities\n",
    "\n",
    "    The dataset is automatically configured from:\n",
    "    - fer2013new.csv (labels) from Microsoft GitHub\n",
    "    - fer2013.csv (images) from Kaggle\n",
    "    \"\"\"\n",
    "\n",
    "    NUM_CLASSES = 8\n",
    "\n",
    "    # FER+ CSV columns: usage, neutral, happiness, surprise, sadness, anger, disgust, fear, contempt, unknown, NF\n",
    "    FERPLUS_EMOTIONS = ['neutral', 'happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear', 'contempt']\n",
    "\n",
    "    # Mapping FER+ order -> Unified order (AffectNet)\n",
    "    # FER+: neutral(0), happiness(1), surprise(2), sadness(3), anger(4), disgust(5), fear(6), contempt(7)\n",
    "    # Unified: Anger(0), Disgust(1), Fear(2), Happy(3), Sad(4), Surprise(5), Neutral(6), Contempt(7)\n",
    "    FERPLUS_TO_UNIFIED = {\n",
    "        0: 6,  # neutral -> Neutral\n",
    "        1: 3,  # happiness -> Happy\n",
    "        2: 5,  # surprise -> Surprise\n",
    "        3: 4,  # sadness -> Sad\n",
    "        4: 0,  # anger -> Anger\n",
    "        5: 1,  # disgust -> Disgust\n",
    "        6: 2,  # fear -> Fear\n",
    "        7: 7,  # contempt -> Contempt\n",
    "    }\n",
    "\n",
    "    IDX_TO_EMOTION = {\n",
    "        0: 'Anger', 1: 'Disgust', 2: 'Fear', 3: 'Happy',\n",
    "        4: 'Sad', 5: 'Surprise', 6: 'Neutral', 7: 'Contempt'\n",
    "    }\n",
    "\n",
    "    def __init__(self, root_dir, split='train', transform=None,\n",
    "                 use_albumentations=False, target_size=75,\n",
    "                 label_mode='majority', min_votes=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Path to FER+ dataset (with FER2013Train/, etc.)\n",
    "            split: 'train', 'val' or 'test'\n",
    "            label_mode: 'majority' (most voted label) or 'probability' (distribution)\n",
    "            min_votes: Minimum number of votes to include an image\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.use_albumentations = use_albumentations\n",
    "        self.target_size = target_size\n",
    "        self.label_mode = label_mode\n",
    "        self.min_votes = min_votes\n",
    "\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.vote_distributions = []  # For probability mode\n",
    "\n",
    "        # Mapping split -> folder\n",
    "        split_to_folder = {\n",
    "            'train': 'FER2013Train',\n",
    "            'val': 'FER2013Valid',\n",
    "            'test': 'FER2013Test'\n",
    "        }\n",
    "\n",
    "        folder_name = split_to_folder.get(split, 'FER2013Train')\n",
    "        split_dir = os.path.join(root_dir, folder_name)\n",
    "\n",
    "        if not os.path.exists(split_dir):\n",
    "            print(f\"âš ï¸ FER+ {split} not found: {split_dir}\")\n",
    "            return\n",
    "\n",
    "        # Load labels from fer2013new.csv\n",
    "        ferplus_csv = os.path.join(root_dir, 'fer2013new.csv')\n",
    "        if not os.path.exists(ferplus_csv):\n",
    "            print(f\"âš ï¸ fer2013new.csv not found in {root_dir}\")\n",
    "            return\n",
    "\n",
    "        self._load_data(split_dir, ferplus_csv, split)\n",
    "\n",
    "        print(f\"ğŸ“‚ FER+ {split}: {len(self.images)} images loaded (mode: {label_mode})\")\n",
    "\n",
    "    def _load_data(self, split_dir, ferplus_csv, split):\n",
    "        \"\"\"Loads images and labels.\"\"\"\n",
    "        import csv\n",
    "\n",
    "        # Mapping usage in CSV\n",
    "        usage_mapping = {\n",
    "            'train': 'Training',\n",
    "            'val': 'PublicTest',\n",
    "            'test': 'PrivateTest'\n",
    "        }\n",
    "        target_usage = usage_mapping.get(split, 'Training')\n",
    "\n",
    "        with open(ferplus_csv, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            header = next(reader)  # Skip header\n",
    "\n",
    "            for idx, row in enumerate(reader):\n",
    "                if len(row) < 10:\n",
    "                    continue\n",
    "\n",
    "                usage = row[0]\n",
    "\n",
    "                # Filter by split\n",
    "                if usage != target_usage:\n",
    "                    continue\n",
    "\n",
    "                # Votes for each emotion (columns 1-8)\n",
    "                # Format: usage, neutral, happiness, surprise, sadness, anger, disgust, fear, contempt, unknown, NF\n",
    "                try:\n",
    "                    votes = [int(v) if v.strip().isdigit() else 0 for v in row[2:10]]\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                total_votes = sum(votes)\n",
    "\n",
    "                # Ignore if not enough valid votes or if it is \"unknown\" / \"NF\"\n",
    "                if total_votes < self.min_votes:\n",
    "                    continue\n",
    "\n",
    "                # Image path\n",
    "                img_name = f\"fer{idx:08d}.png\"\n",
    "                img_path = os.path.join(split_dir, img_name)\n",
    "\n",
    "                if not os.path.exists(img_path):\n",
    "                    continue\n",
    "\n",
    "                # Calculate label\n",
    "                ferplus_label = np.argmax(votes)\n",
    "                unified_label = self.FERPLUS_TO_UNIFIED[ferplus_label]\n",
    "\n",
    "                self.images.append(img_path)\n",
    "                self.labels.append(unified_label)\n",
    "\n",
    "                # Store distribution for probability mode\n",
    "                if self.label_mode == 'probability':\n",
    "                    vote_dist = np.array(votes, dtype=np.float32)\n",
    "                    vote_dist = vote_dist / vote_dist.sum()  # Normalize\n",
    "                    # Reorder according to unified order\n",
    "                    unified_dist = np.zeros(8, dtype=np.float32)\n",
    "                    for ferplus_idx, unified_idx in self.FERPLUS_TO_UNIFIED.items():\n",
    "                        unified_dist[unified_idx] = vote_dist[ferplus_idx]\n",
    "                    self.vote_distributions.append(unified_dist)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        # Convert to RGB\n",
    "        if image.mode == 'L':\n",
    "            image = image.convert('RGB')\n",
    "        else:\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        # Resize\n",
    "        if image.size != (self.target_size, self.target_size):\n",
    "            image = image.resize((self.target_size, self.target_size), Image.BILINEAR)\n",
    "\n",
    "        image = np.array(image)\n",
    "\n",
    "        if self.transform:\n",
    "            if self.use_albumentations:\n",
    "                augmented = self.transform(image=image)\n",
    "                image = augmented['image']\n",
    "            else:\n",
    "                image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "\n",
    "        # Return according to mode\n",
    "        if self.label_mode == 'probability' and len(self.vote_distributions) > idx:\n",
    "            return image, label, torch.tensor(self.vote_distributions[idx])\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def get_class_distribution(self):\n",
    "        return np.bincount(self.labels, minlength=self.NUM_CLASSES)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return np.array(self.labels)\n",
    "\n",
    "\n",
    "print(\"âœ… FER2013 and FER+ Datasets defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eda0ed",
   "metadata": {
    "id": "78eda0ed"
   },
   "source": [
    "## 6ter. ğŸ”€ Combined Multi-Source Dataset\n",
    "\n",
    "This dataset combines AffectNet, FER2013 and/or FER+ by unifying classes into 8 emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501edca3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "501edca3",
    "outputId": "a93c789b-2bd4-4f3d-90b9-b9e15f6f818c"
   },
   "outputs": [],
   "source": [
    "class CombinedEmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset combining multiple sources with unified class mapping.\n",
    "\n",
    "    Combines AffectNet, FER2013 and FER+ with:\n",
    "    - Automatic resizing to target_size\n",
    "    - Automatic grayscale -> RGB conversion\n",
    "    - Unified mapping to 8 classes (AffectNet order)\n",
    "\n",
    "    Unified classes:\n",
    "        0: Anger, 1: Disgust, 2: Fear, 3: Happy,\n",
    "        4: Sad, 5: Surprise, 6: Neutral, 7: Contempt\n",
    "    \"\"\"\n",
    "\n",
    "    # 8 unified classes (AffectNet order)\n",
    "    UNIFIED_CLASSES = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral', 'Contempt']\n",
    "    NUM_CLASSES = 8\n",
    "    IDX_TO_EMOTION = {i: c for i, c in enumerate(UNIFIED_CLASSES)}\n",
    "\n",
    "    def __init__(self, datasets_config, split='train', transform=None,\n",
    "                 use_albumentations=False, target_size=75):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            datasets_config: dict {dataset_name: root_path}\n",
    "                Example: {'affectnet': '/path/to/affectnet', 'fer2013': '/path/to/fer2013'}\n",
    "            split: 'train', 'val', or 'test'\n",
    "            target_size: uniform output size (75 by default)\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.use_albumentations = use_albumentations\n",
    "        self.target_size = target_size\n",
    "\n",
    "        self.images = []  # List of dicts: {'path': str, 'is_grayscale': bool}\n",
    "        self.labels = []\n",
    "        self.sources = []  # For tracking/debug\n",
    "\n",
    "        total_by_source = {}\n",
    "\n",
    "        for dataset_name, root_dir in datasets_config.items():\n",
    "            if root_dir is None or not os.path.exists(root_dir):\n",
    "                print(f\"âš ï¸ {dataset_name} ignored (not found): {root_dir}\")\n",
    "                continue\n",
    "\n",
    "            count_before = len(self.images)\n",
    "\n",
    "            if dataset_name == 'affectnet':\n",
    "                self._load_affectnet(root_dir, split)\n",
    "            elif dataset_name == 'fer2013':\n",
    "                self._load_fer2013(root_dir, split)\n",
    "            elif dataset_name == 'ferplus':\n",
    "                self._load_ferplus(root_dir, split)\n",
    "            else:\n",
    "                print(f\"âš ï¸ Unknown dataset: {dataset_name}\")\n",
    "                continue\n",
    "\n",
    "            total_by_source[dataset_name] = len(self.images) - count_before\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ğŸ“Š COMBINED DATASET ({split})\")\n",
    "        print(f\"{'='*50}\")\n",
    "        for src, count in total_by_source.items():\n",
    "            print(f\"  {src:15s}: {count:6d} images\")\n",
    "        print(f\"  {'TOTAL':15s}: {len(self.images):6d} images\")\n",
    "        print(f\"{'='*50}\")\n",
    "        self._print_class_distribution()\n",
    "\n",
    "    def _load_affectnet(self, root_dir, split):\n",
    "        \"\"\"Loads AffectNet images.\"\"\"\n",
    "        affectnet_mapping = {\n",
    "            'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3,\n",
    "            'Sad': 4, 'Surprise': 5, 'Neutral': 6, 'Contempt': 7\n",
    "        }\n",
    "\n",
    "        split_dir = os.path.join(root_dir, split)\n",
    "        if not os.path.exists(split_dir):\n",
    "            return\n",
    "\n",
    "        for emotion_name, unified_idx in affectnet_mapping.items():\n",
    "            emotion_dir = os.path.join(split_dir, emotion_name)\n",
    "            if not os.path.exists(emotion_dir):\n",
    "                continue\n",
    "\n",
    "            for img_name in os.listdir(emotion_dir):\n",
    "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                    self.images.append({\n",
    "                        'path': os.path.join(emotion_dir, img_name),\n",
    "                        'is_grayscale': False\n",
    "                    })\n",
    "                    self.labels.append(unified_idx)\n",
    "                    self.sources.append('affectnet')\n",
    "\n",
    "    def _load_fer2013(self, root_dir, split):\n",
    "        \"\"\"Loads FER2013 images.\"\"\"\n",
    "        # FER2013 only has train/test, no val\n",
    "        fer_split = 'train' if split == 'train' else 'test'\n",
    "        split_dir = os.path.join(root_dir, fer_split)\n",
    "\n",
    "        if not os.path.exists(split_dir):\n",
    "            return\n",
    "\n",
    "        # Mapping FER2013 (7 classes) -> unified (8 classes)\n",
    "        fer_to_unified = {\n",
    "            'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3,\n",
    "            'sad': 4, 'surprise': 5, 'neutral': 6\n",
    "        }\n",
    "\n",
    "        for emotion_name, unified_idx in fer_to_unified.items():\n",
    "            emotion_dir = os.path.join(split_dir, emotion_name)\n",
    "            if not os.path.exists(emotion_dir):\n",
    "                continue\n",
    "\n",
    "            for img_name in os.listdir(emotion_dir):\n",
    "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.images.append({\n",
    "                        'path': os.path.join(emotion_dir, img_name),\n",
    "                        'is_grayscale': True  # FER2013 is grayscale\n",
    "                    })\n",
    "                    self.labels.append(unified_idx)\n",
    "                    self.sources.append('fer2013')\n",
    "\n",
    "    def _load_ferplus(self, root_dir, split):\n",
    "        \"\"\"Loads FER+ images (with labels corrected by Microsoft).\"\"\"\n",
    "        import csv\n",
    "\n",
    "        # Mapping FER+ order -> Unified order\n",
    "        ferplus_to_unified = {\n",
    "            0: 6,  # neutral -> Neutral\n",
    "            1: 3,  # happiness -> Happy\n",
    "            2: 5,  # surprise -> Surprise\n",
    "            3: 4,  # sadness -> Sad\n",
    "            4: 0,  # anger -> Anger\n",
    "            5: 1,  # disgust -> Disgust\n",
    "            6: 2,  # fear -> Fear\n",
    "            7: 7,  # contempt -> Contempt\n",
    "        }\n",
    "\n",
    "        # Mapping split -> folder and usage\n",
    "        split_mapping = {\n",
    "            'train': ('FER2013Train', 'Training'),\n",
    "            'val': ('FER2013Valid', 'PublicTest'),\n",
    "            'test': ('FER2013Test', 'PrivateTest')\n",
    "        }\n",
    "\n",
    "        folder_name, target_usage = split_mapping.get(split, ('FER2013Train', 'Training'))\n",
    "        split_dir = os.path.join(root_dir, folder_name)\n",
    "\n",
    "        if not os.path.exists(split_dir):\n",
    "            print(f\"    âš ï¸ FER+ {split} not found: {split_dir}\")\n",
    "            return\n",
    "\n",
    "        # Find fer2013new.csv\n",
    "        ferplus_csv = os.path.join(root_dir, 'fer2013new.csv')\n",
    "        if not os.path.exists(ferplus_csv):\n",
    "            print(f\"    âš ï¸ fer2013new.csv not found in {root_dir}\")\n",
    "            return\n",
    "\n",
    "        # Load data\n",
    "        with open(ferplus_csv, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            header = next(reader)  # Skip header\n",
    "\n",
    "            for idx, row in enumerate(reader):\n",
    "                if len(row) < 10:\n",
    "                    continue\n",
    "\n",
    "                usage = row[0]\n",
    "\n",
    "                # Filter by split\n",
    "                if usage != target_usage:\n",
    "                    continue\n",
    "\n",
    "                # Votes for each emotion (columns 1-8)\n",
    "                try:\n",
    "                    votes = [int(v.strip()) if v.strip().isdigit() else 0 for v in row[2:10]]\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if sum(votes) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Label = emotion with most votes\n",
    "                ferplus_label = np.argmax(votes)\n",
    "                unified_label = ferplus_to_unified[ferplus_label]\n",
    "\n",
    "                # Image path\n",
    "                img_name = f\"fer{idx:08d}.png\"\n",
    "                img_path = os.path.join(split_dir, img_name)\n",
    "\n",
    "                if os.path.exists(img_path):\n",
    "                    self.images.append({\n",
    "                        'path': img_path,\n",
    "                        'is_grayscale': True\n",
    "                    })\n",
    "                    self.labels.append(unified_label)\n",
    "                    self.sources.append('ferplus')\n",
    "\n",
    "    def _print_class_distribution(self):\n",
    "        \"\"\"Displays class distribution.\"\"\"\n",
    "        if len(self.labels) == 0:\n",
    "            return\n",
    "        counts = self.get_class_distribution()\n",
    "        print(\"\\n  Distribution by class:\")\n",
    "        max_count = max(counts) if len(counts) > 0 else 1\n",
    "        for i, (cls, count) in enumerate(zip(self.UNIFIED_CLASSES, counts)):\n",
    "            bar_len = int(30 * count / max_count) if max_count > 0 else 0\n",
    "            bar = 'â–ˆ' * bar_len\n",
    "            print(f\"    {cls:10s}: {count:6d} {bar}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_info['path'])\n",
    "\n",
    "        # Convert to RGB if necessary\n",
    "        if img_info['is_grayscale'] or image.mode == 'L':\n",
    "            image = image.convert('RGB')\n",
    "        else:\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        # Resize to target_size\n",
    "        if image.size != (self.target_size, self.target_size):\n",
    "            image = image.resize((self.target_size, self.target_size), Image.BILINEAR)\n",
    "\n",
    "        image = np.array(image)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            if self.use_albumentations:\n",
    "                augmented = self.transform(image=image)\n",
    "                image = augmented['image']\n",
    "            else:\n",
    "                image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def get_class_distribution(self):\n",
    "        if len(self.labels) == 0:\n",
    "            return np.zeros(self.NUM_CLASSES, dtype=int)\n",
    "        return np.bincount(self.labels, minlength=self.NUM_CLASSES)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return np.array(self.labels)\n",
    "\n",
    "    def get_source_distribution(self):\n",
    "        \"\"\"Returns the number of images per source.\"\"\"\n",
    "        from collections import Counter\n",
    "        return Counter(self.sources)\n",
    "\n",
    "\n",
    "print(\"âœ… CombinedEmotionDataset defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea561e",
   "metadata": {
    "id": "3eea561e"
   },
   "source": [
    "## 7. ğŸ§  CNN Model Architecture (with SE Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10cd35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b10cd35",
    "outputId": "befe1d6f-1f1c-4185-926a-015399ea6fc2"
   },
   "outputs": [],
   "source": [
    "# âœ… NEW: Squeeze-and-Excitation Block to improve feature attention\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation Block - improves feature quality.\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block with BatchNorm, ReLU and optional SE Block.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_se=True, reduction=16):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.se = SEBlock(out_channels, reduction) if use_se else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.se(x)  # âœ… Attention via SE Block\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FaceEmotionCNN(nn.Module):\n",
    "    \"\"\"Improved CNN with SE Blocks for emotion recognition.\"\"\"\n",
    "    def __init__(self, num_classes=8, in_channels=3, input_size=75):\n",
    "        super(FaceEmotionCNN, self).__init__()\n",
    "\n",
    "        # âœ… Blocks with SE attention\n",
    "        self.block1 = ConvBlock(in_channels, 32, use_se=True, reduction=8)   # 75 -> 37\n",
    "        self.block2 = ConvBlock(32, 64, use_se=True, reduction=8)            # 37 -> 18\n",
    "        self.block3 = ConvBlock(64, 128, use_se=True, reduction=16)          # 18 -> 9\n",
    "        self.block4 = ConvBlock(128, 256, use_se=True, reduction=16)         # 9 -> 4\n",
    "\n",
    "        # Classifier with Global Average Pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)  # âœ… Makes the model size-independent\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),  # âœ… Reduced to avoid underfitting\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_model(dataset='affectnet', num_classes=8):\n",
    "    if dataset == 'affectnet':\n",
    "        return FaceEmotionCNN(num_classes=num_classes, in_channels=config.IN_CHANNELS, input_size=config.INPUT_SIZE)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset}\")\n",
    "\n",
    "# Create and display the model\n",
    "model = create_model(dataset='affectnet', num_classes=config.NUM_CLASSES)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"ğŸ§  Model created with SE Blocks: {total_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a29893",
   "metadata": {
    "id": "91a29893"
   },
   "source": [
    "## 8. ğŸ”§ Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85c195",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c85c195",
    "outputId": "3178b421-0926-4df1-9df7-1ce2d1673894"
   },
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Tracks average values.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, per_class=False, use_amp=False):\n",
    "    \"\"\"Validation with optional per-class metrics and AMP support.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    loss_meter = AverageMeter()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if per_class:\n",
    "        class_correct = defaultdict(int)\n",
    "        class_total = defaultdict(int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "            # âš¡ Mixed Precision for validation too\n",
    "            with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            loss_meter.update(loss.item(), inputs.size(0))\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            if per_class:\n",
    "                for pred, label in zip(predicted, labels):\n",
    "                    class_total[label.item()] += 1\n",
    "                    if pred == label:\n",
    "                        class_correct[label.item()] += 1\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "\n",
    "    if per_class:\n",
    "        emotions = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral', 'Contempt']\n",
    "        print(\"\\n  ğŸ“Š Accuracy per class:\")\n",
    "        for i, emo in enumerate(emotions):\n",
    "            if class_total[i] > 0:\n",
    "                acc = 100.0 * class_correct[i] / class_total[i]\n",
    "                print(f\"    {emo:10s}: {acc:5.1f}% ({class_correct[i]}/{class_total[i]})\")\n",
    "\n",
    "    return loss_meter.avg, accuracy\n",
    "\n",
    "print(\"âœ… Utilities defined (with AMP support)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36818018",
   "metadata": {
    "id": "36818018"
   },
   "source": [
    "## 9. ğŸ“‚ Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb74ff1",
   "metadata": {
    "id": "3eb74ff1"
   },
   "source": [
    "### Dataset Configuration\n",
    "\n",
    "Choose the datasets you want to use for training. The notebook will automatically download the selected datasets via `kagglehub`.\n",
    "\n",
    "**Available datasets:**\n",
    "- `affectnet`: Main dataset (8 classes, ~50k images, RGB)\n",
    "- `fer2013`: Classic dataset (7 classes, ~35k images, grayscale 48x48)\n",
    "- `ferplus`: FER2013 with labels corrected by Microsoft (8 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4157b08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4157b08",
    "outputId": "4a44a356-8231-4440-a2a0-3ba2c3f815d3"
   },
   "outputs": [],
   "source": [
    "# Install kagglehub\n",
    "!pip install -q kagglehub\n",
    "print(\"âœ… kagglehub installed.\")\n",
    "\n",
    "# ===============================================================================\n",
    "# ğŸ¯ DATASET CONFIGURATION\n",
    "# ===============================================================================\n",
    "#\n",
    "# âš ï¸ IMPORTANT: FER2013 and FER+ use the SAME images but with different labels!\n",
    "#    - fer2013: Original labels (7 classes, noisier)\n",
    "#    - ferplus: Labels corrected by Microsoft (8 classes, 10 annotators)\n",
    "#\n",
    "#    â†’ Do not use both at the same time (duplicates)!\n",
    "#    â†’ Prefer FER+ for better quality\n",
    "#\n",
    "\n",
    "DATASETS_TO_USE = [\n",
    "    'affectnet',    # âœ… Main dataset (8 classes, ~50k images)\n",
    "    # 'fer2013',    # âŒ Replaced by FER+ (same images, worse labels)\n",
    "    'ferplus',      # âœ… FER+ with corrected labels (8 classes, ~35k images)\n",
    "]\n",
    "\n",
    "# Mode: 'combined' to merge all datasets, 'single' to use only the first one\n",
    "DATASET_MODE = 'combined' if len(DATASETS_TO_USE) > 1 else 'single'\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“‹ DATASET CONFIGURATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Selected datasets: {DATASETS_TO_USE}\")\n",
    "print(f\"  Mode: {DATASET_MODE}\")\n",
    "if 'ferplus' in DATASETS_TO_USE:\n",
    "    print(f\"  â„¹ï¸ FER+ = FER2013 with labels corrected by Microsoft (better quality)\")\n",
    "if 'fer2013' in DATASETS_TO_USE and 'ferplus' in DATASETS_TO_USE:\n",
    "    print(f\"  âš ï¸ WARNING: fer2013 and ferplus use the same images!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06138371",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06138371",
    "outputId": "e79d8b77-5b37-423a-c93a-8072dec0a288"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# ===============================================================================\n",
    "# \n",
    "# ===============================================================================\n",
    "\n",
    "# Kaggle IDs for each dataset\n",
    "KAGGLE_IDS = {\n",
    "    'affectnet': 'dollyprajapati182/balanced-affectnet',\n",
    "    'fer2013': 'msambare/fer2013',              # Folder version (images directly)\n",
    "    'fer2013_csv': 'deadskull7/fer2013',        # Original CSV version (for FER+)\n",
    "}\n",
    "\n",
    "dataset_paths = {}\n",
    "\n",
    "print(\"ğŸ“¥ Downloading datasets...\\n\")\n",
    "\n",
    "# ===============================================================================\n",
    "# 1. Download AffectNet\n",
    "# ===============================================================================\n",
    "if 'affectnet' in DATASETS_TO_USE:\n",
    "    print(f\"ğŸ“¦ [1/3] AffectNet...\")\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(KAGGLE_IDS['affectnet'])\n",
    "        dataset_paths['affectnet'] = str(path)\n",
    "        print(f\"  âœ“ Downloaded: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "        dataset_paths['affectnet'] = None\n",
    "\n",
    "# ===============================================================================\n",
    "# 2. Download FER2013 (folder version)\n",
    "# ===============================================================================\n",
    "if 'fer2013' in DATASETS_TO_USE:\n",
    "    print(f\"\\nğŸ“¦ [2/3] FER2013...\")\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(KAGGLE_IDS['fer2013'])\n",
    "        dataset_paths['fer2013'] = str(path)\n",
    "        print(f\"  âœ“ Downloaded: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "        dataset_paths['fer2013'] = None\n",
    "\n",
    "# ===============================================================================\n",
    "# 3. Configure FER+ (download CSV + generate images)\n",
    "# ===============================================================================\n",
    "if 'ferplus' in DATASETS_TO_USE:\n",
    "    print(f\"\\nğŸ“¦ [3/3] FER+ (FER2013 with Microsoft labels)...\")\n",
    "\n",
    "    # FER+ requires the original FER2013 CSV\n",
    "    print(f\"  ğŸ“¥ Downloading fer2013.csv...\")\n",
    "    try:\n",
    "        fer2013_csv_path = kagglehub.dataset_download(KAGGLE_IDS['fer2013_csv'])\n",
    "        print(f\"  âœ“ fer2013.csv downloaded: {fer2013_csv_path}\")\n",
    "\n",
    "        # Configure FER+ (download labels + generate images)\n",
    "        ferplus_path = setup_ferplus_dataset(fer2013_csv_path, output_folder='/content/ferplus_generated')\n",
    "\n",
    "        if ferplus_path:\n",
    "            dataset_paths['ferplus'] = ferplus_path\n",
    "            print(f\"  âœ“ FER+ configured: {ferplus_path}\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸ FER+ not configured (see errors above)\")\n",
    "            dataset_paths['ferplus'] = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "        dataset_paths['ferplus'] = None\n",
    "\n",
    "# ===============================================================================\n",
    "# Update config\n",
    "# ===============================================================================\n",
    "valid_paths = {k: v for k, v in dataset_paths.items() if v is not None}\n",
    "\n",
    "if 'affectnet' in valid_paths:\n",
    "    config.DATASET_ROOT = valid_paths['affectnet']\n",
    "elif valid_paths:\n",
    "    config.DATASET_ROOT = list(valid_paths.values())[0]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… DATASETS READY\")\n",
    "print(f\"{'='*60}\")\n",
    "for name, path in dataset_paths.items():\n",
    "    status = \"âœ“\" if path else \"âœ—\"\n",
    "    print(f\"  {status} {name}: {path if path else 'Not available'}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc971fb",
   "metadata": {
    "id": "fbc971fb"
   },
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# ğŸ“‚ DATA LOADING (MULTI-DATASET OR SINGLE)\n",
    "# ===============================================================================\n",
    "\n",
    "print(\"ğŸ“‚ Loading datasets...\")\n",
    "\n",
    "train_transform = get_train_transforms()\n",
    "val_transform = get_val_transforms()\n",
    "\n",
    "# Filter valid paths\n",
    "valid_dataset_paths = {k: v for k, v in dataset_paths.items() if v is not None}\n",
    "\n",
    "# ===============================================================================\n",
    "# MULTI-DATASET (combined) or SINGLE-DATASET MODE\n",
    "# ===============================================================================\n",
    "\n",
    "if DATASET_MODE == 'combined' and len(valid_dataset_paths) > 1:\n",
    "    print(f\"\\nğŸ”€ MULTI-DATASET mode activated!\")\n",
    "    print(f\"   Datasets: {list(valid_dataset_paths.keys())}\")\n",
    "\n",
    "    # Use combined dataset\n",
    "    train_dataset = CombinedEmotionDataset(\n",
    "        datasets_config=valid_dataset_paths,\n",
    "        split='train',\n",
    "        transform=train_transform,\n",
    "        use_albumentations=HAS_ALBUMENTATIONS,\n",
    "        target_size=config.INPUT_SIZE\n",
    "    )\n",
    "\n",
    "    val_dataset = CombinedEmotionDataset(\n",
    "        datasets_config=valid_dataset_paths,\n",
    "        split='val',\n",
    "        transform=val_transform,\n",
    "        use_albumentations=HAS_ALBUMENTATIONS,\n",
    "        target_size=config.INPUT_SIZE\n",
    "    )\n",
    "\n",
    "    # Update config with unified classes (8 classes)\n",
    "    config.NUM_CLASSES = CombinedEmotionDataset.NUM_CLASSES\n",
    "\n",
    "else:\n",
    "    print(f\"\\nğŸ“ SINGLE-DATASET Mode\")\n",
    "\n",
    "    # Use the first available dataset\n",
    "    dataset_name = list(valid_dataset_paths.keys())[0]\n",
    "    root_path = valid_dataset_paths[dataset_name]\n",
    "    print(f\"   Dataset: {dataset_name}\")\n",
    "\n",
    "    if dataset_name == 'affectnet':\n",
    "        train_dataset = BalancedAffectNetDataset(\n",
    "            root_dir=root_path,\n",
    "            split='train',\n",
    "            transform=train_transform,\n",
    "            use_albumentations=HAS_ALBUMENTATIONS\n",
    "        )\n",
    "        val_dataset = BalancedAffectNetDataset(\n",
    "            root_dir=root_path,\n",
    "            split='val',\n",
    "            transform=val_transform,\n",
    "            use_albumentations=HAS_ALBUMENTATIONS\n",
    "        )\n",
    "        config.NUM_CLASSES = 8\n",
    "\n",
    "    elif dataset_name == 'fer2013':\n",
    "        train_dataset = FER2013Dataset(\n",
    "            root_dir=root_path,\n",
    "            split='train',\n",
    "            transform=train_transform,\n",
    "            use_albumentations=HAS_ALBUMENTATIONS,\n",
    "            target_size=config.INPUT_SIZE\n",
    "        )\n",
    "        val_dataset = FER2013Dataset(\n",
    "            root_dir=root_path,\n",
    "            split='val',\n",
    "            transform=val_transform,\n",
    "            use_albumentations=HAS_ALBUMENTATIONS,\n",
    "            target_size=config.INPUT_SIZE\n",
    "        )\n",
    "        config.NUM_CLASSES = 7  # FER2013 has no Contempt\n",
    "\n",
    "    elif dataset_name == 'ferplus':\n",
    "        train_dataset = FERPlusDataset(\n",
    "            root_dir=root_path,\n",
    "            split='train',\n",
    "            transform=train_transform,\n",
    "            use_albumentations=HAS_ALBUMENTATIONS,\n",
    "            target_size=config.INPUT_SIZE\n",
    "        )\n",
    "        val_dataset = FERPlusDataset(\n",
    "            root_dir=root_path,\n",
    "            split='val',\n",
    "            transform=val_transform,\n",
    "            use_albumentations=HAS_ALBUMENTATIONS,\n",
    "            target_size=config.INPUT_SIZE\n",
    "        )\n",
    "        config.NUM_CLASSES = 8\n",
    "\n",
    "# ===============================================================================\n",
    "# CLASS WEIGHT CALCULATION (adaptive)\n",
    "# ===============================================================================\n",
    "\n",
    "def get_class_weights_adaptive(dataset, max_weight=5.0):\n",
    "    \"\"\"Calculates weights to balance classes (compatible with all datasets).\"\"\"\n",
    "    counts = dataset.get_class_distribution()\n",
    "    num_classes = len(counts)\n",
    "    counts = np.maximum(counts, 1)\n",
    "\n",
    "    weights = 1.0 / counts\n",
    "    weights = weights / weights.sum() * num_classes\n",
    "    weights = np.clip(weights, 0.3, max_weight)\n",
    "    weights = weights / weights.sum() * num_classes\n",
    "\n",
    "    # Get emotion names depending on dataset type\n",
    "    if hasattr(dataset, 'IDX_TO_EMOTION'):\n",
    "        idx_to_emotion = dataset.IDX_TO_EMOTION\n",
    "    elif hasattr(dataset, 'UNIFIED_CLASSES'):\n",
    "        idx_to_emotion = {i: c for i, c in enumerate(dataset.UNIFIED_CLASSES)}\n",
    "    else:\n",
    "        idx_to_emotion = {i: f\"Class_{i}\" for i in range(num_classes)}\n",
    "\n",
    "    print(f\"\\nğŸ“Š Class weights ({num_classes} classes):\")\n",
    "    for i, (count, weight) in enumerate(zip(counts, weights)):\n",
    "        emotion = idx_to_emotion.get(i, f\"Class_{i}\")\n",
    "        print(f\"    {emotion:10s}: {count:6d} samples, weight: {weight:.3f}\")\n",
    "\n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "class_weights = get_class_weights_adaptive(train_dataset, max_weight=config.MAX_CLASS_WEIGHT).to(config.DEVICE)\n",
    "\n",
    "# ===============================================================================\n",
    "# OPTIMIZED DATALOADERS\n",
    "# ===============================================================================\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None,\n",
    "    persistent_workers=config.PERSISTENT_WORKERS if config.NUM_WORKERS > 0 else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None,\n",
    "    persistent_workers=config.PERSISTENT_WORKERS if config.NUM_WORKERS > 0 else False\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"âœ… DATA LOADED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Train: {len(train_dataset):,} samples ({len(train_loader)} batches)\")\n",
    "print(f\"  Val:   {len(val_dataset):,} samples ({len(val_loader)} batches)\")\n",
    "print(f\"  Classes: {config.NUM_CLASSES}\")\n",
    "print(f\"  Input size: {config.INPUT_SIZE}x{config.INPUT_SIZE}\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"  âš¡ Workers: {config.NUM_WORKERS}, Prefetch: {config.PREFETCH_FACTOR}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed802a6",
   "metadata": {
    "id": "9ed802a6"
   },
   "source": [
    "## 10. ğŸ‘€ Sample Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2cc43f",
   "metadata": {
    "id": "6f2cc43f"
   },
   "outputs": [],
   "source": [
    "# Visualize some images from the dataset (multi-dataset compatible)\n",
    "def show_samples(dataset, n_samples=8):\n",
    "    \"\"\"Displays samples from the dataset (compatible with all datasets).\"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # ImageNet Denormalization\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "    # Get idx -> emotion mapping depending on dataset type\n",
    "    if hasattr(dataset, 'IDX_TO_EMOTION'):\n",
    "        idx_to_emotion = dataset.IDX_TO_EMOTION\n",
    "    elif hasattr(dataset, 'UNIFIED_CLASSES'):\n",
    "        idx_to_emotion = {i: c for i, c in enumerate(dataset.UNIFIED_CLASSES)}\n",
    "    else:\n",
    "        idx_to_emotion = {i: f\"Class_{i}\" for i in range(config.NUM_CLASSES)}\n",
    "\n",
    "    indices = random.sample(range(len(dataset)), min(n_samples, len(dataset)))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, label = dataset[idx]\n",
    "\n",
    "        # Convert tensor to numpy and denormalize\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img_np = img.numpy().transpose(1, 2, 0)\n",
    "        else:\n",
    "            img_np = img.transpose(1, 2, 0) if img.shape[0] == 3 else img\n",
    "\n",
    "        img_np = img_np * std + mean\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "        emotion = idx_to_emotion.get(label, f\"Class_{label}\")\n",
    "\n",
    "        # Display source if available (multi-dataset)\n",
    "        if hasattr(dataset, 'sources') and idx < len(dataset.sources):\n",
    "            source = dataset.sources[idx]\n",
    "            title = f\"{emotion}\\n({source})\"\n",
    "        else:\n",
    "            title = emotion\n",
    "\n",
    "        axes[i].imshow(img_np)\n",
    "        axes[i].set_title(title, fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Title depending on mode\n",
    "    if DATASET_MODE == 'combined' and len(valid_dataset_paths) > 1:\n",
    "        dataset_type = f\"Multi-Dataset ({', '.join(valid_dataset_paths.keys())})\"\n",
    "    else:\n",
    "        dataset_type = list(valid_dataset_paths.keys())[0] if valid_dataset_paths else \"Unknown\"\n",
    "\n",
    "    plt.suptitle(f'Samples - {dataset_type}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_samples(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f6b5b",
   "metadata": {
    "id": "bc8f6b5b"
   },
   "source": [
    "## 11. ğŸš€ Training Configuration (with SWA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc21e438",
   "metadata": {
    "id": "bc21e438"
   },
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# ğŸš€ TRAINING CONFIGURATION\n",
    "# ===============================================================================\n",
    "\n",
    "# Model\n",
    "model = create_model(dataset='affectnet', num_classes=config.NUM_CLASSES).to(config.DEVICE)\n",
    "\n",
    "# Model Compilation (PyTorch 2.0+)\n",
    "if config.USE_COMPILE and hasattr(torch, 'compile'):\n",
    "    try:\n",
    "        # âš¡ max-autotune mode: slower at start but faster afterwards\n",
    "        model = torch.compile(model, mode=config.COMPILE_MODE)\n",
    "        print(f\"âš¡ Model compiled with torch.compile(mode='{config.COMPILE_MODE}')\")\n",
    "        print(\"   Note: First epochs will be slower (compilation)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ torch.compile not available: {e}\")\n",
    "\n",
    "# Loss Function\n",
    "if config.USE_FOCAL_LOSS:\n",
    "    criterion = FocalLoss(\n",
    "        gamma=config.FOCAL_GAMMA,\n",
    "        alpha=class_weights,\n",
    "        label_smoothing=config.LABEL_SMOOTHING if config.USE_LABEL_SMOOTHING else 0.0\n",
    "    )\n",
    "    print(f\"âœ“ Focal Loss (gamma={config.FOCAL_GAMMA})\")\n",
    "elif config.USE_LABEL_SMOOTHING:\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=config.LABEL_SMOOTHING)\n",
    "    print(f\"âœ“ Label Smoothing (smoothing={config.LABEL_SMOOTHING})\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "val_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# OneCycleLR Scheduler\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=config.LEARNING_RATE * 10,\n",
    "    epochs=config.EPOCHS,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "# GradScaler for Mixed Precision\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=config.USE_AMP)\n",
    "\n",
    "# Display configuration\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“‹ Training Configuration:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Dataset: Balanced AffectNet (75x75 RGB, 8 classes)\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {config.LEARNING_RATE} -> {config.LEARNING_RATE * 10}\")\n",
    "print(f\"  Epochs: {config.EPOCHS}, Patience: {config.PATIENCE}\")\n",
    "print(f\"  Mixup: {config.USE_MIXUP} (alpha={config.MIXUP_ALPHA})\")\n",
    "print(f\"  âš¡ Mixed Precision (AMP): {config.USE_AMP}\")\n",
    "print(f\"  âš¡ torch.compile: {config.COMPILE_MODE}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedaa5fa",
   "metadata": {
    "id": "dedaa5fa"
   },
   "source": [
    "## 12. ğŸ‹ï¸ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2378eb2",
   "metadata": {
    "id": "f2378eb2"
   },
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# ğŸ‹ï¸ UNIFIED TRAINING LOOP\n",
    "# ===============================================================================\n",
    "\n",
    "import gc\n",
    "\n",
    "# Tracking variables\n",
    "best_val_acc = 0.0\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_epoch = 0\n",
    "\n",
    "# History for plots\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [],\n",
    "    'lr': [], 'epoch_time': [], 'gpu_memory': []\n",
    "}\n",
    "\n",
    "# SWA Setup (optional)\n",
    "swa_model = None\n",
    "swa_scheduler = None\n",
    "if config.USE_SWA and not config.USE_COMPILE:\n",
    "    from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "    swa_model = AveragedModel(model)\n",
    "    swa_scheduler = SWALR(optimizer, swa_lr=config.SWA_LR)\n",
    "    print(f\"âœ… SWA activated (starts at epoch {config.SWA_START_EPOCH})\")\n",
    "elif config.USE_SWA and config.USE_COMPILE:\n",
    "    print(\"âš ï¸ SWA disabled because torch.compile is enabled (incompatible)\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸš€ STARTING TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Mixed Precision: {config.USE_AMP}\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"Workers: {config.NUM_WORKERS}\")\n",
    "print(f\"Epochs: {config.EPOCHS}, Patience: {config.PATIENCE}\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "\n",
    "    loss_meter = AverageMeter()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(config.DEVICE, non_blocking=True), labels.to(config.DEVICE, non_blocking=True)\n",
    "\n",
    "        # Mixup only (CutMix disabled as it lowers performance)\n",
    "        use_mixup = config.USE_MIXUP and random.random() > 0.5\n",
    "\n",
    "        if use_mixup:\n",
    "            inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, config.MIXUP_ALPHA)\n",
    "\n",
    "        # Mixed Precision Forward Pass\n",
    "        with torch.amp.autocast('cuda', enabled=config.USE_AMP):\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if use_mixup:\n",
    "                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            loss = loss / config.ACCUMULATION_STEPS\n",
    "\n",
    "        # Backward with GradScaler\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (batch_idx + 1) % config.ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Scheduler step (not during SWA)\n",
    "            if swa_model is None or epoch < config.SWA_START_EPOCH:\n",
    "                scheduler.step()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Metrics\n",
    "        loss_meter.update(loss.item() * config.ACCUMULATION_STEPS, inputs.size(0))\n",
    "\n",
    "        if not use_mixup:\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_acc = 100.0 * correct / max(total, 1)\n",
    "\n",
    "    # SWA update after SWA_START_EPOCH\n",
    "    if swa_model is not None and epoch >= config.SWA_START_EPOCH:\n",
    "        swa_model.update_parameters(model)\n",
    "        swa_scheduler.step()\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_acc = validate(model, val_loader, val_criterion, config.DEVICE,\n",
    "                                 per_class=(epoch % 10 == 0), use_amp=config.USE_AMP)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # GPU memory tracking\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "        gpu_mem_max = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    else:\n",
    "        gpu_mem = 0\n",
    "        gpu_mem_max = 0\n",
    "\n",
    "    # Save history\n",
    "    history['train_loss'].append(loss_meter.avg)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    history['epoch_time'].append(epoch_time)\n",
    "    history['gpu_memory'].append(gpu_mem_max)\n",
    "\n",
    "    swa_status = \" [SWA]\" if swa_model is not None and epoch >= config.SWA_START_EPOCH else \"\"\n",
    "    print(f\"Epoch {epoch+1:3d}/{config.EPOCHS} | \"\n",
    "          f\"Loss: {loss_meter.avg:.4f} | Acc: {train_acc:.1f}% | \"\n",
    "          f\"Val: {val_acc:.1f}% | LR: {current_lr:.6f} | \"\n",
    "          f\"Time: {epoch_time:.1f}s | GPU: {gpu_mem:.1f}/{gpu_mem_max:.1f}GB{swa_status}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        patience_counter = 0\n",
    "\n",
    "        # Get model weights (handle torch.compile)\n",
    "        model_to_save = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_to_save.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'history': history,\n",
    "            'config': {\n",
    "                'num_classes': config.NUM_CLASSES,\n",
    "                'in_channels': config.IN_CHANNELS,\n",
    "                'input_size': config.INPUT_SIZE,\n",
    "                'dataset': 'affectnet',\n",
    "            }\n",
    "        }, config.SAVE_PATH)\n",
    "        print(f\"  âœ… [BEST] New best model! (Val Acc: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= config.PATIENCE:\n",
    "            print(f\"\\nâ¹ï¸ Early stopping after {epoch+1} epochs!\")\n",
    "            break\n",
    "\n",
    "# Memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "avg_epoch_time = np.mean(history['epoch_time'])\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"âœ… TRAINING COMPLETED!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total time: {elapsed/60:.1f} minutes\")\n",
    "print(f\"Average time per epoch: {avg_epoch_time:.1f} seconds\")\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Max GPU memory: {max(history['gpu_memory']):.2f} GB\")\n",
    "print(f\"Model saved: {config.SAVE_PATH}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f42c0b",
   "metadata": {
    "id": "13f42c0b"
   },
   "source": [
    "## 13. ğŸ“ˆ Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e82ef0",
   "metadata": {
    "id": "e8e82ef0"
   },
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# ğŸ“ˆ TRAINING RESULTS VISUALIZATION\n",
    "# ===============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# 1. Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train', color='blue')\n",
    "axes[0, 0].plot(history['val_loss'], label='Validation', color='orange')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('ğŸ“‰ Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Accuracy\n",
    "axes[0, 1].plot(history['train_acc'], label='Train', color='blue')\n",
    "axes[0, 1].plot(history['val_acc'], label='Validation', color='orange')\n",
    "axes[0, 1].axhline(y=best_val_acc, color='green', linestyle='--', label=f'Best: {best_val_acc:.1f}%')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('ğŸ“Š Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Learning Rate\n",
    "axes[0, 2].plot(history['lr'], color='green')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('Learning Rate')\n",
    "axes[0, 2].set_title('ğŸ“ˆ Learning Rate (OneCycleLR)')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Time per epoch\n",
    "axes[1, 0].plot(history['epoch_time'], color='purple')\n",
    "axes[1, 0].axhline(y=np.mean(history['epoch_time']), color='red', linestyle='--',\n",
    "                   label=f'Average: {np.mean(history[\"epoch_time\"]):.1f}s')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Time (s)')\n",
    "axes[1, 0].set_title('â±ï¸ Time per Epoch')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. GPU Memory\n",
    "axes[1, 1].plot(history['gpu_memory'], color='red')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Memory (GB)')\n",
    "axes[1, 1].set_title('ğŸ® Max GPU Memory')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Summary\n",
    "axes[1, 2].axis('off')\n",
    "summary_text = f\"\"\"\n",
    "ğŸ“‹ TRAINING SUMMARY\n",
    "\n",
    "Best accuracy: {best_val_acc:.2f}%\n",
    "Best epoch: {best_epoch}\n",
    "\n",
    "âš™ï¸ Configuration:\n",
    "â€¢ Batch Size: {config.BATCH_SIZE}\n",
    "â€¢ Epochs: {len(history['train_loss'])}\n",
    "â€¢ Mixed Precision: {config.USE_AMP}\n",
    "â€¢ torch.compile: {config.USE_COMPILE}\n",
    "â€¢ Mixup: {config.USE_MIXUP} (Î±={config.MIXUP_ALPHA})\n",
    "â€¢ CutMix: {config.USE_CUTMIX}\n",
    "â€¢ SE Blocks: {config.USE_SE_BLOCKS}\n",
    "\n",
    "â±ï¸ Performance:\n",
    "â€¢ Average time/epoch: {np.mean(history['epoch_time']):.1f}s\n",
    "\"\"\"\n",
    "axes[1, 2].text(0.1, 0.5, summary_text, fontsize=12, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e32d2",
   "metadata": {
    "id": "587e32d2"
   },
   "source": [
    "## 14. ğŸ” Final Evaluation (with TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca470346",
   "metadata": {
    "id": "ca470346"
   },
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# ğŸ” FINAL EVALUATION WITH TTA (Test-Time Augmentation)\n",
    "# ===============================================================================\n",
    "# âš ï¸ If OOM: Restart the kernel (Runtime > Restart) before running this cell\n",
    "# The CUDA Graphs cache of torch.compile cannot be released otherwise.\n",
    "\n",
    "import gc\n",
    "\n",
    "# âš¡ AGGRESSIVE GPU MEMORY CLEANUP\n",
    "print(\"ğŸ§¹ Cleaning GPU memory...\")\n",
    "\n",
    "# Delete all possible models and tensors\n",
    "for var_name in ['model', 'swa_model', 'optimizer', 'scheduler', 'scaler', 'criterion']:\n",
    "    if var_name in dir():\n",
    "        try:\n",
    "            exec(f'del {var_name}')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Force cleanup\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "gc.collect()\n",
    "\n",
    "# Display available memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_free = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
    "    gpu_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"   Free GPU memory: {gpu_free / 1024**3:.2f} GB\")\n",
    "    print(f\"   PyTorch reserved memory: {gpu_reserved:.2f} GB\")\n",
    "    if gpu_free < 2 * 1024**3:  # Less than 2GB free\n",
    "        print(\"   âš ï¸ Low free memory - using small batches\")\n",
    "\n",
    "\n",
    "def validate_with_tta(model, val_loader, criterion, device, n_augmentations=5, use_amp=False):\n",
    "    \"\"\"Validation with Test-Time Augmentation - average over multiple augmentations.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0\n",
    "\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            # Collect predictions from multiple augmentations\n",
    "            all_outputs = []\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "                # 1. Original\n",
    "                all_outputs.append(model(inputs))\n",
    "\n",
    "                # 2. Horizontal flip\n",
    "                all_outputs.append(model(torch.flip(inputs, dims=[3])))\n",
    "\n",
    "                # 3-5. Slight brightness variations\n",
    "                if n_augmentations >= 3:\n",
    "                    all_outputs.append(model(inputs * 0.95))\n",
    "                if n_augmentations >= 4:\n",
    "                    all_outputs.append(model(inputs * 1.05))\n",
    "                if n_augmentations >= 5:\n",
    "                    all_outputs.append(model(torch.flip(inputs, dims=[3]) * 0.98))\n",
    "\n",
    "            # Average predictions (soft voting)\n",
    "            avg_outputs = torch.stack(all_outputs).mean(dim=0)\n",
    "\n",
    "            loss = criterion(avg_outputs, labels)\n",
    "            loss_sum += loss.item() * batch_size\n",
    "\n",
    "            _, predicted = avg_outputs.max(1)\n",
    "            total += batch_size\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            for pred, label in zip(predicted, labels):\n",
    "                class_total[label.item()] += 1\n",
    "                if pred == label:\n",
    "                    class_correct[label.item()] += 1\n",
    "\n",
    "            # Free memory of intermediate outputs\n",
    "            del all_outputs, avg_outputs, inputs, labels\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    avg_loss = loss_sum / total\n",
    "\n",
    "    emotions = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral', 'Contempt']\n",
    "    print(f\"\\n  ğŸ“Š Accuracy per class (TTA x{n_augmentations}):\")\n",
    "    for i, emo in enumerate(emotions):\n",
    "        if class_total[i] > 0:\n",
    "            acc = 100.0 * class_correct[i] / class_total[i]\n",
    "            print(f\"    {emo:10s}: {acc:5.1f}% ({class_correct[i]}/{class_total[i]})\")\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Load the best model\n",
    "print(\"\\nğŸ“¥ Loading the best model...\")\n",
    "checkpoint = torch.load(config.SAVE_PATH, weights_only=False)\n",
    "\n",
    "# Create a new model (without compilation) to load weights\n",
    "eval_model = create_model(dataset='affectnet', num_classes=config.NUM_CLASSES).to(config.DEVICE)\n",
    "eval_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# âš¡ REDUCED BATCH SIZE to avoid OOM (CUDA Graphs cache takes ~13GB)\n",
    "EVAL_BATCH_SIZE = 256  # Much smaller to leave space\n",
    "print(f\"   âš¡ Reduced evaluation batch size: {EVAL_BATCH_SIZE} (instead of {config.BATCH_SIZE})\")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=EVAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Recreate validation criterion\n",
    "val_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“Š STANDARD EVALUATION\")\n",
    "print(f\"{'='*60}\")\n",
    "val_loss, val_acc = validate(eval_model, eval_loader, val_criterion, config.DEVICE, per_class=True, use_amp=config.USE_AMP)\n",
    "print(f\"\\nğŸ¯ Standard results:\")\n",
    "print(f\"   - Global accuracy: {val_acc:.2f}%\")\n",
    "print(f\"   - Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Cleanup before TTA (which uses more memory)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“Š EVALUATION WITH TTA (Test-Time Augmentation)\")\n",
    "print(f\"{'='*60}\")\n",
    "tta_loss, tta_acc = validate_with_tta(eval_model, eval_loader, val_criterion, config.DEVICE,\n",
    "                                       n_augmentations=5, use_amp=config.USE_AMP)\n",
    "print(f\"\\nğŸ¯ Results with TTA:\")\n",
    "print(f\"   - Global accuracy: {tta_acc:.2f}%\")\n",
    "print(f\"   - Loss: {tta_loss:.4f}\")\n",
    "print(f\"   - TTA Improvement: {tta_acc - val_acc:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126d542b",
   "metadata": {
    "id": "126d542b"
   },
   "source": [
    "## 15. ğŸ’¾ Final Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc6b5f",
   "metadata": {
    "id": "8cfc6b5f"
   },
   "outputs": [],
   "source": [
    "# Save final model (weights only) - lightweight version for deployment\n",
    "torch.save({\n",
    "    'model_state_dict': eval_model.state_dict(),  # Uses eval_model (the loaded model)\n",
    "    'num_classes': config.NUM_CLASSES,\n",
    "    'in_channels': config.IN_CHANNELS,\n",
    "    'input_size': config.INPUT_SIZE,\n",
    "    'dataset': 'affectnet',\n",
    "    'best_val_acc': checkpoint['val_acc'],  # Uses the checkpoint value\n",
    "}, 'emotion_model.pth')\n",
    "\n",
    "print(\"âœ… Model saved in 'emotion_model.pth'\")\n",
    "print(f\"   Size: {os.path.getsize('emotion_model.pth') / 1024 / 1024:.2f} MB\")\n",
    "print(f\"   Best Val Acc: {checkpoint['val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f580e241",
   "metadata": {
    "id": "f580e241"
   },
   "source": [
    "## 16. ğŸ§ª Test on Some Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c907058",
   "metadata": {
    "id": "1c907058"
   },
   "outputs": [],
   "source": [
    "def predict_emotion(model, image_tensor, device):\n",
    "    \"\"\"Predicts emotion for an image.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "        with torch.amp.autocast('cuda', enabled=config.USE_AMP):\n",
    "            outputs = model(image_tensor)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        pred_idx = outputs.argmax(1).item()\n",
    "        confidence = probs[0, pred_idx].item()\n",
    "    return pred_idx, confidence, probs[0].cpu().numpy()\n",
    "\n",
    "# Test on some validation images\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "emotions = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral', 'Contempt']\n",
    "\n",
    "indices = random.sample(range(len(val_dataset)), 8)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    img, true_label = val_dataset[idx]\n",
    "    pred_idx, confidence, probs = predict_emotion(eval_model, img, config.DEVICE)  # âš¡ Uses eval_model\n",
    "\n",
    "    img_np = img.numpy().transpose(1, 2, 0)\n",
    "    img_np = img_np * std + mean\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "    true_emotion = emotions[true_label]\n",
    "    pred_emotion = emotions[pred_idx]\n",
    "\n",
    "    color = 'green' if pred_idx == true_label else 'red'\n",
    "\n",
    "    axes[i].imshow(img_np)\n",
    "    axes[i].set_title(f\"True: {true_emotion}\\nPred: {pred_emotion} ({confidence*100:.1f}%)\",\n",
    "                      color=color, fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('ğŸ” Predictions on Validation Set', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
