{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ce613dfb",
      "metadata": {
        "id": "ce613dfb"
      },
      "source": [
        "## 1. üì¶ Imports et V√©rification GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ae9b7e4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae9b7e4b",
        "outputId": "7346ae7b-f315-45ac-e7f4-5c630a29f446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA disponible: True\n",
            "GPU: Tesla T4\n",
            "CUDA version: 12.6\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# V√©rification GPU\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9b58df31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b58df31",
        "outputId": "c97e2786-7f16-4a35-d983-3c8a190e3ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Albumentations disponible pour l'augmentation avanc√©e\n"
          ]
        }
      ],
      "source": [
        "# Import albumentations pour l'augmentation avanc√©e\n",
        "try:\n",
        "    import albumentations as A\n",
        "    from albumentations.pytorch import ToTensorV2\n",
        "    HAS_ALBUMENTATIONS = True\n",
        "    print(\"‚úÖ Albumentations disponible pour l'augmentation avanc√©e\")\n",
        "except ImportError:\n",
        "    HAS_ALBUMENTATIONS = False\n",
        "    print(\"‚ö†Ô∏è Installez albumentations: pip install albumentations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc9cb719",
      "metadata": {
        "id": "fc9cb719"
      },
      "source": [
        "## 2. ‚öôÔ∏è Configuration des Hyperparam√®tres\n",
        "\n",
        "Tous les param√®tres d'entra√Ænement sont centralis√©s ici pour faciliter l'exp√©rimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "158351ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "158351ef",
        "outputId": "f3d0b830-7cdf-4afb-91c7-9df2065adae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n",
            "VRAM totale: 14.7 GB\n",
            "‚ö° Mode: max-autotune (epoch 1 lent, suite tr√®s rapide)\n",
            "\n",
            "============================================================\n",
            "üìã CONFIGURATION MAXIMALE (Qualit√© + Vitesse)\n",
            "============================================================\n",
            "Device: cuda\n",
            "‚ö° Batch size: 1536\n",
            "‚ö° Learning rate: 0.0015\n",
            "‚ö° torch.compile: max-autotune\n",
            "‚ö° Mixed Precision: True\n",
            "‚ö° TF32: Activ√©\n",
            "‚ö†Ô∏è Epoch 1: ~2-3 min (compilation)\n",
            "‚úÖ Epochs 2+: ~20-25s (tr√®s rapide)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "class Config:\n",
        "    \"\"\"Configuration MAXIMALE - Meilleur r√©sultat possible (epoch 1 lent, suite rapide)\"\"\"\n",
        "\n",
        "    # === DONN√âES ===\n",
        "    DATASET_ROOT = './kaggle/input/balanced-affectnet'\n",
        "\n",
        "    # === MOD√àLE ===\n",
        "    NUM_CLASSES = 8\n",
        "    IN_CHANNELS = 3\n",
        "    INPUT_SIZE = 75\n",
        "    USE_SE_BLOCKS = True\n",
        "\n",
        "    # === ENTRA√éNEMENT ===\n",
        "    BATCH_SIZE = 1536     # ‚ö° Bon compromis GPU (utilise ~10-11GB)\n",
        "    ACCUMULATION_STEPS = 1\n",
        "    LEARNING_RATE = 0.0015  # Ajust√© pour batch 1536\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    EPOCHS = 100\n",
        "    PATIENCE = 20\n",
        "\n",
        "    # === TECHNIQUES AVANC√âES ===\n",
        "    USE_MIXUP = True\n",
        "    MIXUP_ALPHA = 0.2\n",
        "    USE_CUTMIX = False\n",
        "    CUTMIX_ALPHA = 1.0\n",
        "    CUTMIX_PROB = 0.0\n",
        "\n",
        "    USE_LABEL_SMOOTHING = True\n",
        "    LABEL_SMOOTHING = 0.1\n",
        "\n",
        "    USE_FOCAL_LOSS = False\n",
        "    FOCAL_GAMMA = 2.0\n",
        "\n",
        "    # === AUGMENTATION ===\n",
        "    USE_ADVANCED_AUG = True\n",
        "    USE_CLAHE = False\n",
        "    USE_GRID_DISTORTION = False\n",
        "\n",
        "    # === √âQUILIBRAGE DES CLASSES ===\n",
        "    USE_OVERSAMPLING = False\n",
        "    MAX_CLASS_WEIGHT = 3.0\n",
        "\n",
        "    # === OPTIMISATION GPU MAXIMALE ===\n",
        "    USE_AMP = True                    # ‚úÖ Mixed Precision\n",
        "    USE_COMPILE = True                # ‚úÖ torch.compile\n",
        "    COMPILE_MODE = 'max-autotune'     # ‚ö° MAXIMUM: epoch 1 lent (~2-3min) mais suite tr√®s rapide\n",
        "    NUM_WORKERS = 2                   # Optimal\n",
        "    PREFETCH_FACTOR = 4\n",
        "    PERSISTENT_WORKERS = True\n",
        "\n",
        "    # === SWA ===\n",
        "    USE_SWA = False\n",
        "    SWA_START_EPOCH = 75\n",
        "    SWA_LR = 0.0001\n",
        "\n",
        "    # === DEVICE ===\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # === SAUVEGARDE ===\n",
        "    SAVE_PATH = 'emotion_model_best.pth'\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# ‚ö° Optimisations CUDA MAXIMALES\n",
        "if torch.cuda.is_available():\n",
        "    # Performance\n",
        "    torch.backends.cudnn.benchmark = True          # Auto-tune kernels\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True   # TensorFloat-32 (2x plus rapide)\n",
        "    torch.backends.cudnn.allow_tf32 = True         # TF32 pour cuDNN\n",
        "    torch.backends.cudnn.deterministic = False     # Non-d√©terministe = plus rapide\n",
        "    torch.set_float32_matmul_precision('high')     # Tensor Cores optimis√©s\n",
        "\n",
        "    # ‚ö° NOUVEAU: Optimisations m√©moire pour batches plus grands\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)  # Utilise 95% de la VRAM\n",
        "\n",
        "    gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM totale: {gpu_mem_total:.1f} GB\")\n",
        "    print(f\"‚ö° Mode: max-autotune (epoch 1 lent, suite tr√®s rapide)\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"üìã CONFIGURATION MAXIMALE (Qualit√© + Vitesse)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Device: {config.DEVICE}\")\n",
        "print(f\"‚ö° Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"‚ö° Learning rate: {config.LEARNING_RATE}\")\n",
        "print(f\"‚ö° torch.compile: {config.COMPILE_MODE}\")\n",
        "print(f\"‚ö° Mixed Precision: {config.USE_AMP}\")\n",
        "print(f\"‚ö° TF32: Activ√©\")\n",
        "print(f\"‚ö†Ô∏è Epoch 1: ~2-3 min (compilation)\")\n",
        "print(f\"‚úÖ Epochs 2+: ~20-25s (tr√®s rapide)\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2cd297a",
      "metadata": {
        "id": "c2cd297a"
      },
      "source": [
        "## 3. üìâ Fonctions de Perte (Loss Functions)\n",
        "\n",
        "### Focal Loss\n",
        "Utile pour les datasets d√©s√©quilibr√©s - r√©duit l'importance des exemples faciles.\n",
        "\n",
        "### Label Smoothing Cross Entropy\n",
        "Emp√™che le mod√®le d'√™tre trop confiant sur les pr√©dictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "eb893d83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb893d83",
        "outputId": "68c1ada2-bf83-45fb-e35b-31c1b0329b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fonctions de perte d√©finies\n"
          ]
        }
      ],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss pour g√©rer le d√©s√©quilibre de classes.\"\"\"\n",
        "    def __init__(self, gamma=2.0, alpha=None, reduction='mean', label_smoothing=0.0):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "        self.label_smoothing = label_smoothing\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        if self.label_smoothing > 0:\n",
        "            n_classes = inputs.size(-1)\n",
        "            targets_smooth = torch.zeros_like(inputs)\n",
        "            targets_smooth.fill_(self.label_smoothing / (n_classes - 1))\n",
        "            targets_smooth.scatter_(1, targets.unsqueeze(1), 1.0 - self.label_smoothing)\n",
        "\n",
        "            log_probs = F.log_softmax(inputs, dim=-1)\n",
        "            ce_loss = -(targets_smooth * log_probs).sum(dim=-1)\n",
        "        else:\n",
        "            ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "\n",
        "        probs = torch.softmax(inputs, dim=-1)\n",
        "        pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
        "        focal_weight = (1 - pt) ** self.gamma\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            alpha_t = self.alpha.gather(0, targets)\n",
        "            focal_weight = focal_weight * alpha_t\n",
        "\n",
        "        loss = focal_weight * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        return loss\n",
        "\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"Cross Entropy avec label smoothing.\"\"\"\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        n_classes = inputs.size(-1)\n",
        "        log_probs = F.log_softmax(inputs, dim=-1)\n",
        "\n",
        "        targets_smooth = torch.zeros_like(log_probs)\n",
        "        targets_smooth.fill_(self.smoothing / (n_classes - 1))\n",
        "        targets_smooth.scatter_(1, targets.unsqueeze(1), 1.0 - self.smoothing)\n",
        "\n",
        "        loss = -(targets_smooth * log_probs).sum(dim=-1)\n",
        "        return loss.mean()\n",
        "\n",
        "print(\"‚úÖ Fonctions de perte d√©finies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88f7661f",
      "metadata": {
        "id": "88f7661f"
      },
      "source": [
        "## 4. üîÄ Mixup & CutMix\n",
        "\n",
        "Techniques d'augmentation qui m√©langent des images pour am√©liorer la g√©n√©ralisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d3c6cd7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3c6cd7f",
        "outputId": "d4dcff92-c947-4c0a-c83f-94dd022a09bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fonctions Mixup et CutMix d√©finies\n"
          ]
        }
      ],
      "source": [
        "def mixup_data(x, y, alpha=0.2):\n",
        "    \"\"\"Mixup: m√©lange deux √©chantillons.\"\"\"\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0):\n",
        "    \"\"\"CutMix: coupe et colle des patches entre √©chantillons.\"\"\"\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "    _, _, H, W = x.shape\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
        "\n",
        "    return x, y, y[index], lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"Calcule la loss mix√©e pour mixup/cutmix.\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "print(\"‚úÖ Fonctions Mixup et CutMix d√©finies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c3e9094",
      "metadata": {
        "id": "5c3e9094"
      },
      "source": [
        "## 5. üñºÔ∏è Transformations et Augmentation de Donn√©es\n",
        "\n",
        "Utilise Albumentations pour des augmentations avanc√©es (rotation, bruit, flou, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6cf4f889",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cf4f889",
        "outputId": "f7136aad-d192-437e-ef0d-89f17efe9269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Transformations d√©finies (version √©quilibr√©e)\n"
          ]
        }
      ],
      "source": [
        "def get_train_transforms():\n",
        "    \"\"\"Transformations pour l'entra√Ænement - VERSION √âQUILIBR√âE (ni trop ni trop peu).\"\"\"\n",
        "    if HAS_ALBUMENTATIONS and config.USE_ADVANCED_AUG:\n",
        "        return A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Affine(\n",
        "                translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)},\n",
        "                scale=(0.95, 1.05),   # Mod√©r√©\n",
        "                rotate=(-10, 10),     # Mod√©r√© (pas 15 qui est trop)\n",
        "                p=0.4\n",
        "            ),\n",
        "            # PAS de CLAHE ni GridDistortion (trop agressif sur 75x75)\n",
        "            A.OneOf([\n",
        "                A.GaussNoise(std_range=(0.02, 0.08), p=1),\n",
        "                A.GaussianBlur(blur_limit=(3, 5), p=1),\n",
        "            ], p=0.2),\n",
        "            A.OneOf([\n",
        "                A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=1),\n",
        "                A.RandomGamma(gamma_limit=(85, 115), p=1),\n",
        "                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=15, p=1),\n",
        "            ], p=0.4),\n",
        "            A.CoarseDropout(\n",
        "                num_holes_range=(1, 2),\n",
        "                hole_height_range=(4, 8),\n",
        "                hole_width_range=(4, 8),\n",
        "                fill=0,\n",
        "                p=0.2\n",
        "            ),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    else:\n",
        "        # Fallback vers torchvision\n",
        "        return transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
        "            transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "\n",
        "def get_val_transforms():\n",
        "    \"\"\"Transformations pour la validation (juste normalisation).\"\"\"\n",
        "    if HAS_ALBUMENTATIONS:\n",
        "        return A.Compose([\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "print(\"‚úÖ Transformations d√©finies (version √©quilibr√©e)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7d2d4b7",
      "metadata": {
        "id": "d7d2d4b7"
      },
      "source": [
        "## 6. üìÅ Dataset AffectNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4c7a3354",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c7a3354",
        "outputId": "eb9ff69b-48e2-4056-fc90-5fbdd04c3b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Classes Dataset d√©finies\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, WeightedRandomSampler\n",
        "\n",
        "class BalancedAffectNetDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset pour Balanced AffectNet.\n",
        "\n",
        "    Structure attendue:\n",
        "    data/\n",
        "        train/Anger/, Contempt/, Disgust/, Fear/, Happy/, Neutral/, Sad/, Surprise/\n",
        "        val/...\n",
        "        test/...\n",
        "    \"\"\"\n",
        "\n",
        "    NUM_CLASSES = 8\n",
        "\n",
        "    EMOTION_CLASSES = {\n",
        "        'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3,\n",
        "        'Sad': 4, 'Surprise': 5, 'Neutral': 6, 'Contempt': 7,\n",
        "    }\n",
        "\n",
        "    IDX_TO_EMOTION = {v: k for k, v in EMOTION_CLASSES.items()}\n",
        "\n",
        "    def __init__(self, root_dir='./kaggle/input/balanced-affectnet', split='train', transform=None, use_albumentations=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.use_albumentations = use_albumentations\n",
        "\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        split_dir = os.path.join(root_dir, split)\n",
        "\n",
        "        if not os.path.exists(split_dir):\n",
        "            raise FileNotFoundError(\n",
        "                f\"Dataset non trouv√©: {split_dir}\\n\"\n",
        "                f\"T√©l√©chargez depuis: https://www.kaggle.com/datasets/dollyprajapati182/balanced-affectnet\"\n",
        "            )\n",
        "\n",
        "        # Charger toutes les images\n",
        "        for emotion_name, emotion_idx in self.EMOTION_CLASSES.items():\n",
        "            emotion_dir = os.path.join(split_dir, emotion_name)\n",
        "            if not os.path.exists(emotion_dir):\n",
        "                print(f\"‚ö†Ô∏è {emotion_dir} non trouv√©, ignor√©...\")\n",
        "                continue\n",
        "\n",
        "            for img_name in os.listdir(emotion_dir):\n",
        "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "                    self.images.append(os.path.join(emotion_dir, img_name))\n",
        "                    self.labels.append(emotion_idx)\n",
        "\n",
        "        print(f\"üìÇ Charg√© {len(self.images)} images depuis AffectNet {split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        image = np.array(image)\n",
        "\n",
        "        if self.transform:\n",
        "            if self.use_albumentations:\n",
        "                augmented = self.transform(image=image)\n",
        "                image = augmented['image']\n",
        "            else:\n",
        "                image = self.transform(image)\n",
        "        else:\n",
        "            image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def get_class_distribution(self):\n",
        "        return np.bincount(self.labels, minlength=self.NUM_CLASSES)\n",
        "\n",
        "    def get_labels(self):\n",
        "        return np.array(self.labels)\n",
        "\n",
        "\n",
        "def get_class_weights(dataset, max_weight=5.0):\n",
        "    \"\"\"Calcule les poids pour √©quilibrer les classes.\"\"\"\n",
        "    counts = dataset.get_class_distribution()\n",
        "    counts = np.maximum(counts, 1)\n",
        "\n",
        "    weights = 1.0 / counts\n",
        "    weights = weights / weights.sum() * len(weights)\n",
        "    weights = np.clip(weights, 0.3, max_weight)\n",
        "    weights = weights / weights.sum() * len(weights)\n",
        "\n",
        "    print(\"\\nüìä Poids des classes:\")\n",
        "    for i, (count, weight) in enumerate(zip(counts, weights)):\n",
        "        emotion = BalancedAffectNetDataset.IDX_TO_EMOTION.get(i, f\"Class_{i}\")\n",
        "        print(f\"    {emotion:10s}: {count:5d} samples, poids: {weight:.3f}\")\n",
        "\n",
        "    return torch.FloatTensor(weights)\n",
        "\n",
        "\n",
        "def get_balanced_sampler(dataset):\n",
        "    \"\"\"Cr√©e un sampler √©quilibr√© pour l'entra√Ænement.\"\"\"\n",
        "    labels = dataset.get_labels()\n",
        "    counts = np.bincount(labels, minlength=BalancedAffectNetDataset.NUM_CLASSES)\n",
        "    counts = np.maximum(counts, 1)\n",
        "\n",
        "    weights = 1.0 / counts\n",
        "    sample_weights = weights[labels]\n",
        "\n",
        "    return WeightedRandomSampler(\n",
        "        weights=sample_weights,\n",
        "        num_samples=len(sample_weights),\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Classes Dataset d√©finies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af2a8b07",
      "metadata": {
        "id": "af2a8b07"
      },
      "source": [
        "## 6bis. üìÅ Dataset FER2013 / FER2013+\n",
        "\n",
        "FER2013 est un dataset classique de reconnaissance d'√©motions avec ~35k images en 48x48 grayscale.\n",
        "FER2013+ est une version avec des labels corrig√©s et am√©lior√©s par Microsoft."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "65bc33f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65bc33f5",
        "outputId": "4f204b50-8525-42dc-f658-6e58537d3af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Datasets FER2013 et FER+ d√©finis\n"
          ]
        }
      ],
      "source": [
        "class FER2013Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset FER2013 avec conversion grayscale -> RGB.\n",
        "\n",
        "    Structure Kaggle (msambare/fer2013):\n",
        "        train/angry/, disgust/, fear/, happy/, neutral/, sad/, surprise/\n",
        "        test/...\n",
        "\n",
        "    Les images sont en 48x48 grayscale, automatiquement:\n",
        "    - Redimensionn√©es vers target_size (75x75 par d√©faut)\n",
        "    - Converties en RGB (pour compatibilit√© avec le mod√®le)\n",
        "    \"\"\"\n",
        "\n",
        "    NUM_CLASSES = 7  # Pas de Contempt dans FER2013\n",
        "\n",
        "    EMOTION_CLASSES = {\n",
        "        'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3,\n",
        "        'sad': 4, 'surprise': 5, 'neutral': 6\n",
        "    }\n",
        "\n",
        "    IDX_TO_EMOTION = {v: k.capitalize() for k, v in EMOTION_CLASSES.items()}\n",
        "\n",
        "    def __init__(self, root_dir, split='train', transform=None,\n",
        "                 use_albumentations=False, target_size=75):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = 'train' if split == 'train' else 'test'  # FER2013 n'a que train/test\n",
        "        self.transform = transform\n",
        "        self.use_albumentations = use_albumentations\n",
        "        self.target_size = target_size\n",
        "\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        split_dir = os.path.join(root_dir, self.split)\n",
        "\n",
        "        if not os.path.exists(split_dir):\n",
        "            raise FileNotFoundError(f\"FER2013 non trouv√©: {split_dir}\")\n",
        "\n",
        "        for emotion_name, emotion_idx in self.EMOTION_CLASSES.items():\n",
        "            emotion_dir = os.path.join(split_dir, emotion_name)\n",
        "            if not os.path.exists(emotion_dir):\n",
        "                continue\n",
        "\n",
        "            for img_name in os.listdir(emotion_dir):\n",
        "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    self.images.append(os.path.join(emotion_dir, img_name))\n",
        "                    self.labels.append(emotion_idx)\n",
        "\n",
        "        print(f\"üìÇ FER2013 {self.split}: {len(self.images)} images charg√©es\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Charger l'image (peut √™tre grayscale)\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        # Convertir en RGB si grayscale\n",
        "        if image.mode == 'L':\n",
        "            image = image.convert('RGB')\n",
        "        else:\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        # Redimensionner vers target_size (48x48 -> 75x75)\n",
        "        if image.size != (self.target_size, self.target_size):\n",
        "            image = image.resize((self.target_size, self.target_size), Image.BILINEAR)\n",
        "\n",
        "        image = np.array(image)\n",
        "\n",
        "        if self.transform:\n",
        "            if self.use_albumentations:\n",
        "                augmented = self.transform(image=image)\n",
        "                image = augmented['image']\n",
        "            else:\n",
        "                image = self.transform(image)\n",
        "        else:\n",
        "            image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def get_class_distribution(self):\n",
        "        return np.bincount(self.labels, minlength=self.NUM_CLASSES)\n",
        "\n",
        "    def get_labels(self):\n",
        "        return np.array(self.labels)\n",
        "\n",
        "\n",
        "# ===============================================================================\n",
        "# üì¶ FER+ (FER2013 avec labels corrig√©s par Microsoft)\n",
        "# ===============================================================================\n",
        "\n",
        "def download_ferplus_labels(dest_folder):\n",
        "    \"\"\"\n",
        "    T√©l√©charge fer2013new.csv depuis le repo Microsoft FERPlus.\n",
        "\n",
        "    Returns:\n",
        "        str: Chemin vers le fichier t√©l√©charg√©\n",
        "    \"\"\"\n",
        "    import urllib.request\n",
        "\n",
        "    url = \"https://raw.githubusercontent.com/microsoft/FERPlus/master/fer2013new.csv\"\n",
        "    dest_path = os.path.join(dest_folder, \"fer2013new.csv\")\n",
        "\n",
        "    if os.path.exists(dest_path):\n",
        "        print(f\"  ‚úì fer2013new.csv d√©j√† pr√©sent\")\n",
        "        return dest_path\n",
        "\n",
        "    print(f\"  üì• T√©l√©chargement de fer2013new.csv depuis GitHub...\")\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, dest_path)\n",
        "        print(f\"  ‚úì T√©l√©charg√©: {dest_path}\")\n",
        "        return dest_path\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Erreur: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def generate_ferplus_images(fer2013_csv_path, ferplus_csv_path, output_folder):\n",
        "    \"\"\"\n",
        "    G√©n√®re les images PNG depuis fer2013.csv avec les labels FER+.\n",
        "\n",
        "    Structure de sortie:\n",
        "        output_folder/\n",
        "            FER2013Train/\n",
        "                fer0000000.png\n",
        "                ...\n",
        "            FER2013Valid/\n",
        "            FER2013Test/\n",
        "\n",
        "    Returns:\n",
        "        bool: True si succ√®s\n",
        "    \"\"\"\n",
        "    import csv\n",
        "\n",
        "    # Cr√©er les dossiers\n",
        "    for split in ['FER2013Train', 'FER2013Valid', 'FER2013Test']:\n",
        "        os.makedirs(os.path.join(output_folder, split), exist_ok=True)\n",
        "\n",
        "    # Mapping usage -> dossier\n",
        "    usage_to_folder = {\n",
        "        'Training': 'FER2013Train',\n",
        "        'PublicTest': 'FER2013Valid',\n",
        "        'PrivateTest': 'FER2013Test'\n",
        "    }\n",
        "\n",
        "    # Lire fer2013.csv et g√©n√©rer les images\n",
        "    print(f\"  üñºÔ∏è G√©n√©ration des images depuis fer2013.csv...\")\n",
        "\n",
        "    with open(fer2013_csv_path, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader)  # emotion,pixels,Usage\n",
        "\n",
        "        for idx, row in enumerate(reader):\n",
        "            if len(row) < 3:\n",
        "                continue\n",
        "\n",
        "            emotion = row[0]\n",
        "            pixels = row[1]\n",
        "            usage = row[2]\n",
        "\n",
        "            # Convertir pixels en image\n",
        "            pixel_values = [int(p) for p in pixels.split()]\n",
        "            img_array = np.array(pixel_values, dtype=np.uint8).reshape(48, 48)\n",
        "            img = Image.fromarray(img_array, mode='L')\n",
        "\n",
        "            # Sauvegarder\n",
        "            folder = usage_to_folder.get(usage, 'FER2013Train')\n",
        "            img_name = f\"fer{idx:08d}.png\"\n",
        "            img_path = os.path.join(output_folder, folder, img_name)\n",
        "            img.save(img_path)\n",
        "\n",
        "            if idx % 5000 == 0:\n",
        "                print(f\"    Progression: {idx} images...\")\n",
        "\n",
        "    print(f\"  ‚úì {idx + 1} images g√©n√©r√©es\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def setup_ferplus_dataset(fer2013_kaggle_path, output_folder=None):\n",
        "    \"\"\"\n",
        "    Configure le dataset FER+ complet:\n",
        "    1. T√©l√©charge fer2013new.csv depuis GitHub\n",
        "    2. Trouve/t√©l√©charge fer2013.csv depuis Kaggle\n",
        "    3. G√©n√®re les images PNG\n",
        "\n",
        "    Args:\n",
        "        fer2013_kaggle_path: Chemin vers le dataset FER2013 Kaggle (msambare/fer2013)\n",
        "        output_folder: Dossier de sortie (optionnel)\n",
        "\n",
        "    Returns:\n",
        "        str: Chemin vers le dataset FER+ pr√™t √† l'emploi\n",
        "    \"\"\"\n",
        "    if output_folder is None:\n",
        "        output_folder = os.path.join(os.path.dirname(fer2013_kaggle_path), 'ferplus_generated')\n",
        "\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # V√©rifier si d√©j√† g√©n√©r√©\n",
        "    train_folder = os.path.join(output_folder, 'FER2013Train')\n",
        "    if os.path.exists(train_folder) and len(os.listdir(train_folder)) > 1000:\n",
        "        print(f\"  ‚úì FER+ d√©j√† g√©n√©r√© dans {output_folder}\")\n",
        "        # T√©l√©charger quand m√™me les labels si pas pr√©sents\n",
        "        download_ferplus_labels(output_folder)\n",
        "        return output_folder\n",
        "\n",
        "    print(\"\\nüîß Configuration de FER+ (premi√®re utilisation)...\")\n",
        "\n",
        "    # 1. T√©l√©charger les labels FER+ depuis GitHub\n",
        "    ferplus_csv = download_ferplus_labels(output_folder)\n",
        "    if ferplus_csv is None:\n",
        "        return None\n",
        "\n",
        "    # 2. Trouver fer2013.csv\n",
        "    # Le dataset Kaggle msambare/fer2013 est en format dossiers, pas CSV\n",
        "    # On doit utiliser le dataset original: deadskull7/fer2013\n",
        "    fer2013_csv = os.path.join(fer2013_kaggle_path, 'fer2013.csv')\n",
        "\n",
        "    if not os.path.exists(fer2013_csv):\n",
        "        # Chercher dans d'autres emplacements possibles\n",
        "        for alt_path in [\n",
        "            os.path.join(fer2013_kaggle_path, 'fer2013', 'fer2013.csv'),\n",
        "            os.path.join(fer2013_kaggle_path, 'data', 'fer2013.csv'),\n",
        "        ]:\n",
        "            if os.path.exists(alt_path):\n",
        "                fer2013_csv = alt_path\n",
        "                break\n",
        "\n",
        "    if not os.path.exists(fer2013_csv):\n",
        "        print(f\"  ‚ö†Ô∏è fer2013.csv non trouv√©. FER+ n√©cessite le dataset CSV original.\")\n",
        "        print(f\"     Le dataset Kaggle 'msambare/fer2013' est en format image.\")\n",
        "        print(f\"     Pour FER+, utilisez 'deadskull7/fer2013' qui contient le CSV.\")\n",
        "        return None\n",
        "\n",
        "    # 3. G√©n√©rer les images\n",
        "    success = generate_ferplus_images(fer2013_csv, ferplus_csv, output_folder)\n",
        "\n",
        "    if success:\n",
        "        return output_folder\n",
        "    return None\n",
        "\n",
        "\n",
        "class FERPlusDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset FER2013+ (FER+) avec labels corrig√©s par Microsoft.\n",
        "\n",
        "    FER+ am√©liore FER2013 avec:\n",
        "    - Labels vot√©s par 10 annotateurs (plus fiables)\n",
        "    - 8 classes (ajout de Contempt)\n",
        "    - Possibilit√© d'utiliser les probabilit√©s de vote\n",
        "\n",
        "    Le dataset est automatiquement configur√© depuis:\n",
        "    - fer2013new.csv (labels) depuis GitHub Microsoft\n",
        "    - fer2013.csv (images) depuis Kaggle\n",
        "    \"\"\"\n",
        "\n",
        "    NUM_CLASSES = 8\n",
        "\n",
        "    # Colonnes du CSV FER+: usage, neutral, happiness, surprise, sadness, anger, disgust, fear, contempt, unknown, NF\n",
        "    FERPLUS_EMOTIONS = ['neutral', 'happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear', 'contempt']\n",
        "\n",
        "    # Mapping FER+ order -> Unified order (AffectNet)\n",
        "    # FER+: neutral(0), happiness(1), surprise(2), sadness(3), anger(4), disgust(5), fear(6), contempt(7)\n",
        "    # Unified: Anger(0), Disgust(1), Fear(2), Happy(3), Sad(4), Surprise(5), Neutral(6), Contempt(7)\n",
        "    FERPLUS_TO_UNIFIED = {\n",
        "        0: 6,  # neutral -> Neutral\n",
        "        1: 3,  # happiness -> Happy\n",
        "        2: 5,  # surprise -> Surprise\n",
        "        3: 4,  # sadness -> Sad\n",
        "        4: 0,  # anger -> Anger\n",
        "        5: 1,  # disgust -> Disgust\n",
        "        6: 2,  # fear -> Fear\n",
        "        7: 7,  # contempt -> Contempt\n",
        "    }\n",
        "\n",
        "    IDX_TO_EMOTION = {\n",
        "        0: 'Anger', 1: 'Disgust', 2: 'Fear', 3: 'Happy',\n",
        "        4: 'Sad', 5: 'Surprise', 6: 'Neutral', 7: 'Contempt'\n",
        "    }\n",
        "\n",
        "    def __init__(self, root_dir, split='train', transform=None,\n",
        "                 use_albumentations=False, target_size=75,\n",
        "                 label_mode='majority', min_votes=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir: Chemin vers le dataset FER+ (avec FER2013Train/, etc.)\n",
        "            split: 'train', 'val' ou 'test'\n",
        "            label_mode: 'majority' (label le plus vot√©) ou 'probability' (distribution)\n",
        "            min_votes: Nombre minimum de votes pour inclure une image\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.use_albumentations = use_albumentations\n",
        "        self.target_size = target_size\n",
        "        self.label_mode = label_mode\n",
        "        self.min_votes = min_votes\n",
        "\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.vote_distributions = []  # Pour le mode probability\n",
        "\n",
        "        # Mapping split -> folder\n",
        "        split_to_folder = {\n",
        "            'train': 'FER2013Train',\n",
        "            'val': 'FER2013Valid',\n",
        "            'test': 'FER2013Test'\n",
        "        }\n",
        "\n",
        "        folder_name = split_to_folder.get(split, 'FER2013Train')\n",
        "        split_dir = os.path.join(root_dir, folder_name)\n",
        "\n",
        "        if not os.path.exists(split_dir):\n",
        "            print(f\"‚ö†Ô∏è FER+ {split} non trouv√©: {split_dir}\")\n",
        "            return\n",
        "\n",
        "        # Charger les labels depuis fer2013new.csv\n",
        "        ferplus_csv = os.path.join(root_dir, 'fer2013new.csv')\n",
        "        if not os.path.exists(ferplus_csv):\n",
        "            print(f\"‚ö†Ô∏è fer2013new.csv non trouv√© dans {root_dir}\")\n",
        "            return\n",
        "\n",
        "        self._load_data(split_dir, ferplus_csv, split)\n",
        "\n",
        "        print(f\"üìÇ FER+ {split}: {len(self.images)} images charg√©es (mode: {label_mode})\")\n",
        "\n",
        "    def _load_data(self, split_dir, ferplus_csv, split):\n",
        "        \"\"\"Charge les images et labels.\"\"\"\n",
        "        import csv\n",
        "\n",
        "        # Mapping usage dans le CSV\n",
        "        usage_mapping = {\n",
        "            'train': 'Training',\n",
        "            'val': 'PublicTest',\n",
        "            'test': 'PrivateTest'\n",
        "        }\n",
        "        target_usage = usage_mapping.get(split, 'Training')\n",
        "\n",
        "        with open(ferplus_csv, 'r') as f:\n",
        "            reader = csv.reader(f)\n",
        "            header = next(reader)  # Skip header\n",
        "\n",
        "            for idx, row in enumerate(reader):\n",
        "                if len(row) < 10:\n",
        "                    continue\n",
        "\n",
        "                usage = row[0]\n",
        "\n",
        "                # Filtrer par split\n",
        "                if usage != target_usage:\n",
        "                    continue\n",
        "\n",
        "                # Votes pour chaque √©motion (colonnes 1-8)\n",
        "                # Format: usage, neutral, happiness, surprise, sadness, anger, disgust, fear, contempt, unknown, NF\n",
        "                try:\n",
        "                    votes = [int(v) if v.strip().isdigit() else 0 for v in row[1:9]]\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "                total_votes = sum(votes)\n",
        "\n",
        "                # Ignorer si pas assez de votes valides ou si c'est \"unknown\" / \"NF\"\n",
        "                if total_votes < self.min_votes:\n",
        "                    continue\n",
        "\n",
        "                # Chemin de l'image\n",
        "                img_name = f\"fer{idx:08d}.png\"\n",
        "                img_path = os.path.join(split_dir, img_name)\n",
        "\n",
        "                if not os.path.exists(img_path):\n",
        "                    continue\n",
        "\n",
        "                # Calculer le label\n",
        "                ferplus_label = np.argmax(votes)\n",
        "                unified_label = self.FERPLUS_TO_UNIFIED[ferplus_label]\n",
        "\n",
        "                self.images.append(img_path)\n",
        "                self.labels.append(unified_label)\n",
        "\n",
        "                # Stocker la distribution pour le mode probability\n",
        "                if self.label_mode == 'probability':\n",
        "                    vote_dist = np.array(votes, dtype=np.float32)\n",
        "                    vote_dist = vote_dist / vote_dist.sum()  # Normaliser\n",
        "                    # R√©ordonner selon l'ordre unifi√©\n",
        "                    unified_dist = np.zeros(8, dtype=np.float32)\n",
        "                    for ferplus_idx, unified_idx in self.FERPLUS_TO_UNIFIED.items():\n",
        "                        unified_dist[unified_idx] = vote_dist[ferplus_idx]\n",
        "                    self.vote_distributions.append(unified_dist)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Charger l'image\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        # Convertir en RGB\n",
        "        if image.mode == 'L':\n",
        "            image = image.convert('RGB')\n",
        "        else:\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        # Redimensionner\n",
        "        if image.size != (self.target_size, self.target_size):\n",
        "            image = image.resize((self.target_size, self.target_size), Image.BILINEAR)\n",
        "\n",
        "        image = np.array(image)\n",
        "\n",
        "        if self.transform:\n",
        "            if self.use_albumentations:\n",
        "                augmented = self.transform(image=image)\n",
        "                image = augmented['image']\n",
        "            else:\n",
        "                image = self.transform(image)\n",
        "        else:\n",
        "            image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
        "\n",
        "        # Retourner selon le mode\n",
        "        if self.label_mode == 'probability' and len(self.vote_distributions) > idx:\n",
        "            return image, label, torch.tensor(self.vote_distributions[idx])\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def get_class_distribution(self):\n",
        "        return np.bincount(self.labels, minlength=self.NUM_CLASSES)\n",
        "\n",
        "    def get_labels(self):\n",
        "        return np.array(self.labels)\n",
        "\n",
        "\n",
        "print(\"‚úÖ Datasets FER2013 et FER+ d√©finis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78eda0ed",
      "metadata": {
        "id": "78eda0ed"
      },
      "source": [
        "## 6ter. üîÄ Dataset Combin√© Multi-Sources\n",
        "\n",
        "Ce dataset combine AffectNet, FER2013 et/ou FER+ en unifiant les classes vers 8 √©motions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "501edca3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "501edca3",
        "outputId": "a93c789b-2bd4-4f3d-90b9-b9e15f6f818c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ CombinedEmotionDataset d√©fini\n"
          ]
        }
      ],
      "source": [
        "class CombinedEmotionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset combinant plusieurs sources avec mapping unifi√© des classes.\n",
        "\n",
        "    Combine AffectNet, FER2013 et FER+ avec:\n",
        "    - Redimensionnement automatique vers target_size\n",
        "    - Conversion grayscale -> RGB automatique\n",
        "    - Mapping unifi√© vers 8 classes (ordre AffectNet)\n",
        "\n",
        "    Classes unifi√©es:\n",
        "        0: Anger, 1: Disgust, 2: Fear, 3: Happy,\n",
        "        4: Sad, 5: Surprise, 6: Neutral, 7: Contempt\n",
        "    \"\"\"\n",
        "\n",
        "    # 8 classes unifi√©es (ordre AffectNet)\n",
        "    UNIFIED_CLASSES = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral', 'Contempt']\n",
        "    NUM_CLASSES = 8\n",
        "    IDX_TO_EMOTION = {i: c for i, c in enumerate(UNIFIED_CLASSES)}\n",
        "\n",
        "    def __init__(self, datasets_config, split='train', transform=None,\n",
        "                 use_albumentations=False, target_size=75):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            datasets_config: dict {nom_dataset: chemin_racine}\n",
        "                Exemple: {'affectnet': '/path/to/affectnet', 'fer2013': '/path/to/fer2013'}\n",
        "            split: 'train', 'val', ou 'test'\n",
        "            target_size: taille de sortie uniforme (75 par d√©faut)\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        self.use_albumentations = use_albumentations\n",
        "        self.target_size = target_size\n",
        "\n",
        "        self.images = []  # Liste de dicts: {'path': str, 'is_grayscale': bool}\n",
        "        self.labels = []\n",
        "        self.sources = []  # Pour tracking/debug\n",
        "\n",
        "        total_by_source = {}\n",
        "\n",
        "        for dataset_name, root_dir in datasets_config.items():\n",
        "            if root_dir is None or not os.path.exists(root_dir):\n",
        "                print(f\"‚ö†Ô∏è {dataset_name} ignor√© (non trouv√©): {root_dir}\")\n",
        "                continue\n",
        "\n",
        "            count_before = len(self.images)\n",
        "\n",
        "            if dataset_name == 'affectnet':\n",
        "                self._load_affectnet(root_dir, split)\n",
        "            elif dataset_name == 'fer2013':\n",
        "                self._load_fer2013(root_dir, split)\n",
        "            elif dataset_name == 'ferplus':\n",
        "                self._load_ferplus(root_dir, split)\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Dataset inconnu: {dataset_name}\")\n",
        "                continue\n",
        "\n",
        "            total_by_source[dataset_name] = len(self.images) - count_before\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"üìä DATASET COMBIN√â ({split})\")\n",
        "        print(f\"{'='*50}\")\n",
        "        for src, count in total_by_source.items():\n",
        "            print(f\"  {src:15s}: {count:6d} images\")\n",
        "        print(f\"  {'TOTAL':15s}: {len(self.images):6d} images\")\n",
        "        print(f\"{'='*50}\")\n",
        "        self._print_class_distribution()\n",
        "\n",
        "    def _load_affectnet(self, root_dir, split):\n",
        "        \"\"\"Charge les images AffectNet.\"\"\"\n",
        "        affectnet_mapping = {\n",
        "            'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3,\n",
        "            'Sad': 4, 'Surprise': 5, 'Neutral': 6, 'Contempt': 7\n",
        "        }\n",
        "\n",
        "        split_dir = os.path.join(root_dir, split)\n",
        "        if not os.path.exists(split_dir):\n",
        "            return\n",
        "\n",
        "        for emotion_name, unified_idx in affectnet_mapping.items():\n",
        "            emotion_dir = os.path.join(split_dir, emotion_name)\n",
        "            if not os.path.exists(emotion_dir):\n",
        "                continue\n",
        "\n",
        "            for img_name in os.listdir(emotion_dir):\n",
        "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "                    self.images.append({\n",
        "                        'path': os.path.join(emotion_dir, img_name),\n",
        "                        'is_grayscale': False\n",
        "                    })\n",
        "                    self.labels.append(unified_idx)\n",
        "                    self.sources.append('affectnet')\n",
        "\n",
        "    def _load_fer2013(self, root_dir, split):\n",
        "        \"\"\"Charge les images FER2013.\"\"\"\n",
        "        # FER2013 n'a que train/test, pas de val\n",
        "        fer_split = 'train' if split == 'train' else 'test'\n",
        "        split_dir = os.path.join(root_dir, fer_split)\n",
        "\n",
        "        if not os.path.exists(split_dir):\n",
        "            return\n",
        "\n",
        "        # Mapping FER2013 (7 classes) -> unifi√© (8 classes)\n",
        "        fer_to_unified = {\n",
        "            'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3,\n",
        "            'sad': 4, 'surprise': 5, 'neutral': 6\n",
        "        }\n",
        "\n",
        "        for emotion_name, unified_idx in fer_to_unified.items():\n",
        "            emotion_dir = os.path.join(split_dir, emotion_name)\n",
        "            if not os.path.exists(emotion_dir):\n",
        "                continue\n",
        "\n",
        "            for img_name in os.listdir(emotion_dir):\n",
        "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    self.images.append({\n",
        "                        'path': os.path.join(emotion_dir, img_name),\n",
        "                        'is_grayscale': True  # FER2013 est en grayscale\n",
        "                    })\n",
        "                    self.labels.append(unified_idx)\n",
        "                    self.sources.append('fer2013')\n",
        "\n",
        "    def _load_ferplus(self, root_dir, split):\n",
        "        \"\"\"Charge les images FER+ (avec labels corrig√©s par Microsoft).\"\"\"\n",
        "        import csv\n",
        "\n",
        "        # Mapping FER+ order -> Unified order\n",
        "        ferplus_to_unified = {\n",
        "            0: 6,  # neutral -> Neutral\n",
        "            1: 3,  # happiness -> Happy\n",
        "            2: 5,  # surprise -> Surprise\n",
        "            3: 4,  # sadness -> Sad\n",
        "            4: 0,  # anger -> Anger\n",
        "            5: 1,  # disgust -> Disgust\n",
        "            6: 2,  # fear -> Fear\n",
        "            7: 7,  # contempt -> Contempt\n",
        "        }\n",
        "\n",
        "        # Mapping split -> folder et usage\n",
        "        split_mapping = {\n",
        "            'train': ('FER2013Train', 'Training'),\n",
        "            'val': ('FER2013Valid', 'PublicTest'),\n",
        "            'test': ('FER2013Test', 'PrivateTest')\n",
        "        }\n",
        "\n",
        "        folder_name, target_usage = split_mapping.get(split, ('FER2013Train', 'Training'))\n",
        "        split_dir = os.path.join(root_dir, folder_name)\n",
        "\n",
        "        if not os.path.exists(split_dir):\n",
        "            print(f\"    ‚ö†Ô∏è FER+ {split} non trouv√©: {split_dir}\")\n",
        "            return\n",
        "\n",
        "        # Trouver fer2013new.csv\n",
        "        ferplus_csv = os.path.join(root_dir, 'fer2013new.csv')\n",
        "        if not os.path.exists(ferplus_csv):\n",
        "            print(f\"    ‚ö†Ô∏è fer2013new.csv non trouv√© dans {root_dir}\")\n",
        "            return\n",
        "\n",
        "        # Charger les donn√©es\n",
        "        with open(ferplus_csv, 'r') as f:\n",
        "            reader = csv.reader(f)\n",
        "            header = next(reader)  # Skip header\n",
        "\n",
        "            for idx, row in enumerate(reader):\n",
        "                if len(row) < 10:\n",
        "                    continue\n",
        "\n",
        "                usage = row[0]\n",
        "\n",
        "                # Filtrer par split\n",
        "                if usage != target_usage:\n",
        "                    continue\n",
        "\n",
        "                # Votes pour chaque √©motion (colonnes 1-8)\n",
        "                try:\n",
        "                    votes = [int(v.strip()) if v.strip().isdigit() else 0 for v in row[1:9]]\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "                if sum(votes) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Label = √©motion avec le plus de votes\n",
        "                ferplus_label = np.argmax(votes)\n",
        "                unified_label = ferplus_to_unified[ferplus_label]\n",
        "\n",
        "                # Chemin de l'image\n",
        "                img_name = f\"fer{idx:08d}.png\"\n",
        "                img_path = os.path.join(split_dir, img_name)\n",
        "\n",
        "                if os.path.exists(img_path):\n",
        "                    self.images.append({\n",
        "                        'path': img_path,\n",
        "                        'is_grayscale': True\n",
        "                    })\n",
        "                    self.labels.append(unified_label)\n",
        "                    self.sources.append('ferplus')\n",
        "\n",
        "    def _print_class_distribution(self):\n",
        "        \"\"\"Affiche la distribution des classes.\"\"\"\n",
        "        if len(self.labels) == 0:\n",
        "            return\n",
        "        counts = self.get_class_distribution()\n",
        "        print(\"\\n  Distribution par classe:\")\n",
        "        max_count = max(counts) if len(counts) > 0 else 1\n",
        "        for i, (cls, count) in enumerate(zip(self.UNIFIED_CLASSES, counts)):\n",
        "            bar_len = int(30 * count / max_count) if max_count > 0 else 0\n",
        "            bar = '‚ñà' * bar_len\n",
        "            print(f\"    {cls:10s}: {count:6d} {bar}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_info = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Charger l'image\n",
        "        image = Image.open(img_info['path'])\n",
        "\n",
        "        # Convertir en RGB si n√©cessaire\n",
        "        if img_info['is_grayscale'] or image.mode == 'L':\n",
        "            image = image.convert('RGB')\n",
        "        else:\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        # Redimensionner vers target_size\n",
        "        if image.size != (self.target_size, self.target_size):\n",
        "            image = image.resize((self.target_size, self.target_size), Image.BILINEAR)\n",
        "\n",
        "        image = np.array(image)\n",
        "\n",
        "        # Appliquer les transformations\n",
        "        if self.transform:\n",
        "            if self.use_albumentations:\n",
        "                augmented = self.transform(image=image)\n",
        "                image = augmented['image']\n",
        "            else:\n",
        "                image = self.transform(image)\n",
        "        else:\n",
        "            image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def get_class_distribution(self):\n",
        "        if len(self.labels) == 0:\n",
        "            return np.zeros(self.NUM_CLASSES, dtype=int)\n",
        "        return np.bincount(self.labels, minlength=self.NUM_CLASSES)\n",
        "\n",
        "    def get_labels(self):\n",
        "        return np.array(self.labels)\n",
        "\n",
        "    def get_source_distribution(self):\n",
        "        \"\"\"Retourne le nombre d'images par source.\"\"\"\n",
        "        from collections import Counter\n",
        "        return Counter(self.sources)\n",
        "\n",
        "\n",
        "print(\"‚úÖ CombinedEmotionDataset d√©fini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eea561e",
      "metadata": {
        "id": "3eea561e"
      },
      "source": [
        "## 7. üß† Architecture du Mod√®le CNN (avec SE Blocks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3b10cd35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b10cd35",
        "outputId": "befe1d6f-1f1c-4185-926a-015399ea6fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Mod√®le cr√©√© avec SE Blocks: 1,321,384 param√®tres\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ NOUVEAU: Squeeze-and-Excitation Block pour am√©liorer l'attention sur les features\n",
        "class SEBlock(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation Block - am√©liore la qualit√© des features.\"\"\"\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"Bloc convolutif avec BatchNorm, ReLU et SE Block optionnel.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, use_se=True, reduction=16):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.se = SEBlock(out_channels, reduction) if use_se else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.se(x)  # ‚úÖ Attention par SE Block\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FaceEmotionCNN(nn.Module):\n",
        "    \"\"\"CNN am√©lior√© avec SE Blocks pour la reconnaissance d'√©motions.\"\"\"\n",
        "    def __init__(self, num_classes=8, in_channels=3, input_size=75):\n",
        "        super(FaceEmotionCNN, self).__init__()\n",
        "\n",
        "        # ‚úÖ Blocs avec SE attention\n",
        "        self.block1 = ConvBlock(in_channels, 32, use_se=True, reduction=8)   # 75 -> 37\n",
        "        self.block2 = ConvBlock(32, 64, use_se=True, reduction=8)            # 37 -> 18\n",
        "        self.block3 = ConvBlock(64, 128, use_se=True, reduction=16)          # 18 -> 9\n",
        "        self.block4 = ConvBlock(128, 256, use_se=True, reduction=16)         # 9 -> 4\n",
        "\n",
        "        # Classifier avec Global Average Pooling\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)  # ‚úÖ Rend le mod√®le ind√©pendant de la taille\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),  # ‚úÖ R√©duit pour √©viter l'underfitting\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def create_model(dataset='affectnet', num_classes=8):\n",
        "    if dataset == 'affectnet':\n",
        "        return FaceEmotionCNN(num_classes=num_classes, in_channels=config.IN_CHANNELS, input_size=config.INPUT_SIZE)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset}\")\n",
        "\n",
        "# Cr√©er et afficher le mod√®le\n",
        "model = create_model(dataset='affectnet', num_classes=config.NUM_CLASSES)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"üß† Mod√®le cr√©√© avec SE Blocks: {total_params:,} param√®tres\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a29893",
      "metadata": {
        "id": "91a29893"
      },
      "source": [
        "## 8. üîß Utilitaires d'Entra√Ænement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8c85c195",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c85c195",
        "outputId": "3178b421-0926-4df1-9df7-1ce2d1673894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Utilitaires d√©finis (avec support AMP)\n"
          ]
        }
      ],
      "source": [
        "class AverageMeter:\n",
        "    \"\"\"Suit les valeurs moyennes.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device, per_class=False, use_amp=False):\n",
        "    \"\"\"Validation avec m√©triques optionnelles par classe et support AMP.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    loss_meter = AverageMeter()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    if per_class:\n",
        "        class_correct = defaultdict(int)\n",
        "        class_total = defaultdict(int)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "            # ‚ö° Mixed Precision pour la validation aussi\n",
        "            with torch.amp.autocast('cuda', enabled=use_amp):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            loss_meter.update(loss.item(), inputs.size(0))\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if per_class:\n",
        "                for pred, label in zip(predicted, labels):\n",
        "                    class_total[label.item()] += 1\n",
        "                    if pred == label:\n",
        "                        class_correct[label.item()] += 1\n",
        "\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    if per_class:\n",
        "        emotions = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral', 'Contempt']\n",
        "        print(\"\\n  üìä Pr√©cision par classe:\")\n",
        "        for i, emo in enumerate(emotions):\n",
        "            if class_total[i] > 0:\n",
        "                acc = 100.0 * class_correct[i] / class_total[i]\n",
        "                print(f\"    {emo:10s}: {acc:5.1f}% ({class_correct[i]}/{class_total[i]})\")\n",
        "\n",
        "    return loss_meter.avg, accuracy\n",
        "\n",
        "print(\"‚úÖ Utilitaires d√©finis (avec support AMP)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36818018",
      "metadata": {
        "id": "36818018"
      },
      "source": [
        "## 9. üìÇ Chargement des Donn√©es"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eb74ff1",
      "metadata": {
        "id": "3eb74ff1"
      },
      "source": [
        "### Configuration des Datasets √† Utiliser\n",
        "\n",
        "Choisissez les datasets que vous voulez utiliser pour l'entra√Ænement. Le notebook t√©l√©chargera automatiquement les datasets s√©lectionn√©s via `kagglehub`.\n",
        "\n",
        "**Datasets disponibles:**\n",
        "- `affectnet`: Dataset principal (8 classes, ~50k images, RGB)\n",
        "- `fer2013`: Dataset classique (7 classes, ~35k images, grayscale 48x48)\n",
        "- `ferplus`: FER2013 avec labels corrig√©s par Microsoft (8 classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b4157b08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4157b08",
        "outputId": "4a44a356-8231-4440-a2a0-3ba2c3f815d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ kagglehub install√©.\n",
            "\n",
            "============================================================\n",
            "üìã CONFIGURATION DES DATASETS\n",
            "============================================================\n",
            "  Datasets s√©lectionn√©s: ['affectnet', 'ferplus']\n",
            "  Mode: combined\n",
            "  ‚ÑπÔ∏è FER+ = FER2013 avec labels corrig√©s par Microsoft (meilleure qualit√©)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Installation de kagglehub\n",
        "!pip install -q kagglehub\n",
        "print(\"‚úÖ kagglehub install√©.\")\n",
        "\n",
        "# ===============================================================================\n",
        "# üéØ CONFIGURATION DES DATASETS √Ä UTILISER\n",
        "# ===============================================================================\n",
        "#\n",
        "# ‚ö†Ô∏è IMPORTANT: FER2013 et FER+ utilisent les M√äMES images mais avec des labels diff√©rents!\n",
        "#    - fer2013: Labels originaux (7 classes, plus bruit√©s)\n",
        "#    - ferplus: Labels corrig√©s par Microsoft (8 classes, 10 annotateurs)\n",
        "#\n",
        "#    ‚Üí Ne pas utiliser les deux en m√™me temps (duplicatas)!\n",
        "#    ‚Üí Pr√©f√©rer FER+ pour une meilleure qualit√©\n",
        "#\n",
        "\n",
        "DATASETS_TO_USE = [\n",
        "    'affectnet',    # ‚úÖ Dataset principal (8 classes, ~50k images)\n",
        "    # 'fer2013',    # ‚ùå Remplac√© par FER+ (m√™mes images, labels moins bons)\n",
        "    'ferplus',      # ‚úÖ FER+ avec labels corrig√©s (8 classes, ~35k images)\n",
        "]\n",
        "\n",
        "# Mode: 'combined' pour fusionner tous les datasets, 'single' pour utiliser le premier uniquement\n",
        "DATASET_MODE = 'combined' if len(DATASETS_TO_USE) > 1 else 'single'\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üìã CONFIGURATION DES DATASETS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Datasets s√©lectionn√©s: {DATASETS_TO_USE}\")\n",
        "print(f\"  Mode: {DATASET_MODE}\")\n",
        "if 'ferplus' in DATASETS_TO_USE:\n",
        "    print(f\"  ‚ÑπÔ∏è FER+ = FER2013 avec labels corrig√©s par Microsoft (meilleure qualit√©)\")\n",
        "if 'fer2013' in DATASETS_TO_USE and 'ferplus' in DATASETS_TO_USE:\n",
        "    print(f\"  ‚ö†Ô∏è ATTENTION: fer2013 et ferplus utilisent les m√™mes images!\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06138371",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06138371",
        "outputId": "e79d8b77-5b37-423a-c93a-8072dec0a288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• T√©l√©chargement des datasets...\n",
            "\n",
            "üì¶ [1/3] AffectNet...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/dollyprajapati182/balanced-affectnet?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 273M/273M [00:07<00:00, 39.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úì T√©l√©charg√©: /root/.cache/kagglehub/datasets/dollyprajapati182/balanced-affectnet/versions/1\n",
            "\n",
            "üì¶ [3/3] FER+ (FER2013 avec labels Microsoft)...\n",
            "  üì• T√©l√©chargement de fer2013.csv...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/deadskull7/fer2013?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96.6M/96.6M [00:03<00:00, 33.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úì fer2013.csv t√©l√©charg√©: /root/.cache/kagglehub/datasets/deadskull7/fer2013/versions/1\n",
            "\n",
            "üîß Configuration de FER+ (premi√®re utilisation)...\n",
            "  üì• T√©l√©chargement de fer2013new.csv depuis GitHub...\n",
            "  ‚úì T√©l√©charg√©: /root/.cache/kagglehub/datasets/deadskull7/fer2013/versions/ferplus_generated/fer2013new.csv\n",
            "  üñºÔ∏è G√©n√©ration des images depuis fer2013.csv...\n",
            "    Progression: 0 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2955467183.py:167: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  img = Image.fromarray(img_array, mode='L')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Progression: 5000 images...\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# ===============================================================================\n",
        "# üì• T√âL√âCHARGEMENT DES DATASETS\n",
        "# ===============================================================================\n",
        "\n",
        "# IDs Kaggle pour chaque dataset\n",
        "KAGGLE_IDS = {\n",
        "    'affectnet': 'dollyprajapati182/balanced-affectnet',\n",
        "    'fer2013': 'msambare/fer2013',              # Version en dossiers (images directement)\n",
        "    'fer2013_csv': 'deadskull7/fer2013',        # Version CSV originale (pour FER+)\n",
        "}\n",
        "\n",
        "dataset_paths = {}\n",
        "\n",
        "print(\"üì• T√©l√©chargement des datasets...\\n\")\n",
        "\n",
        "# ===============================================================================\n",
        "# 1. T√©l√©charger AffectNet\n",
        "# ===============================================================================\n",
        "if 'affectnet' in DATASETS_TO_USE:\n",
        "    print(f\"üì¶ [1/3] AffectNet...\")\n",
        "    try:\n",
        "        path = kagglehub.dataset_download(KAGGLE_IDS['affectnet'])\n",
        "        dataset_paths['affectnet'] = str(path)\n",
        "        print(f\"  ‚úì T√©l√©charg√©: {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Erreur: {e}\")\n",
        "        dataset_paths['affectnet'] = None\n",
        "\n",
        "# ===============================================================================\n",
        "# 2. T√©l√©charger FER2013 (version dossiers)\n",
        "# ===============================================================================\n",
        "if 'fer2013' in DATASETS_TO_USE:\n",
        "    print(f\"\\nüì¶ [2/3] FER2013...\")\n",
        "    try:\n",
        "        path = kagglehub.dataset_download(KAGGLE_IDS['fer2013'])\n",
        "        dataset_paths['fer2013'] = str(path)\n",
        "        print(f\"  ‚úì T√©l√©charg√©: {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Erreur: {e}\")\n",
        "        dataset_paths['fer2013'] = None\n",
        "\n",
        "# ===============================================================================\n",
        "# 3. Configurer FER+ (t√©l√©charge CSV + g√©n√®re images)\n",
        "# ===============================================================================\n",
        "if 'ferplus' in DATASETS_TO_USE:\n",
        "    print(f\"\\nüì¶ [3/3] FER+ (FER2013 avec labels Microsoft)...\")\n",
        "\n",
        "    # FER+ n√©cessite le CSV original de FER2013\n",
        "    print(f\"  üì• T√©l√©chargement de fer2013.csv...\")\n",
        "    try:\n",
        "        fer2013_csv_path = kagglehub.dataset_download(KAGGLE_IDS['fer2013_csv'])\n",
        "        print(f\"  ‚úì fer2013.csv t√©l√©charg√©: {fer2013_csv_path}\")\n",
        "\n",
        "        # Configurer FER+ (t√©l√©charge labels + g√©n√®re images)\n",
        "        ferplus_path = setup_ferplus_dataset(fer2013_csv_path)\n",
        "\n",
        "        if ferplus_path:\n",
        "            dataset_paths['ferplus'] = ferplus_path\n",
        "            print(f\"  ‚úì FER+ configur√©: {ferplus_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è FER+ non configur√© (voir erreurs ci-dessus)\")\n",
        "            dataset_paths['ferplus'] = None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Erreur: {e}\")\n",
        "        dataset_paths['ferplus'] = None\n",
        "\n",
        "# ===============================================================================\n",
        "# Mise √† jour de la config\n",
        "# ===============================================================================\n",
        "valid_paths = {k: v for k, v in dataset_paths.items() if v is not None}\n",
        "\n",
        "if 'affectnet' in valid_paths:\n",
        "    config.DATASET_ROOT = valid_paths['affectnet']\n",
        "elif valid_paths:\n",
        "    config.DATASET_ROOT = list(valid_paths.values())[0]\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"‚úÖ DATASETS PR√äTS\")\n",
        "print(f\"{'='*60}\")\n",
        "for name, path in dataset_paths.items():\n",
        "    status = \"‚úì\" if path else \"‚úó\"\n",
        "    print(f\"  {status} {name}: {path if path else 'Non disponible'}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbc971fb",
      "metadata": {
        "id": "fbc971fb"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# üìÇ CHARGEMENT DES DONN√âES (MULTI-DATASET OU SINGLE)\n",
        "# ===============================================================================\n",
        "\n",
        "print(\"üìÇ Chargement des datasets...\")\n",
        "\n",
        "train_transform = get_train_transforms()\n",
        "val_transform = get_val_transforms()\n",
        "\n",
        "# Filtrer les paths valides\n",
        "valid_dataset_paths = {k: v for k, v in dataset_paths.items() if v is not None}\n",
        "\n",
        "# ===============================================================================\n",
        "# MODE MULTI-DATASET (combin√©) ou SINGLE-DATASET\n",
        "# ===============================================================================\n",
        "\n",
        "if DATASET_MODE == 'combined' and len(valid_dataset_paths) > 1:\n",
        "    print(f\"\\nüîÄ Mode MULTI-DATASET activ√©!\")\n",
        "    print(f\"   Datasets: {list(valid_dataset_paths.keys())}\")\n",
        "\n",
        "    # Utiliser le dataset combin√©\n",
        "    train_dataset = CombinedEmotionDataset(\n",
        "        datasets_config=valid_dataset_paths,\n",
        "        split='train',\n",
        "        transform=train_transform,\n",
        "        use_albumentations=HAS_ALBUMENTATIONS,\n",
        "        target_size=config.INPUT_SIZE\n",
        "    )\n",
        "\n",
        "    val_dataset = CombinedEmotionDataset(\n",
        "        datasets_config=valid_dataset_paths,\n",
        "        split='val',\n",
        "        transform=val_transform,\n",
        "        use_albumentations=HAS_ALBUMENTATIONS,\n",
        "        target_size=config.INPUT_SIZE\n",
        "    )\n",
        "\n",
        "    # Mettre √† jour la config avec les classes unifi√©es (8 classes)\n",
        "    config.NUM_CLASSES = CombinedEmotionDataset.NUM_CLASSES\n",
        "\n",
        "else:\n",
        "    print(f\"\\nüìÅ Mode SINGLE-DATASET\")\n",
        "\n",
        "    # Utiliser le premier dataset disponible\n",
        "    dataset_name = list(valid_dataset_paths.keys())[0]\n",
        "    root_path = valid_dataset_paths[dataset_name]\n",
        "    print(f\"   Dataset: {dataset_name}\")\n",
        "\n",
        "    if dataset_name == 'affectnet':\n",
        "        train_dataset = BalancedAffectNetDataset(\n",
        "            root_dir=root_path,\n",
        "            split='train',\n",
        "            transform=train_transform,\n",
        "            use_albumentations=HAS_ALBUMENTATIONS\n",
        "        )\n",
        "        val_dataset = BalancedAffectNetDataset(\n",
        "            root_dir=root_path,\n",
        "            split='val',\n",
        "            transform=val_transform,\n",
        "            use_albumentations=HAS_ALBUMENTATIONS\n",
        "        )\n",
        "        config.NUM_CLASSES = 8\n",
        "\n",
        "    elif dataset_name == 'fer2013':\n",
        "        train_dataset = FER2013Dataset(\n",
        "            root_dir=root_path,\n",
        "            split='train',\n",
        "            transform=train_transform,\n",
        "            use_albumentations=HAS_ALBUMENTATIONS,\n",
        "            target_size=config.INPUT_SIZE\n",
        "        )\n",
        "        val_dataset = FER2013Dataset(\n",
        "            root_dir=root_path,\n",
        "            split='val',\n",
        "            transform=val_transform,\n",
        "            use_albumentations=HAS_ALBUMENTATIONS,\n",
        "            target_size=config.INPUT_SIZE\n",
        "        )\n",
        "        config.NUM_CLASSES = 7  # FER2013 n'a pas Contempt\n",
        "\n",
        "    elif dataset_name == 'ferplus':\n",
        "        train_dataset = FERPlusDataset(\n",
        "            root_dir=root_path,\n",
        "            split='train',\n",
        "            transform=train_transform,\n",
        "            use_albumentations=HAS_ALBUMENTATIONS,\n",
        "            target_size=config.INPUT_SIZE\n",
        "        )\n",
        "        val_dataset = FERPlusDataset(\n",
        "            root_dir=root_path,\n",
        "            split='val',\n",
        "            transform=val_transform,\n",
        "            use_albumentations=HAS_ALBUMENTATIONS,\n",
        "            target_size=config.INPUT_SIZE\n",
        "        )\n",
        "        config.NUM_CLASSES = 8\n",
        "\n",
        "# ===============================================================================\n",
        "# CALCUL DES POIDS DE CLASSES (adaptatif)\n",
        "# ===============================================================================\n",
        "\n",
        "def get_class_weights_adaptive(dataset, max_weight=5.0):\n",
        "    \"\"\"Calcule les poids pour √©quilibrer les classes (compatible tous datasets).\"\"\"\n",
        "    counts = dataset.get_class_distribution()\n",
        "    num_classes = len(counts)\n",
        "    counts = np.maximum(counts, 1)\n",
        "\n",
        "    weights = 1.0 / counts\n",
        "    weights = weights / weights.sum() * num_classes\n",
        "    weights = np.clip(weights, 0.3, max_weight)\n",
        "    weights = weights / weights.sum() * num_classes\n",
        "\n",
        "    # R√©cup√©rer les noms d'√©motions selon le type de dataset\n",
        "    if hasattr(dataset, 'IDX_TO_EMOTION'):\n",
        "        idx_to_emotion = dataset.IDX_TO_EMOTION\n",
        "    elif hasattr(dataset, 'UNIFIED_CLASSES'):\n",
        "        idx_to_emotion = {i: c for i, c in enumerate(dataset.UNIFIED_CLASSES)}\n",
        "    else:\n",
        "        idx_to_emotion = {i: f\"Class_{i}\" for i in range(num_classes)}\n",
        "\n",
        "    print(f\"\\nüìä Poids des classes ({num_classes} classes):\")\n",
        "    for i, (count, weight) in enumerate(zip(counts, weights)):\n",
        "        emotion = idx_to_emotion.get(i, f\"Class_{i}\")\n",
        "        print(f\"    {emotion:10s}: {count:6d} samples, poids: {weight:.3f}\")\n",
        "\n",
        "    return torch.FloatTensor(weights)\n",
        "\n",
        "class_weights = get_class_weights_adaptive(train_dataset, max_weight=config.MAX_CLASS_WEIGHT).to(config.DEVICE)\n",
        "\n",
        "# ===============================================================================\n",
        "# DATALOADERS OPTIMIS√âS\n",
        "# ===============================================================================\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=config.NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        "    prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None,\n",
        "    persistent_workers=config.PERSISTENT_WORKERS if config.NUM_WORKERS > 0 else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.BATCH_SIZE * 2,\n",
        "    shuffle=False,\n",
        "    num_workers=config.NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None,\n",
        "    persistent_workers=config.PERSISTENT_WORKERS if config.NUM_WORKERS > 0 else False\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"‚úÖ DONN√âES CHARG√âES\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Train: {len(train_dataset):,} samples ({len(train_loader)} batches)\")\n",
        "print(f\"  Val:   {len(val_dataset):,} samples ({len(val_loader)} batches)\")\n",
        "print(f\"  Classes: {config.NUM_CLASSES}\")\n",
        "print(f\"  Input size: {config.INPUT_SIZE}x{config.INPUT_SIZE}\")\n",
        "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"  ‚ö° Workers: {config.NUM_WORKERS}, Prefetch: {config.PREFETCH_FACTOR}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed802a6",
      "metadata": {
        "id": "9ed802a6"
      },
      "source": [
        "## 10. üëÄ Visualisation d'√âchantillons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f2cc43f",
      "metadata": {
        "id": "6f2cc43f"
      },
      "outputs": [],
      "source": [
        "# Visualiser quelques images du dataset (compatible multi-dataset)\n",
        "def show_samples(dataset, n_samples=8):\n",
        "    \"\"\"Affiche des √©chantillons du dataset (compatible avec tous les datasets).\"\"\"\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # D√©normalisation ImageNet\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    # R√©cup√©rer le mapping idx -> emotion selon le type de dataset\n",
        "    if hasattr(dataset, 'IDX_TO_EMOTION'):\n",
        "        idx_to_emotion = dataset.IDX_TO_EMOTION\n",
        "    elif hasattr(dataset, 'UNIFIED_CLASSES'):\n",
        "        idx_to_emotion = {i: c for i, c in enumerate(dataset.UNIFIED_CLASSES)}\n",
        "    else:\n",
        "        idx_to_emotion = {i: f\"Class_{i}\" for i in range(config.NUM_CLASSES)}\n",
        "\n",
        "    indices = random.sample(range(len(dataset)), min(n_samples, len(dataset)))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, label = dataset[idx]\n",
        "\n",
        "        # Convertir tensor en numpy et d√©normaliser\n",
        "        if isinstance(img, torch.Tensor):\n",
        "            img_np = img.numpy().transpose(1, 2, 0)\n",
        "        else:\n",
        "            img_np = img.transpose(1, 2, 0) if img.shape[0] == 3 else img\n",
        "\n",
        "        img_np = img_np * std + mean\n",
        "        img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "        emotion = idx_to_emotion.get(label, f\"Class_{label}\")\n",
        "\n",
        "        # Afficher la source si disponible (multi-dataset)\n",
        "        if hasattr(dataset, 'sources') and idx < len(dataset.sources):\n",
        "            source = dataset.sources[idx]\n",
        "            title = f\"{emotion}\\n({source})\"\n",
        "        else:\n",
        "            title = emotion\n",
        "\n",
        "        axes[i].imshow(img_np)\n",
        "        axes[i].set_title(title, fontsize=10)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    # Titre selon le mode\n",
        "    if DATASET_MODE == 'combined' and len(valid_dataset_paths) > 1:\n",
        "        dataset_type = f\"Multi-Dataset ({', '.join(valid_dataset_paths.keys())})\"\n",
        "    else:\n",
        "        dataset_type = list(valid_dataset_paths.keys())[0] if valid_dataset_paths else \"Unknown\"\n",
        "\n",
        "    plt.suptitle(f'√âchantillons - {dataset_type}', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_samples(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc8f6b5b",
      "metadata": {
        "id": "bc8f6b5b"
      },
      "source": [
        "## 11. üöÄ Configuration de l'Entra√Ænement (avec SWA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc21e438",
      "metadata": {
        "id": "bc21e438"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# üöÄ CONFIGURATION DE L'ENTRA√éNEMENT\n",
        "# ===============================================================================\n",
        "\n",
        "# Mod√®le\n",
        "model = create_model(dataset='affectnet', num_classes=config.NUM_CLASSES).to(config.DEVICE)\n",
        "\n",
        "# Compilation du mod√®le (PyTorch 2.0+)\n",
        "if config.USE_COMPILE and hasattr(torch, 'compile'):\n",
        "    try:\n",
        "        # ‚ö° Mode max-autotune: plus lent au d√©but mais plus rapide apr√®s\n",
        "        model = torch.compile(model, mode=config.COMPILE_MODE)\n",
        "        print(f\"‚ö° Mod√®le compil√© avec torch.compile(mode='{config.COMPILE_MODE}')\")\n",
        "        print(\"   Note: Les premi√®res √©poques seront plus lentes (compilation)\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è torch.compile non disponible: {e}\")\n",
        "\n",
        "# Fonction de perte\n",
        "if config.USE_FOCAL_LOSS:\n",
        "    criterion = FocalLoss(\n",
        "        gamma=config.FOCAL_GAMMA,\n",
        "        alpha=class_weights,\n",
        "        label_smoothing=config.LABEL_SMOOTHING if config.USE_LABEL_SMOOTHING else 0.0\n",
        "    )\n",
        "    print(f\"‚úì Focal Loss (gamma={config.FOCAL_GAMMA})\")\n",
        "elif config.USE_LABEL_SMOOTHING:\n",
        "    criterion = LabelSmoothingCrossEntropy(smoothing=config.LABEL_SMOOTHING)\n",
        "    print(f\"‚úì Label Smoothing (smoothing={config.LABEL_SMOOTHING})\")\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "val_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimiseur\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config.LEARNING_RATE,\n",
        "    weight_decay=config.WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "# Scheduler OneCycleLR\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=config.LEARNING_RATE * 10,\n",
        "    epochs=config.EPOCHS,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.3,\n",
        "    anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "# GradScaler pour Mixed Precision\n",
        "scaler = torch.amp.GradScaler('cuda', enabled=config.USE_AMP)\n",
        "\n",
        "# Affichage de la configuration\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"üìã Configuration d'entra√Ænement:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Dataset: Balanced AffectNet (75x75 RGB, 8 classes)\")\n",
        "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {config.LEARNING_RATE} -> {config.LEARNING_RATE * 10}\")\n",
        "print(f\"  Epochs: {config.EPOCHS}, Patience: {config.PATIENCE}\")\n",
        "print(f\"  Mixup: {config.USE_MIXUP} (alpha={config.MIXUP_ALPHA})\")\n",
        "print(f\"  ‚ö° Mixed Precision (AMP): {config.USE_AMP}\")\n",
        "print(f\"  ‚ö° torch.compile: {config.COMPILE_MODE}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dedaa5fa",
      "metadata": {
        "id": "dedaa5fa"
      },
      "source": [
        "## 12. üèãÔ∏è Boucle d'Entra√Ænement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2378eb2",
      "metadata": {
        "id": "f2378eb2"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# üèãÔ∏è BOUCLE D'ENTRA√éNEMENT UNIFI√âE\n",
        "# ===============================================================================\n",
        "\n",
        "import gc\n",
        "\n",
        "# Variables de suivi\n",
        "best_val_acc = 0.0\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "best_epoch = 0\n",
        "\n",
        "# Historique pour les graphiques\n",
        "history = {\n",
        "    'train_loss': [], 'train_acc': [],\n",
        "    'val_loss': [], 'val_acc': [],\n",
        "    'lr': [], 'epoch_time': [], 'gpu_memory': []\n",
        "}\n",
        "\n",
        "# SWA Setup (optionnel)\n",
        "swa_model = None\n",
        "swa_scheduler = None\n",
        "if config.USE_SWA and not config.USE_COMPILE:\n",
        "    from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "    swa_model = AveragedModel(model)\n",
        "    swa_scheduler = SWALR(optimizer, swa_lr=config.SWA_LR)\n",
        "    print(f\"‚úÖ SWA activ√© (d√©marre √† l'√©poque {config.SWA_START_EPOCH})\")\n",
        "elif config.USE_SWA and config.USE_COMPILE:\n",
        "    print(\"‚ö†Ô∏è SWA d√©sactiv√© car torch.compile est activ√© (incompatible)\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ D√âMARRAGE DE L'ENTRA√éNEMENT\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Mixed Precision: {config.USE_AMP}\")\n",
        "print(f\"Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"Workers: {config.NUM_WORKERS}\")\n",
        "print(f\"Epochs: {config.EPOCHS}, Patience: {config.PATIENCE}\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "for epoch in range(config.EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "    model.train()\n",
        "\n",
        "    loss_meter = AverageMeter()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(config.DEVICE, non_blocking=True), labels.to(config.DEVICE, non_blocking=True)\n",
        "\n",
        "        # Mixup uniquement (CutMix d√©sactiv√© car baisse les performances)\n",
        "        use_mixup = config.USE_MIXUP and random.random() > 0.5\n",
        "\n",
        "        if use_mixup:\n",
        "            inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, config.MIXUP_ALPHA)\n",
        "\n",
        "        # Mixed Precision Forward Pass\n",
        "        with torch.amp.autocast('cuda', enabled=config.USE_AMP):\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            if use_mixup:\n",
        "                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            loss = loss / config.ACCUMULATION_STEPS\n",
        "\n",
        "        # Backward avec GradScaler\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Gradient accumulation\n",
        "        if (batch_idx + 1) % config.ACCUMULATION_STEPS == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Scheduler step (pas pendant SWA)\n",
        "            if swa_model is None or epoch < config.SWA_START_EPOCH:\n",
        "                scheduler.step()\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # M√©triques\n",
        "        loss_meter.update(loss.item() * config.ACCUMULATION_STEPS, inputs.size(0))\n",
        "\n",
        "        if not use_mixup:\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_acc = 100.0 * correct / max(total, 1)\n",
        "\n",
        "    # Mise √† jour SWA apr√®s SWA_START_EPOCH\n",
        "    if swa_model is not None and epoch >= config.SWA_START_EPOCH:\n",
        "        swa_model.update_parameters(model)\n",
        "        swa_scheduler.step()\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_acc = validate(model, val_loader, val_criterion, config.DEVICE,\n",
        "                                 per_class=(epoch % 10 == 0), use_amp=config.USE_AMP)\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    # Suivi m√©moire GPU\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
        "        gpu_mem_max = torch.cuda.max_memory_allocated() / 1024**3\n",
        "    else:\n",
        "        gpu_mem = 0\n",
        "        gpu_mem_max = 0\n",
        "\n",
        "    # Sauvegarder historique\n",
        "    history['train_loss'].append(loss_meter.avg)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['lr'].append(current_lr)\n",
        "    history['epoch_time'].append(epoch_time)\n",
        "    history['gpu_memory'].append(gpu_mem_max)\n",
        "\n",
        "    swa_status = \" [SWA]\" if swa_model is not None and epoch >= config.SWA_START_EPOCH else \"\"\n",
        "    print(f\"Epoch {epoch+1:3d}/{config.EPOCHS} | \"\n",
        "          f\"Loss: {loss_meter.avg:.4f} | Acc: {train_acc:.1f}% | \"\n",
        "          f\"Val: {val_acc:.1f}% | LR: {current_lr:.6f} | \"\n",
        "          f\"Time: {epoch_time:.1f}s | GPU: {gpu_mem:.1f}/{gpu_mem_max:.1f}GB{swa_status}\")\n",
        "\n",
        "    # Sauvegarder le meilleur mod√®le\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch + 1\n",
        "        patience_counter = 0\n",
        "\n",
        "        # R√©cup√©rer les poids du mod√®le (g√©rer torch.compile)\n",
        "        model_to_save = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model_to_save.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scaler_state_dict': scaler.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'history': history,\n",
        "            'config': {\n",
        "                'num_classes': config.NUM_CLASSES,\n",
        "                'in_channels': config.IN_CHANNELS,\n",
        "                'input_size': config.INPUT_SIZE,\n",
        "                'dataset': 'affectnet',\n",
        "            }\n",
        "        }, config.SAVE_PATH)\n",
        "        print(f\"  ‚úÖ [BEST] Nouveau meilleur mod√®le! (Val Acc: {val_acc:.2f}%)\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= config.PATIENCE:\n",
        "            print(f\"\\n‚èπÔ∏è Early stopping apr√®s {epoch+1} √©poques!\")\n",
        "            break\n",
        "\n",
        "# Nettoyage m√©moire\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "avg_epoch_time = np.mean(history['epoch_time'])\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"‚úÖ ENTRA√éNEMENT TERMIN√â!\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Temps total: {elapsed/60:.1f} minutes\")\n",
        "print(f\"Temps moyen par epoch: {avg_epoch_time:.1f} secondes\")\n",
        "print(f\"Meilleure √©poque: {best_epoch}\")\n",
        "print(f\"Meilleure pr√©cision validation: {best_val_acc:.2f}%\")\n",
        "print(f\"Meilleure loss validation: {best_val_loss:.4f}\")\n",
        "print(f\"M√©moire GPU max: {max(history['gpu_memory']):.2f} GB\")\n",
        "print(f\"Mod√®le sauv√©: {config.SAVE_PATH}\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13f42c0b",
      "metadata": {
        "id": "13f42c0b"
      },
      "source": [
        "## 13. üìà Visualisation des R√©sultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8e82ef0",
      "metadata": {
        "id": "e8e82ef0"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# üìà VISUALISATION DES R√âSULTATS D'ENTRA√éNEMENT\n",
        "# ===============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "\n",
        "# 1. Loss\n",
        "axes[0, 0].plot(history['train_loss'], label='Train', color='blue')\n",
        "axes[0, 0].plot(history['val_loss'], label='Validation', color='orange')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('üìâ Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Accuracy\n",
        "axes[0, 1].plot(history['train_acc'], label='Train', color='blue')\n",
        "axes[0, 1].plot(history['val_acc'], label='Validation', color='orange')\n",
        "axes[0, 1].axhline(y=best_val_acc, color='green', linestyle='--', label=f'Best: {best_val_acc:.1f}%')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy (%)')\n",
        "axes[0, 1].set_title('üìä Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Learning Rate\n",
        "axes[0, 2].plot(history['lr'], color='green')\n",
        "axes[0, 2].set_xlabel('Epoch')\n",
        "axes[0, 2].set_ylabel('Learning Rate')\n",
        "axes[0, 2].set_title('üìà Learning Rate (OneCycleLR)')\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Temps par epoch\n",
        "axes[1, 0].plot(history['epoch_time'], color='purple')\n",
        "axes[1, 0].axhline(y=np.mean(history['epoch_time']), color='red', linestyle='--',\n",
        "                   label=f'Moyenne: {np.mean(history[\"epoch_time\"]):.1f}s')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Temps (s)')\n",
        "axes[1, 0].set_title('‚è±Ô∏è Temps par Epoch')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 5. M√©moire GPU\n",
        "axes[1, 1].plot(history['gpu_memory'], color='red')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('M√©moire (GB)')\n",
        "axes[1, 1].set_title('üéÆ M√©moire GPU Max')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 6. R√©sum√©\n",
        "axes[1, 2].axis('off')\n",
        "summary_text = f\"\"\"\n",
        "üìã R√âSUM√â DE L'ENTRA√éNEMENT\n",
        "\n",
        "Meilleure accuracy: {best_val_acc:.2f}%\n",
        "Meilleure √©poque: {best_epoch}\n",
        "\n",
        "‚öôÔ∏è Configuration:\n",
        "‚Ä¢ Batch Size: {config.BATCH_SIZE}\n",
        "‚Ä¢ Epochs: {len(history['train_loss'])}\n",
        "‚Ä¢ Mixed Precision: {config.USE_AMP}\n",
        "‚Ä¢ torch.compile: {config.USE_COMPILE}\n",
        "‚Ä¢ Mixup: {config.USE_MIXUP} (Œ±={config.MIXUP_ALPHA})\n",
        "‚Ä¢ CutMix: {config.USE_CUTMIX}\n",
        "‚Ä¢ SE Blocks: {config.USE_SE_BLOCKS}\n",
        "\n",
        "‚è±Ô∏è Performance:\n",
        "‚Ä¢ Temps moyen/epoch: {np.mean(history['epoch_time']):.1f}s\n",
        "‚Ä¢ M√©moire GPU max: {max(history['gpu_memory']):.2f} GB\n",
        "\"\"\"\n",
        "axes[1, 2].text(0.05, 0.5, summary_text, fontsize=10, verticalalignment='center',\n",
        "                fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "\n",
        "plt.suptitle('üìà M√©triques d\\'Entra√Ænement', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Graphiques sauvegard√©s dans 'training_curves.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "587e32d2",
      "metadata": {
        "id": "587e32d2"
      },
      "source": [
        "## 14. üîç √âvaluation Finale (avec TTA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca470346",
      "metadata": {
        "id": "ca470346"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# üîç √âVALUATION FINALE AVEC TTA (Test-Time Augmentation)\n",
        "# ===============================================================================\n",
        "# ‚ö†Ô∏è Si OOM: Red√©marrer le kernel (Runtime > Restart) avant de lancer cette cellule\n",
        "# Le cache CUDA Graphs de torch.compile ne peut pas √™tre lib√©r√© autrement.\n",
        "\n",
        "import gc\n",
        "\n",
        "# ‚ö° NETTOYAGE M√âMOIRE GPU AGRESSIF\n",
        "print(\"üßπ Nettoyage m√©moire GPU...\")\n",
        "\n",
        "# Supprimer tous les mod√®les et tenseurs possibles\n",
        "for var_name in ['model', 'swa_model', 'optimizer', 'scheduler', 'scaler', 'criterion']:\n",
        "    if var_name in dir():\n",
        "        try:\n",
        "            exec(f'del {var_name}')\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Forcer le nettoyage\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()\n",
        "gc.collect()\n",
        "\n",
        "# Afficher m√©moire disponible\n",
        "if torch.cuda.is_available():\n",
        "    gpu_free = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
        "    gpu_reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "    print(f\"   M√©moire GPU libre: {gpu_free / 1024**3:.2f} GB\")\n",
        "    print(f\"   M√©moire r√©serv√©e PyTorch: {gpu_reserved:.2f} GB\")\n",
        "    if gpu_free < 2 * 1024**3:  # Moins de 2GB libre\n",
        "        print(\"   ‚ö†Ô∏è Peu de m√©moire libre - utilisation de petits batches\")\n",
        "\n",
        "\n",
        "def validate_with_tta(model, val_loader, criterion, device, n_augmentations=5, use_amp=False):\n",
        "    \"\"\"Validation avec Test-Time Augmentation - moyenne sur plusieurs augmentations.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_sum = 0\n",
        "\n",
        "    class_correct = defaultdict(int)\n",
        "    class_total = defaultdict(int)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            batch_size = inputs.size(0)\n",
        "\n",
        "            # Collecter les pr√©dictions de plusieurs augmentations\n",
        "            all_outputs = []\n",
        "\n",
        "            with torch.amp.autocast('cuda', enabled=use_amp):\n",
        "                # 1. Original\n",
        "                all_outputs.append(model(inputs))\n",
        "\n",
        "                # 2. Flip horizontal\n",
        "                all_outputs.append(model(torch.flip(inputs, dims=[3])))\n",
        "\n",
        "                # 3-5. L√©g√®res variations de luminosit√©\n",
        "                if n_augmentations >= 3:\n",
        "                    all_outputs.append(model(inputs * 0.95))\n",
        "                if n_augmentations >= 4:\n",
        "                    all_outputs.append(model(inputs * 1.05))\n",
        "                if n_augmentations >= 5:\n",
        "                    all_outputs.append(model(torch.flip(inputs, dims=[3]) * 0.98))\n",
        "\n",
        "            # Moyenne des pr√©dictions (soft voting)\n",
        "            avg_outputs = torch.stack(all_outputs).mean(dim=0)\n",
        "\n",
        "            loss = criterion(avg_outputs, labels)\n",
        "            loss_sum += loss.item() * batch_size\n",
        "\n",
        "            _, predicted = avg_outputs.max(1)\n",
        "            total += batch_size\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            for pred, label in zip(predicted, labels):\n",
        "                class_total[label.item()] += 1\n",
        "                if pred == label:\n",
        "                    class_correct[label.item()] += 1\n",
        "\n",
        "            # Lib√©rer la m√©moire des outputs interm√©diaires\n",
        "            del all_outputs, avg_outputs, inputs, labels\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    accuracy = 100.0 * correct / total\n",
        "    avg_loss = loss_sum / total\n",
        "\n",
        "    emotions = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral', 'Contempt']\n",
        "    print(f\"\\n  üìä Pr√©cision par classe (TTA x{n_augmentations}):\")\n",
        "    for i, emo in enumerate(emotions):\n",
        "        if class_total[i] > 0:\n",
        "            acc = 100.0 * class_correct[i] / class_total[i]\n",
        "            print(f\"    {emo:10s}: {acc:5.1f}% ({class_correct[i]}/{class_total[i]})\")\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "# Charger le meilleur mod√®le\n",
        "print(\"\\nüì• Chargement du meilleur mod√®le...\")\n",
        "checkpoint = torch.load(config.SAVE_PATH, weights_only=False)\n",
        "\n",
        "# Cr√©er un nouveau mod√®le (sans compilation) pour charger les poids\n",
        "eval_model = create_model(dataset='affectnet', num_classes=config.NUM_CLASSES).to(config.DEVICE)\n",
        "eval_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# ‚ö° BATCH SIZE R√âDUIT pour √©viter OOM (le cache CUDA Graphs prend ~13GB)\n",
        "EVAL_BATCH_SIZE = 256  # Beaucoup plus petit pour laisser de la place\n",
        "print(f\"   ‚ö° Batch size √©valuation r√©duit: {EVAL_BATCH_SIZE} (au lieu de {config.BATCH_SIZE})\")\n",
        "\n",
        "eval_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=EVAL_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=config.NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Recr√©er le crit√®re de validation\n",
        "val_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üìä √âVALUATION STANDARD\")\n",
        "print(f\"{'='*60}\")\n",
        "val_loss, val_acc = validate(eval_model, eval_loader, val_criterion, config.DEVICE, per_class=True, use_amp=config.USE_AMP)\n",
        "print(f\"\\nüéØ R√©sultats standards:\")\n",
        "print(f\"   - Pr√©cision globale: {val_acc:.2f}%\")\n",
        "print(f\"   - Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Nettoyage avant TTA (qui utilise plus de m√©moire)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üìä √âVALUATION AVEC TTA (Test-Time Augmentation)\")\n",
        "print(f\"{'='*60}\")\n",
        "tta_loss, tta_acc = validate_with_tta(eval_model, eval_loader, val_criterion, config.DEVICE,\n",
        "                                       n_augmentations=5, use_amp=config.USE_AMP)\n",
        "print(f\"\\nüéØ R√©sultats avec TTA:\")\n",
        "print(f\"   - Pr√©cision globale: {tta_acc:.2f}%\")\n",
        "print(f\"   - Loss: {tta_loss:.4f}\")\n",
        "print(f\"   - Am√©lioration TTA: {tta_acc - val_acc:+.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "126d542b",
      "metadata": {
        "id": "126d542b"
      },
      "source": [
        "## 15. üíæ Sauvegarde du Mod√®le Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cfc6b5f",
      "metadata": {
        "id": "8cfc6b5f"
      },
      "outputs": [],
      "source": [
        "# Sauvegarder le mod√®le final (poids uniquement) - version l√©g√®re pour d√©ploiement\n",
        "torch.save({\n",
        "    'model_state_dict': eval_model.state_dict(),  # Utilise eval_model (le mod√®le charg√©)\n",
        "    'num_classes': config.NUM_CLASSES,\n",
        "    'in_channels': config.IN_CHANNELS,\n",
        "    'input_size': config.INPUT_SIZE,\n",
        "    'dataset': 'affectnet',\n",
        "    'best_val_acc': checkpoint['val_acc'],  # Utilise la valeur du checkpoint\n",
        "}, 'emotion_model.pth')\n",
        "\n",
        "print(\"‚úÖ Mod√®le sauvegard√© dans 'emotion_model.pth'\")\n",
        "print(f\"   Taille: {os.path.getsize('emotion_model.pth') / 1024 / 1024:.2f} MB\")\n",
        "print(f\"   Best Val Acc: {checkpoint['val_acc']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f580e241",
      "metadata": {
        "id": "f580e241"
      },
      "source": [
        "## 16. üß™ Test sur Quelques Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c907058",
      "metadata": {
        "id": "1c907058"
      },
      "outputs": [],
      "source": [
        "def predict_emotion(model, image_tensor, device):\n",
        "    \"\"\"Pr√©dit l'√©motion pour une image.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "        with torch.amp.autocast('cuda', enabled=config.USE_AMP):\n",
        "            outputs = model(image_tensor)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        pred_idx = outputs.argmax(1).item()\n",
        "        confidence = probs[0, pred_idx].item()\n",
        "    return pred_idx, confidence, probs[0].cpu().numpy()\n",
        "\n",
        "# Test sur quelques images de validation\n",
        "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
        "axes = axes.flatten()\n",
        "\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "emotions = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral', 'Contempt']\n",
        "\n",
        "indices = random.sample(range(len(val_dataset)), 8)\n",
        "\n",
        "for i, idx in enumerate(indices):\n",
        "    img, true_label = val_dataset[idx]\n",
        "    pred_idx, confidence, probs = predict_emotion(eval_model, img, config.DEVICE)  # ‚ö° Utilise eval_model\n",
        "\n",
        "    img_np = img.numpy().transpose(1, 2, 0)\n",
        "    img_np = img_np * std + mean\n",
        "    img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "    true_emotion = emotions[true_label]\n",
        "    pred_emotion = emotions[pred_idx]\n",
        "\n",
        "    color = 'green' if pred_idx == true_label else 'red'\n",
        "\n",
        "    axes[i].imshow(img_np)\n",
        "    axes[i].set_title(f\"Vrai: {true_emotion}\\nPr√©d: {pred_emotion} ({confidence*100:.1f}%)\",\n",
        "                      color=color, fontsize=10)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('üîç Pr√©dictions sur le Set de Validation', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}